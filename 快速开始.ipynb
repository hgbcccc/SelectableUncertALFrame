{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并数据集...\n",
      "数据根目录: /data/22_huangguobin/mmdetection-3.1.0/data/ForestDamages\n",
      "训练集标注文件: /data/22_huangguobin/mmdetection-3.1.0/data/ForestDamages/annotations/instances_train2024.json\n",
      "验证集标注文件: /data/22_huangguobin/mmdetection-3.1.0/data/ForestDamages/annotations/instances_val2024.json\n",
      "训练集图片目录: /data/22_huangguobin/mmdetection-3.1.0/data/ForestDamages/train2024\n",
      "验证集图片目录: /data/22_huangguobin/mmdetection-3.1.0/data/ForestDamages/val2024\n",
      "合并后总图像数: 3170\n",
      "合并后总标注数: 194287\n",
      "\n",
      "数据集分割:\n",
      "总样本数: 3170\n",
      "训练集大小: 126 (4.0%)\n",
      "验证集大小: 31 (1.0%)\n",
      "未标注集大小: 3013 (95.0%)\n",
      "\n",
      "处理 labeled_train 数据集...\n",
      "从以下目录复制图片:\n",
      "  - /data/22_huangguobin/mmdetection-3.1.0/data/ForestDamages/train2024\n",
      "  - /data/22_huangguobin/mmdetection-3.1.0/data/ForestDamages/val2024\n",
      "成功复制 126 张图片到 data/ForestDamages/active_learning_cascade_ssc/images_labeled_train\n",
      "保存标注到: data/ForestDamages/active_learning_cascade_ssc/annotations/instances_labeled_train.json\n",
      "图像数量: 126\n",
      "标注数量: 7836\n",
      "\n",
      "处理 labeled_val 数据集...\n",
      "从以下目录复制图片:\n",
      "  - /data/22_huangguobin/mmdetection-3.1.0/data/ForestDamages/train2024\n",
      "  - /data/22_huangguobin/mmdetection-3.1.0/data/ForestDamages/val2024\n",
      "成功复制 31 张图片到 data/ForestDamages/active_learning_cascade_ssc/images_labeled_val\n",
      "保存标注到: data/ForestDamages/active_learning_cascade_ssc/annotations/instances_labeled_val.json\n",
      "图像数量: 31\n",
      "标注数量: 2362\n",
      "\n",
      "处理 unlabeled 数据集...\n",
      "从以下目录复制图片:\n",
      "  - /data/22_huangguobin/mmdetection-3.1.0/data/ForestDamages/train2024\n",
      "  - /data/22_huangguobin/mmdetection-3.1.0/data/ForestDamages/val2024\n",
      "成功复制 3013 张图片到 data/ForestDamages/active_learning_cascade_ssc/images_unlabeled\n",
      "保存标注到: data/ForestDamages/active_learning_cascade_ssc/annotations/instances_unlabeled.json\n",
      "图像数量: 3013\n",
      "标注数量: 184089\n",
      "\n",
      "数据集准备完成!\n",
      "\n",
      "目录结构:\n",
      "data/ForestDamages/active_learning_cascade_ssc/\n",
      "├── images_labeled_train/\n",
      "├── images_labeled_val/\n",
      "├── images_unlabeled/\n",
      "└── annotations/\n",
      "    ├── instances_labeled_train.json\n",
      "    ├── instances_labeled_val.json\n",
      "    └── instances_unlabeled.json\n"
     ]
    }
   ],
   "source": [
    "!python sual/tools/prepare_dataset/prepare_active_dataset.py \\\n",
    "    data/ForestDamages \\\n",
    "    data/ForestDamages/active_learning_cascade_ssc \\\n",
    "    --train-ratio 0.04 \\\n",
    "    --val-ratio 0.01 \\\n",
    "    --seed 42\n",
    "# 批量运行  \n",
    "# ./custom_config/prepare_experiments.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并数据集...\n",
      "数据根目录: /data/22_huangguobin/mmdetection-3.1.0/data/Bamberg_coco1024\n",
      "训练集标注文件: /data/22_huangguobin/mmdetection-3.1.0/data/Bamberg_coco1024/annotations/instances_train2024.json\n",
      "验证集标注文件: /data/22_huangguobin/mmdetection-3.1.0/data/Bamberg_coco1024/annotations/instances_val2024.json\n",
      "训练集图片目录: /data/22_huangguobin/mmdetection-3.1.0/data/Bamberg_coco1024/train2024\n",
      "验证集图片目录: /data/22_huangguobin/mmdetection-3.1.0/data/Bamberg_coco1024/val2024\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/22_huangguobin/mmdetection-3.1.0/sual/tools/prepare_dataset/prepare_active_dataset.py\", line 294, in <module>\n",
      "    prepare_active_dataset(\n",
      "  File \"/data/22_huangguobin/mmdetection-3.1.0/sual/tools/prepare_dataset/prepare_active_dataset.py\", line 252, in prepare_active_dataset\n",
      "    merged_data, src_img_dirs = merge_coco_datasets(data_root)\n",
      "  File \"/data/22_huangguobin/mmdetection-3.1.0/sual/tools/prepare_dataset/prepare_active_dataset.py\", line 87, in merge_coco_datasets\n",
      "    raise FileNotFoundError(f\"找不到训练集标注文件: {train_ann}\")\n",
      "FileNotFoundError: 找不到训练集标注文件: /data/22_huangguobin/mmdetection-3.1.0/data/Bamberg_coco1024/annotations/instances_train2024.json\n"
     ]
    }
   ],
   "source": [
    "!python sual/tools/prepare_dataset/prepare_active_dataset.py \\\n",
    "    data/Bamberg_coco1024 \\\n",
    "    data/Bamberg_coco1024/active_learning_detr_ssc \\\n",
    "    --train-ratio 0.04 \\\n",
    "    --val-ratio 0.01 \\\n",
    "    --seed 42\n",
    "# 批量运行  \n",
    "# ./custom_config/prepare_experiments.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/26 14:08:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.9.20 (main, Oct  3 2024, 07:27:41) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    MUSA available: False\n",
      "    numpy_random_seed: 2126278232\n",
      "    GPU 0: NVIDIA RTX A6000\n",
      "    CUDA_HOME: :/usr/local/cuda\n",
      "    GCC: gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\n",
      "    PyTorch: 2.0.1+cu118\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201703\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.8\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
      "  - CuDNN 8.7\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.15.2+cu118\n",
      "    OpenCV: 4.11.0\n",
      "    MMEngine: 0.10.5\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: False\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
      "    seed: 2126278232\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "05/26 14:08:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
      "active_learning = dict(\n",
      "    ann_file=\n",
      "    'data/ForestDamages/active_learning_cascade_ssc/annotations/instances_unlabeled.json',\n",
      "    data_prefix=dict(\n",
      "        img='data/ForestDamages/active_learning_cascade_ssc/images_unlabeled'),\n",
      "    data_root='data/ForestDamages/active_learning_cascade_ssc',\n",
      "    inference_options=dict(\n",
      "        batch_size=16,\n",
      "        sample_size=0,\n",
      "        save_results=True,\n",
      "        score_thr=0.08,\n",
      "        selected_metric='ssc_score',\n",
      "        uncertainty_methods=[\n",
      "            'ssc',\n",
      "        ]),\n",
      "    max_iterations=16,\n",
      "    sample_selection=dict(\n",
      "        num_samples=200,\n",
      "        rl_metric='',\n",
      "        sample_selector='default',\n",
      "        uncertainty_metric='ssc_score'),\n",
      "    train_pool_cfg=dict(\n",
      "        ann_file='annotations/instances_labeled_train.json',\n",
      "        data_prefix=dict(img='images_labeled_train'),\n",
      "        data_root='data/ForestDamages/active_learning_cascade_ssc'))\n",
      "auto_scale_lr = dict(base_batch_size=4, enable=False)\n",
      "backend_args = None\n",
      "data_root = 'data/ForestDamages/active_learning_cascade_ssc'\n",
      "dataset_type = 'ForestDamagesDataset'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(interval=1, max_keep_ckpts=3, type='CheckpointHook'),\n",
      "    logger=dict(interval=10, type='LoggerHook'),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    visualization=dict(type='DetVisualizationHook'))\n",
      "default_scope = 'mmdet'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=False,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "interval = 1\n",
      "launcher = 'none'\n",
      "load_from = 'work_dirs/cascade-rcnn_ssc_16_200_weight_chage/round_1/epoch_3.pth'\n",
      "log_level = 'INFO'\n",
      "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)\n",
      "max_boxes_per_img = 300\n",
      "metainfo = dict(\n",
      "    classes=(\n",
      "        'Aspen',\n",
      "        'Birch',\n",
      "        'Other',\n",
      "        'Pine',\n",
      "        'Spruce',\n",
      "    ),\n",
      "    palette=[\n",
      "        (\n",
      "            220,\n",
      "            20,\n",
      "            60,\n",
      "        ),\n",
      "        (\n",
      "            119,\n",
      "            11,\n",
      "            32,\n",
      "        ),\n",
      "        (\n",
      "            0,\n",
      "            0,\n",
      "            142,\n",
      "        ),\n",
      "        (\n",
      "            0,\n",
      "            0,\n",
      "            230,\n",
      "        ),\n",
      "        (\n",
      "            106,\n",
      "            0,\n",
      "            228,\n",
      "        ),\n",
      "    ])\n",
      "model = dict(\n",
      "    backbone=dict(\n",
      "        depth=101,\n",
      "        frozen_stages=1,\n",
      "        init_cfg=dict(checkpoint='torchvision://resnet101', type='Pretrained'),\n",
      "        norm_cfg=dict(requires_grad=True, type='BN'),\n",
      "        norm_eval=True,\n",
      "        num_stages=4,\n",
      "        out_indices=(\n",
      "            0,\n",
      "            1,\n",
      "            2,\n",
      "            3,\n",
      "        ),\n",
      "        style='pytorch',\n",
      "        type='ResNet'),\n",
      "    data_preprocessor=dict(\n",
      "        bgr_to_rgb=True,\n",
      "        mean=[\n",
      "            123.675,\n",
      "            116.28,\n",
      "            103.53,\n",
      "        ],\n",
      "        pad_size_divisor=32,\n",
      "        std=[\n",
      "            58.395,\n",
      "            57.12,\n",
      "            57.375,\n",
      "        ],\n",
      "        type='DetDataPreprocessor'),\n",
      "    neck=dict(\n",
      "        in_channels=[\n",
      "            256,\n",
      "            512,\n",
      "            1024,\n",
      "            2048,\n",
      "        ],\n",
      "        num_outs=5,\n",
      "        out_channels=256,\n",
      "        type='FPN'),\n",
      "    roi_head=dict(\n",
      "        bbox_head=[\n",
      "            dict(\n",
      "                bbox_coder=dict(\n",
      "                    target_means=[\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                    ],\n",
      "                    target_stds=[\n",
      "                        0.1,\n",
      "                        0.1,\n",
      "                        0.2,\n",
      "                        0.2,\n",
      "                    ],\n",
      "                    type='DeltaXYWHBBoxCoder'),\n",
      "                fc_out_channels=1024,\n",
      "                in_channels=256,\n",
      "                loss_bbox=dict(beta=1.0, loss_weight=1.0, type='SmoothL1Loss'),\n",
      "                loss_cls=dict(\n",
      "                    alpha=0.25,\n",
      "                    gamma=2.0,\n",
      "                    loss_weight=1.0,\n",
      "                    type='FocalLoss',\n",
      "                    use_sigmoid=True),\n",
      "                num_classes=5,\n",
      "                reg_class_agnostic=True,\n",
      "                roi_feat_size=7,\n",
      "                type='Shared2FCBBoxHead'),\n",
      "            dict(\n",
      "                bbox_coder=dict(\n",
      "                    target_means=[\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                    ],\n",
      "                    target_stds=[\n",
      "                        0.05,\n",
      "                        0.05,\n",
      "                        0.1,\n",
      "                        0.1,\n",
      "                    ],\n",
      "                    type='DeltaXYWHBBoxCoder'),\n",
      "                fc_out_channels=1024,\n",
      "                in_channels=256,\n",
      "                loss_bbox=dict(beta=1.0, loss_weight=1.0, type='SmoothL1Loss'),\n",
      "                loss_cls=dict(\n",
      "                    alpha=0.25,\n",
      "                    gamma=2.0,\n",
      "                    loss_weight=1.0,\n",
      "                    type='FocalLoss',\n",
      "                    use_sigmoid=True),\n",
      "                num_classes=5,\n",
      "                reg_class_agnostic=True,\n",
      "                roi_feat_size=7,\n",
      "                type='Shared2FCBBoxHead'),\n",
      "            dict(\n",
      "                bbox_coder=dict(\n",
      "                    target_means=[\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                    ],\n",
      "                    target_stds=[\n",
      "                        0.033,\n",
      "                        0.033,\n",
      "                        0.067,\n",
      "                        0.067,\n",
      "                    ],\n",
      "                    type='DeltaXYWHBBoxCoder'),\n",
      "                fc_out_channels=1024,\n",
      "                in_channels=256,\n",
      "                loss_bbox=dict(beta=1.0, loss_weight=1.0, type='SmoothL1Loss'),\n",
      "                loss_cls=dict(\n",
      "                    alpha=0.25,\n",
      "                    gamma=2.0,\n",
      "                    loss_weight=1.0,\n",
      "                    type='FocalLoss',\n",
      "                    use_sigmoid=True),\n",
      "                num_classes=5,\n",
      "                reg_class_agnostic=True,\n",
      "                roi_feat_size=7,\n",
      "                type='Shared2FCBBoxHead'),\n",
      "        ],\n",
      "        bbox_roi_extractor=dict(\n",
      "            featmap_strides=[\n",
      "                4,\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "            ],\n",
      "            out_channels=256,\n",
      "            roi_layer=dict(output_size=7, sampling_ratio=0, type='RoIAlign'),\n",
      "            type='SingleRoIExtractor'),\n",
      "        num_stages=3,\n",
      "        stage_loss_weights=[\n",
      "            1,\n",
      "            0.5,\n",
      "            0.25,\n",
      "        ],\n",
      "        type='CascadeRoIHead'),\n",
      "    rpn_head=dict(\n",
      "        anchor_generator=dict(\n",
      "            ratios=[\n",
      "                0.5,\n",
      "                1.0,\n",
      "                2.0,\n",
      "            ],\n",
      "            scales=[\n",
      "                8,\n",
      "            ],\n",
      "            strides=[\n",
      "                4,\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "                64,\n",
      "            ],\n",
      "            type='AnchorGenerator'),\n",
      "        bbox_coder=dict(\n",
      "            target_means=[\n",
      "                0.0,\n",
      "                0.0,\n",
      "                0.0,\n",
      "                0.0,\n",
      "            ],\n",
      "            target_stds=[\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "            ],\n",
      "            type='DeltaXYWHBBoxCoder'),\n",
      "        feat_channels=256,\n",
      "        in_channels=256,\n",
      "        loss_bbox=dict(loss_weight=1.0, type='L1Loss'),\n",
      "        loss_cls=dict(\n",
      "            alpha=0.25,\n",
      "            gamma=2.0,\n",
      "            loss_weight=1.0,\n",
      "            type='FocalLoss',\n",
      "            use_sigmoid=True),\n",
      "        type='RPNHead'),\n",
      "    test_cfg=dict(\n",
      "        rcnn=dict(\n",
      "            max_per_img=250,\n",
      "            nms=dict(iou_threshold=0.4, type='nms'),\n",
      "            score_thr=0.08),\n",
      "        rpn=dict(\n",
      "            max_per_img=2000,\n",
      "            min_bbox_size=0,\n",
      "            nms=dict(iou_threshold=0.6, type='nms'),\n",
      "            nms_pre=2000)),\n",
      "    train_cfg=dict(\n",
      "        rcnn=[\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    ignore_iof_thr=-1,\n",
      "                    match_low_quality=False,\n",
      "                    min_pos_iou=0.5,\n",
      "                    neg_iou_thr=0.5,\n",
      "                    pos_iou_thr=0.5,\n",
      "                    type='MaxIoUAssigner'),\n",
      "                debug=False,\n",
      "                pos_weight=-1,\n",
      "                sampler=dict(\n",
      "                    add_gt_as_proposals=True,\n",
      "                    neg_pos_ub=-1,\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    type='RandomSampler')),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    ignore_iof_thr=-1,\n",
      "                    match_low_quality=False,\n",
      "                    min_pos_iou=0.6,\n",
      "                    neg_iou_thr=0.6,\n",
      "                    pos_iou_thr=0.6,\n",
      "                    type='MaxIoUAssigner'),\n",
      "                debug=False,\n",
      "                pos_weight=-1,\n",
      "                sampler=dict(\n",
      "                    add_gt_as_proposals=True,\n",
      "                    neg_pos_ub=-1,\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    type='RandomSampler')),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    ignore_iof_thr=-1,\n",
      "                    match_low_quality=False,\n",
      "                    min_pos_iou=0.7,\n",
      "                    neg_iou_thr=0.7,\n",
      "                    pos_iou_thr=0.7,\n",
      "                    type='MaxIoUAssigner'),\n",
      "                debug=False,\n",
      "                pos_weight=-1,\n",
      "                sampler=dict(\n",
      "                    add_gt_as_proposals=True,\n",
      "                    neg_pos_ub=-1,\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    type='RandomSampler')),\n",
      "        ],\n",
      "        rpn=dict(\n",
      "            allowed_border=0,\n",
      "            assigner=dict(\n",
      "                ignore_iof_thr=-1,\n",
      "                match_low_quality=True,\n",
      "                min_pos_iou=0.3,\n",
      "                neg_iou_thr=0.3,\n",
      "                pos_iou_thr=0.7,\n",
      "                type='MaxIoUAssigner'),\n",
      "            debug=False,\n",
      "            pos_weight=-1,\n",
      "            sampler=dict(\n",
      "                add_gt_as_proposals=False,\n",
      "                neg_pos_ub=-1,\n",
      "                num=256,\n",
      "                pos_fraction=0.5,\n",
      "                type='RandomSampler')),\n",
      "        rpn_proposal=dict(\n",
      "            max_per_img=2000,\n",
      "            min_bbox_size=0,\n",
      "            nms=dict(iou_threshold=0.7, type='nms'),\n",
      "            nms_pre=2000)),\n",
      "    type='CascadeRCNN')\n",
      "nms_iou_threshold = 0.5\n",
      "optim_wrapper = dict(\n",
      "    clip_grad=dict(max_norm=35, norm_type=2),\n",
      "    optimizer=dict(\n",
      "        betas=(\n",
      "            0.9,\n",
      "            0.999,\n",
      "        ), lr=0.0001, type='AdamW', weight_decay=0.05),\n",
      "    type='OptimWrapper')\n",
      "param_scheduler = [\n",
      "    dict(\n",
      "        begin=0, by_epoch=False, end=500, start_factor=0.001, type='LinearLR'),\n",
      "    dict(\n",
      "        T_max=3,\n",
      "        begin=0,\n",
      "        by_epoch=True,\n",
      "        convert_to_iter_based=True,\n",
      "        end=3,\n",
      "        eta_min=1e-06,\n",
      "        type='CosineAnnealingLR'),\n",
      "]\n",
      "resume = False\n",
      "score_threshold = 0.08\n",
      "test_cfg = dict(type='TestLoop')\n",
      "test_dataloader = dict(\n",
      "    batch_size=4,\n",
      "    dataset=dict(\n",
      "        ann_file='annotations/instances_labeled_val.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='images_labeled_val'),\n",
      "        data_root='data/ForestDamages/active_learning_cascade_ssc',\n",
      "        metainfo=dict(\n",
      "            classes=(\n",
      "                'Aspen',\n",
      "                'Birch',\n",
      "                'Other',\n",
      "                'Pine',\n",
      "                'Spruce',\n",
      "            ),\n",
      "            palette=[\n",
      "                (\n",
      "                    220,\n",
      "                    20,\n",
      "                    60,\n",
      "                ),\n",
      "                (\n",
      "                    119,\n",
      "                    11,\n",
      "                    32,\n",
      "                ),\n",
      "                (\n",
      "                    0,\n",
      "                    0,\n",
      "                    142,\n",
      "                ),\n",
      "                (\n",
      "                    0,\n",
      "                    0,\n",
      "                    230,\n",
      "                ),\n",
      "                (\n",
      "                    106,\n",
      "                    0,\n",
      "                    228,\n",
      "                ),\n",
      "            ]),\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1536,\n",
      "                1536,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='ForestDamagesDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=4,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(\n",
      "    ann_file=\n",
      "    'data/ForestDamages/active_learning_cascade_ssc/annotations/instances_labeled_val.json',\n",
      "    backend_args=None,\n",
      "    classwise=True,\n",
      "    format_only=False,\n",
      "    metric='bbox',\n",
      "    type='CocoMetric')\n",
      "test_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        1536,\n",
      "        1536,\n",
      "    ), type='Resize'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'scale_factor',\n",
      "        ),\n",
      "        type='PackDetInputs'),\n",
      "]\n",
      "train_cfg = dict(max_epochs=3, type='EpochBasedTrainLoop', val_interval=1)\n",
      "train_dataloader = dict(\n",
      "    batch_sampler=dict(type='AspectRatioBatchSampler'),\n",
      "    batch_size=2,\n",
      "    dataset=dict(\n",
      "        ann_file=\n",
      "        'data/ForestDamages/active_learning_cascade_ssc/annotations/instances_labeled_train.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(\n",
      "            img=\n",
      "            'data/ForestDamages/active_learning_cascade_ssc/images_labeled_train'\n",
      "        ),\n",
      "        data_root='data/ForestDamages/active_learning_cascade_ssc',\n",
      "        filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
      "        metainfo=dict(\n",
      "            classes=(\n",
      "                'Aspen',\n",
      "                'Birch',\n",
      "                'Other',\n",
      "                'Pine',\n",
      "                'Spruce',\n",
      "            ),\n",
      "            palette=[\n",
      "                (\n",
      "                    220,\n",
      "                    20,\n",
      "                    60,\n",
      "                ),\n",
      "                (\n",
      "                    119,\n",
      "                    11,\n",
      "                    32,\n",
      "                ),\n",
      "                (\n",
      "                    0,\n",
      "                    0,\n",
      "                    142,\n",
      "                ),\n",
      "                (\n",
      "                    0,\n",
      "                    0,\n",
      "                    230,\n",
      "                ),\n",
      "                (\n",
      "                    106,\n",
      "                    0,\n",
      "                    228,\n",
      "                ),\n",
      "            ]),\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1280,\n",
      "                1280,\n",
      "            ), type='Resize'),\n",
      "            dict(prob=0.5, type='RandomFlip'),\n",
      "            dict(type='PackDetInputs'),\n",
      "        ],\n",
      "        type='ActiveCocoDataset'),\n",
      "    num_workers=4,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
      "train_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        1280,\n",
      "        1280,\n",
      "    ), type='Resize'),\n",
      "    dict(\n",
      "        brightness_delta=32,\n",
      "        contrast_range=(\n",
      "            0.5,\n",
      "            1.5,\n",
      "        ),\n",
      "        hue_delta=18,\n",
      "        saturation_range=(\n",
      "            0.5,\n",
      "            1.5,\n",
      "        ),\n",
      "        type='PhotoMetricDistortion'),\n",
      "    dict(\n",
      "        min_crop_size=0.3,\n",
      "        min_ious=(\n",
      "            0.4,\n",
      "            0.5,\n",
      "            0.6,\n",
      "            0.7,\n",
      "        ),\n",
      "        type='MinIoURandomCrop'),\n",
      "    dict(angle_range=(\n",
      "        -20,\n",
      "        20,\n",
      "    ), type='RandomRotate'),\n",
      "    dict(prob=0.5, type='RandomFlip'),\n",
      "    dict(type='PackDetInputs'),\n",
      "]\n",
      "val_cfg = dict(type='ValLoop')\n",
      "val_dataloader = dict(\n",
      "    batch_size=4,\n",
      "    dataset=dict(\n",
      "        ann_file='annotations/instances_labeled_val.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='images_labeled_val'),\n",
      "        data_root='data/ForestDamages/active_learning_cascade_ssc',\n",
      "        metainfo=dict(\n",
      "            classes=(\n",
      "                'Aspen',\n",
      "                'Birch',\n",
      "                'Other',\n",
      "                'Pine',\n",
      "                'Spruce',\n",
      "            ),\n",
      "            palette=[\n",
      "                (\n",
      "                    220,\n",
      "                    20,\n",
      "                    60,\n",
      "                ),\n",
      "                (\n",
      "                    119,\n",
      "                    11,\n",
      "                    32,\n",
      "                ),\n",
      "                (\n",
      "                    0,\n",
      "                    0,\n",
      "                    142,\n",
      "                ),\n",
      "                (\n",
      "                    0,\n",
      "                    0,\n",
      "                    230,\n",
      "                ),\n",
      "                (\n",
      "                    106,\n",
      "                    0,\n",
      "                    228,\n",
      "                ),\n",
      "            ]),\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1536,\n",
      "                1536,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='ForestDamagesDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=4,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(\n",
      "    ann_file=\n",
      "    'data/ForestDamages/active_learning_cascade_ssc/annotations/instances_labeled_val.json',\n",
      "    backend_args=None,\n",
      "    classwise=True,\n",
      "    format_only=False,\n",
      "    metric='bbox',\n",
      "    type='CocoMetric')\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "    dict(type='TensorboardVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='DetLocalVisualizer',\n",
      "    vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "        dict(type='TensorboardVisBackend'),\n",
      "    ])\n",
      "work_dir = 'work_dirs/test'\n",
      "\n",
      "05/26 14:08:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "05/26 14:08:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Loads checkpoint by local backend from path: work_dirs/cascade-rcnn_ssc_16_200_weight_chage/round_1/epoch_3.pth\n",
      "05/26 14:08:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Load checkpoint from work_dirs/cascade-rcnn_ssc_16_200_weight_chage/round_1/epoch_3.pth\n",
      "05/26 14:08:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=4.24s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.15s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.011\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.045\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.004\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.016\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.043\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.047\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.047\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.011\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.054\n",
      "05/26 14:08:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "+----------+-------+--------+--------+--------+-------+-------+-------+\n",
      "| category | mAP   | mAP_50 | mAP_75 | mAP_95 | mAP_s | mAP_m | mAP_l |\n",
      "+----------+-------+--------+--------+--------+-------+-------+-------+\n",
      "| Spruce   | 0.02  | 0.076  | 0.01   | 0.0    | nan   | 0.001 | 0.024 |\n",
      "| Pine     | 0.032 | 0.117  | 0.008  | 0.0    | nan   | 0.0   | 0.046 |\n",
      "| Birch    | 0.005 | 0.03   | 0.0    | 0.0    | nan   | 0.0   | 0.009 |\n",
      "| Aspen    | 0.0   | 0.0    | 0.0    | 0.0    | nan   | nan   | 0.0   |\n",
      "| Other    | 0.0   | 0.0    | 0.0    | 0.0    | nan   | 0.0   | 0.0   |\n",
      "+----------+-------+--------+--------+--------+-------+-------+-------+\n",
      "05/26 14:08:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - bbox_mAP_copypaste: 0.011 0.045 0.004 0.000 -1.000 0.000 0.016\n",
      "05/26 14:08:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(test) [8/8]    coco/Spruce_precision: 0.0200  coco/Pine_precision: 0.0320  coco/Birch_precision: 0.0050  coco/Aspen_precision: 0.0000  coco/Other_precision: 0.0000  coco/bbox_mAP: 0.0110  coco/bbox_mAP_50: 0.0450  coco/bbox_mAP_75: 0.0040  coco/bbox_mAP_95: 0.0000  coco/bbox_mAP_s: -1.0000  coco/bbox_mAP_m: 0.0000  coco/bbox_mAP_l: 0.0160  data_time: 0.0536  time: 0.4858\n"
     ]
    }
   ],
   "source": [
    "!python tools/test.py \\\n",
    "    work_dirs/cascade-rcnn_ssc_16_200_weight_chage/round_1/cascade-rcnn_ssc.py \\\n",
    "    work_dirs/cascade-rcnn_ssc_16_200_weight_chage/round_1/epoch_3.pth \\\n",
    "    --work-dir work_dirs/test \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.准备配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto_scale_lr = dict(base_batch_size=16, enable=False)\n",
      "backend_args = None\n",
      "data_root = 'data/coco/'\n",
      "dataset_type = 'CocoDataset'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(interval=1, type='CheckpointHook'),\n",
      "    logger=dict(interval=50, type='LoggerHook'),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    visualization=dict(type='DetVisualizationHook'))\n",
      "default_scope = 'mmdet'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=False,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "load_from = None\n",
      "log_level = 'INFO'\n",
      "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)\n",
      "model = dict(\n",
      "    backbone=dict(\n",
      "        depth=101,\n",
      "        frozen_stages=1,\n",
      "        init_cfg=dict(\n",
      "            checkpoint='open-mmlab://detectron/resnet101_caffe',\n",
      "            type='Pretrained'),\n",
      "        norm_cfg=dict(requires_grad=False, type='BN'),\n",
      "        norm_eval=True,\n",
      "        num_stages=4,\n",
      "        out_indices=(\n",
      "            0,\n",
      "            1,\n",
      "            2,\n",
      "            3,\n",
      "        ),\n",
      "        style='caffe',\n",
      "        type='ResNet'),\n",
      "    bbox_head=dict(\n",
      "        feat_channels=256,\n",
      "        in_channels=256,\n",
      "        loss_bbox=dict(loss_weight=1.0, type='IoULoss'),\n",
      "        loss_centerness=dict(\n",
      "            loss_weight=1.0, type='CrossEntropyLoss', use_sigmoid=True),\n",
      "        loss_cls=dict(\n",
      "            alpha=0.25,\n",
      "            gamma=2.0,\n",
      "            loss_weight=1.0,\n",
      "            type='FocalLoss',\n",
      "            use_sigmoid=True),\n",
      "        num_classes=80,\n",
      "        stacked_convs=4,\n",
      "        strides=[\n",
      "            8,\n",
      "            16,\n",
      "            32,\n",
      "            64,\n",
      "            128,\n",
      "        ],\n",
      "        type='FCOSHead'),\n",
      "    data_preprocessor=dict(\n",
      "        bgr_to_rgb=False,\n",
      "        mean=[\n",
      "            102.9801,\n",
      "            115.9465,\n",
      "            122.7717,\n",
      "        ],\n",
      "        pad_size_divisor=32,\n",
      "        std=[\n",
      "            1.0,\n",
      "            1.0,\n",
      "            1.0,\n",
      "        ],\n",
      "        type='DetDataPreprocessor'),\n",
      "    neck=dict(\n",
      "        add_extra_convs='on_output',\n",
      "        in_channels=[\n",
      "            256,\n",
      "            512,\n",
      "            1024,\n",
      "            2048,\n",
      "        ],\n",
      "        num_outs=5,\n",
      "        out_channels=256,\n",
      "        relu_before_extra_convs=True,\n",
      "        start_level=1,\n",
      "        type='FPN'),\n",
      "    test_cfg=dict(\n",
      "        max_per_img=100,\n",
      "        min_bbox_size=0,\n",
      "        nms=dict(iou_threshold=0.5, type='nms'),\n",
      "        nms_pre=1000,\n",
      "        score_thr=0.05),\n",
      "    type='FCOS')\n",
      "optim_wrapper = dict(\n",
      "    clip_grad=dict(max_norm=35, norm_type=2),\n",
      "    optimizer=dict(lr=0.01, momentum=0.9, type='SGD', weight_decay=0.0001),\n",
      "    paramwise_cfg=dict(bias_decay_mult=0.0, bias_lr_mult=2.0),\n",
      "    type='OptimWrapper')\n",
      "param_scheduler = [\n",
      "    dict(\n",
      "        begin=0,\n",
      "        by_epoch=False,\n",
      "        end=500,\n",
      "        factor=0.3333333333333333,\n",
      "        type='ConstantLR'),\n",
      "    dict(\n",
      "        begin=0,\n",
      "        by_epoch=True,\n",
      "        end=12,\n",
      "        gamma=0.1,\n",
      "        milestones=[\n",
      "            8,\n",
      "            11,\n",
      "        ],\n",
      "        type='MultiStepLR'),\n",
      "]\n",
      "resume = False\n",
      "test_cfg = dict(type='TestLoop')\n",
      "test_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='annotations/instances_val2017.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='val2017/'),\n",
      "        data_root='data/coco/',\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1333,\n",
      "                800,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='CocoDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(\n",
      "    ann_file='data/coco/annotations/instances_val2017.json',\n",
      "    backend_args=None,\n",
      "    format_only=False,\n",
      "    metric='bbox',\n",
      "    type='CocoMetric')\n",
      "test_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        1333,\n",
      "        800,\n",
      "    ), type='Resize'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'scale_factor',\n",
      "        ),\n",
      "        type='PackDetInputs'),\n",
      "]\n",
      "train_cfg = dict(max_epochs=12, type='EpochBasedTrainLoop', val_interval=1)\n",
      "train_dataloader = dict(\n",
      "    batch_sampler=dict(type='AspectRatioBatchSampler'),\n",
      "    batch_size=2,\n",
      "    dataset=dict(\n",
      "        ann_file='annotations/instances_train2017.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='train2017/'),\n",
      "        data_root='data/coco/',\n",
      "        filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1333,\n",
      "                800,\n",
      "            ), type='Resize'),\n",
      "            dict(prob=0.5, type='RandomFlip'),\n",
      "            dict(type='PackDetInputs'),\n",
      "        ],\n",
      "        type='CocoDataset'),\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
      "train_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        1333,\n",
      "        800,\n",
      "    ), type='Resize'),\n",
      "    dict(prob=0.5, type='RandomFlip'),\n",
      "    dict(type='PackDetInputs'),\n",
      "]\n",
      "val_cfg = dict(type='ValLoop')\n",
      "val_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='annotations/instances_val2017.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='val2017/'),\n",
      "        data_root='data/coco/',\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1333,\n",
      "                800,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='CocoDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(\n",
      "    ann_file='data/coco/annotations/instances_val2017.json',\n",
      "    backend_args=None,\n",
      "    format_only=False,\n",
      "    metric='bbox',\n",
      "    type='CocoMetric')\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='DetLocalVisualizer',\n",
      "    vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "    ])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmengine import Config\n",
    "\n",
    "# 加载配置文件\n",
    "cfg = Config.fromfile('configs/fcos/fcos_r101-caffe_fpn_gn-head-1x_coco.py')\n",
    "\n",
    "# 打印完整配置\n",
    "print(cfg.pretty_text)\n",
    "\n",
    "# 保存到文件\n",
    "with open('fcos.py', 'w') as f:\n",
    "    f.write(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto_scale_lr = dict(base_batch_size=16, enable=False)\n",
      "backend_args = None\n",
      "data_root = 'data/coco/'\n",
      "dataset_type = 'CocoDataset'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(interval=1, type='CheckpointHook'),\n",
      "    logger=dict(interval=50, type='LoggerHook'),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    visualization=dict(type='DetVisualizationHook'))\n",
      "default_scope = 'mmdet'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=False,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "load_from = None\n",
      "log_level = 'INFO'\n",
      "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)\n",
      "model = dict(\n",
      "    backbone=dict(\n",
      "        depth=101,\n",
      "        frozen_stages=1,\n",
      "        init_cfg=dict(checkpoint='torchvision://resnet101', type='Pretrained'),\n",
      "        norm_cfg=dict(requires_grad=True, type='BN'),\n",
      "        norm_eval=True,\n",
      "        num_stages=4,\n",
      "        out_indices=(\n",
      "            0,\n",
      "            1,\n",
      "            2,\n",
      "            3,\n",
      "        ),\n",
      "        style='pytorch',\n",
      "        type='ResNet'),\n",
      "    data_preprocessor=dict(\n",
      "        bgr_to_rgb=True,\n",
      "        mean=[\n",
      "            123.675,\n",
      "            116.28,\n",
      "            103.53,\n",
      "        ],\n",
      "        pad_size_divisor=32,\n",
      "        std=[\n",
      "            58.395,\n",
      "            57.12,\n",
      "            57.375,\n",
      "        ],\n",
      "        type='DetDataPreprocessor'),\n",
      "    neck=dict(\n",
      "        in_channels=[\n",
      "            256,\n",
      "            512,\n",
      "            1024,\n",
      "            2048,\n",
      "        ],\n",
      "        num_outs=5,\n",
      "        out_channels=256,\n",
      "        type='FPN'),\n",
      "    roi_head=dict(\n",
      "        bbox_head=[\n",
      "            dict(\n",
      "                bbox_coder=dict(\n",
      "                    target_means=[\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                    ],\n",
      "                    target_stds=[\n",
      "                        0.1,\n",
      "                        0.1,\n",
      "                        0.2,\n",
      "                        0.2,\n",
      "                    ],\n",
      "                    type='DeltaXYWHBBoxCoder'),\n",
      "                fc_out_channels=1024,\n",
      "                in_channels=256,\n",
      "                loss_bbox=dict(beta=1.0, loss_weight=1.0, type='SmoothL1Loss'),\n",
      "                loss_cls=dict(\n",
      "                    loss_weight=1.0,\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False),\n",
      "                num_classes=80,\n",
      "                reg_class_agnostic=True,\n",
      "                roi_feat_size=7,\n",
      "                type='Shared2FCBBoxHead'),\n",
      "            dict(\n",
      "                bbox_coder=dict(\n",
      "                    target_means=[\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                    ],\n",
      "                    target_stds=[\n",
      "                        0.05,\n",
      "                        0.05,\n",
      "                        0.1,\n",
      "                        0.1,\n",
      "                    ],\n",
      "                    type='DeltaXYWHBBoxCoder'),\n",
      "                fc_out_channels=1024,\n",
      "                in_channels=256,\n",
      "                loss_bbox=dict(beta=1.0, loss_weight=1.0, type='SmoothL1Loss'),\n",
      "                loss_cls=dict(\n",
      "                    loss_weight=1.0,\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False),\n",
      "                num_classes=80,\n",
      "                reg_class_agnostic=True,\n",
      "                roi_feat_size=7,\n",
      "                type='Shared2FCBBoxHead'),\n",
      "            dict(\n",
      "                bbox_coder=dict(\n",
      "                    target_means=[\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                    ],\n",
      "                    target_stds=[\n",
      "                        0.033,\n",
      "                        0.033,\n",
      "                        0.067,\n",
      "                        0.067,\n",
      "                    ],\n",
      "                    type='DeltaXYWHBBoxCoder'),\n",
      "                fc_out_channels=1024,\n",
      "                in_channels=256,\n",
      "                loss_bbox=dict(beta=1.0, loss_weight=1.0, type='SmoothL1Loss'),\n",
      "                loss_cls=dict(\n",
      "                    loss_weight=1.0,\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False),\n",
      "                num_classes=80,\n",
      "                reg_class_agnostic=True,\n",
      "                roi_feat_size=7,\n",
      "                type='Shared2FCBBoxHead'),\n",
      "        ],\n",
      "        bbox_roi_extractor=dict(\n",
      "            featmap_strides=[\n",
      "                4,\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "            ],\n",
      "            out_channels=256,\n",
      "            roi_layer=dict(output_size=7, sampling_ratio=0, type='RoIAlign'),\n",
      "            type='SingleRoIExtractor'),\n",
      "        num_stages=3,\n",
      "        stage_loss_weights=[\n",
      "            1,\n",
      "            0.5,\n",
      "            0.25,\n",
      "        ],\n",
      "        type='CascadeRoIHead'),\n",
      "    rpn_head=dict(\n",
      "        anchor_generator=dict(\n",
      "            ratios=[\n",
      "                0.5,\n",
      "                1.0,\n",
      "                2.0,\n",
      "            ],\n",
      "            scales=[\n",
      "                8,\n",
      "            ],\n",
      "            strides=[\n",
      "                4,\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "                64,\n",
      "            ],\n",
      "            type='AnchorGenerator'),\n",
      "        bbox_coder=dict(\n",
      "            target_means=[\n",
      "                0.0,\n",
      "                0.0,\n",
      "                0.0,\n",
      "                0.0,\n",
      "            ],\n",
      "            target_stds=[\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "            ],\n",
      "            type='DeltaXYWHBBoxCoder'),\n",
      "        feat_channels=256,\n",
      "        in_channels=256,\n",
      "        loss_bbox=dict(\n",
      "            beta=0.1111111111111111, loss_weight=1.0, type='SmoothL1Loss'),\n",
      "        loss_cls=dict(\n",
      "            loss_weight=1.0, type='CrossEntropyLoss', use_sigmoid=True),\n",
      "        type='RPNHead'),\n",
      "    test_cfg=dict(\n",
      "        rcnn=dict(\n",
      "            max_per_img=100,\n",
      "            nms=dict(iou_threshold=0.5, type='nms'),\n",
      "            score_thr=0.05),\n",
      "        rpn=dict(\n",
      "            max_per_img=1000,\n",
      "            min_bbox_size=0,\n",
      "            nms=dict(iou_threshold=0.7, type='nms'),\n",
      "            nms_pre=1000)),\n",
      "    train_cfg=dict(\n",
      "        rcnn=[\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    ignore_iof_thr=-1,\n",
      "                    match_low_quality=False,\n",
      "                    min_pos_iou=0.5,\n",
      "                    neg_iou_thr=0.5,\n",
      "                    pos_iou_thr=0.5,\n",
      "                    type='MaxIoUAssigner'),\n",
      "                debug=False,\n",
      "                pos_weight=-1,\n",
      "                sampler=dict(\n",
      "                    add_gt_as_proposals=True,\n",
      "                    neg_pos_ub=-1,\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    type='RandomSampler')),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    ignore_iof_thr=-1,\n",
      "                    match_low_quality=False,\n",
      "                    min_pos_iou=0.6,\n",
      "                    neg_iou_thr=0.6,\n",
      "                    pos_iou_thr=0.6,\n",
      "                    type='MaxIoUAssigner'),\n",
      "                debug=False,\n",
      "                pos_weight=-1,\n",
      "                sampler=dict(\n",
      "                    add_gt_as_proposals=True,\n",
      "                    neg_pos_ub=-1,\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    type='RandomSampler')),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    ignore_iof_thr=-1,\n",
      "                    match_low_quality=False,\n",
      "                    min_pos_iou=0.7,\n",
      "                    neg_iou_thr=0.7,\n",
      "                    pos_iou_thr=0.7,\n",
      "                    type='MaxIoUAssigner'),\n",
      "                debug=False,\n",
      "                pos_weight=-1,\n",
      "                sampler=dict(\n",
      "                    add_gt_as_proposals=True,\n",
      "                    neg_pos_ub=-1,\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    type='RandomSampler')),\n",
      "        ],\n",
      "        rpn=dict(\n",
      "            allowed_border=0,\n",
      "            assigner=dict(\n",
      "                ignore_iof_thr=-1,\n",
      "                match_low_quality=True,\n",
      "                min_pos_iou=0.3,\n",
      "                neg_iou_thr=0.3,\n",
      "                pos_iou_thr=0.7,\n",
      "                type='MaxIoUAssigner'),\n",
      "            debug=False,\n",
      "            pos_weight=-1,\n",
      "            sampler=dict(\n",
      "                add_gt_as_proposals=False,\n",
      "                neg_pos_ub=-1,\n",
      "                num=256,\n",
      "                pos_fraction=0.5,\n",
      "                type='RandomSampler')),\n",
      "        rpn_proposal=dict(\n",
      "            max_per_img=2000,\n",
      "            min_bbox_size=0,\n",
      "            nms=dict(iou_threshold=0.7, type='nms'),\n",
      "            nms_pre=2000)),\n",
      "    type='CascadeRCNN')\n",
      "optim_wrapper = dict(\n",
      "    optimizer=dict(lr=0.02, momentum=0.9, type='SGD', weight_decay=0.0001),\n",
      "    type='OptimWrapper')\n",
      "param_scheduler = [\n",
      "    dict(\n",
      "        begin=0, by_epoch=False, end=500, start_factor=0.001, type='LinearLR'),\n",
      "    dict(\n",
      "        begin=0,\n",
      "        by_epoch=True,\n",
      "        end=12,\n",
      "        gamma=0.1,\n",
      "        milestones=[\n",
      "            8,\n",
      "            11,\n",
      "        ],\n",
      "        type='MultiStepLR'),\n",
      "]\n",
      "resume = False\n",
      "test_cfg = dict(type='TestLoop')\n",
      "test_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='annotations/instances_val2017.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='val2017/'),\n",
      "        data_root='data/coco/',\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1333,\n",
      "                800,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='CocoDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(\n",
      "    ann_file='data/coco/annotations/instances_val2017.json',\n",
      "    backend_args=None,\n",
      "    format_only=False,\n",
      "    metric='bbox',\n",
      "    type='CocoMetric')\n",
      "test_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        1333,\n",
      "        800,\n",
      "    ), type='Resize'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'scale_factor',\n",
      "        ),\n",
      "        type='PackDetInputs'),\n",
      "]\n",
      "train_cfg = dict(max_epochs=12, type='EpochBasedTrainLoop', val_interval=1)\n",
      "train_dataloader = dict(\n",
      "    batch_sampler=dict(type='AspectRatioBatchSampler'),\n",
      "    batch_size=2,\n",
      "    dataset=dict(\n",
      "        ann_file='annotations/instances_train2017.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='train2017/'),\n",
      "        data_root='data/coco/',\n",
      "        filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1333,\n",
      "                800,\n",
      "            ), type='Resize'),\n",
      "            dict(prob=0.5, type='RandomFlip'),\n",
      "            dict(type='PackDetInputs'),\n",
      "        ],\n",
      "        type='CocoDataset'),\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
      "train_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        1333,\n",
      "        800,\n",
      "    ), type='Resize'),\n",
      "    dict(prob=0.5, type='RandomFlip'),\n",
      "    dict(type='PackDetInputs'),\n",
      "]\n",
      "val_cfg = dict(type='ValLoop')\n",
      "val_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='annotations/instances_val2017.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='val2017/'),\n",
      "        data_root='data/coco/',\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1333,\n",
      "                800,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='CocoDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(\n",
      "    ann_file='data/coco/annotations/instances_val2017.json',\n",
      "    backend_args=None,\n",
      "    format_only=False,\n",
      "    metric='bbox',\n",
      "    type='CocoMetric')\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='DetLocalVisualizer',\n",
      "    vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "    ])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmengine import Config\n",
    "\n",
    "# 加载配置文件\n",
    "cfg = Config.fromfile('configs/cascade_rcnn/cascade-rcnn_r101_fpn_1x_coco.py')\n",
    "\n",
    "# 打印完整配置\n",
    "print(cfg.pretty_text)\n",
    "\n",
    "# 保存到文件\n",
    "with open('cascade-rcnn.py', 'w') as f:\n",
    "    f.write(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.各轮teacher模型推理验证集图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "处理目录: work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results\n",
      "推理结果目录不存在: work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results\n",
      "\n",
      "处理目录: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results\n",
      "推理结果目录不存在: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results\n",
      "\n",
      "处理目录: work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results\n",
      "推理结果目录不存在: work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results\n",
      "\n",
      "处理目录: work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results\n",
      "推理结果目录存在: work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results\n",
      "推理结果目录已删除: work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results\n",
      "\n",
      "处理目录: work_dirs/al_faster-rcnn_ssc_default/inference_results\n",
      "推理结果目录存在: work_dirs/al_faster-rcnn_ssc_default/inference_results\n",
      "推理结果目录已删除: work_dirs/al_faster-rcnn_ssc_default/inference_results\n",
      "\n",
      "处理目录: work_dirs/faster-rcnn_margin_default/inference_results\n",
      "推理结果目录不存在: work_dirs/faster-rcnn_margin_default/inference_results\n",
      "\n",
      "处理目录: work_dirs/faster-rcnn_basic_default/inference_results\n",
      "推理结果目录不存在: work_dirs/faster-rcnn_basic_default/inference_results\n",
      "\n",
      "处理目录: work_dirs/faster-rcnn_sor/inference_results\n",
      "推理结果目录不存在: work_dirs/faster-rcnn_sor/inference_results\n",
      "\n",
      "处理目录: work_dirs/faster-rcnn_entropy/inference_results\n",
      "推理结果目录不存在: work_dirs/faster-rcnn_entropy/inference_results\n",
      "\n",
      "处理目录: work_dirs/faster-rcnn_least_confid/inference_results\n",
      "推理结果目录不存在: work_dirs/faster-rcnn_least_confid/inference_results\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "# 遍历work_dirs下的所有目录\n",
    "work_dirs = os.listdir(\"work_dirs\")\n",
    "for work_dir in work_dirs:\n",
    "    if work_dir.startswith(\"al_faster-rcnn_ssc_\") or work_dir.startswith(\"faster-rcnn_\"):\n",
    "        inference_results_dir = f\"work_dirs/{work_dir}/inference_results\"\n",
    "        print(f\"\\n处理目录: {inference_results_dir}\")\n",
    "\n",
    "        if os.path.exists(inference_results_dir):\n",
    "            print(f\"推理结果目录存在: {inference_results_dir}\")\n",
    "            os.system(f\"rm -rf {inference_results_dir}\")\n",
    "            print(f\"推理结果目录已删除: {inference_results_dir}\")\n",
    "        else:\n",
    "            print(f\"推理结果目录不存在: {inference_results_dir}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始处理配置: custom_config/faster-rcnn_ssc_combinatorial.py\n",
      "执行命令: python sual/inference.py         custom_config/faster-rcnn_ssc_combinatorial.py         --base-dir work_dirs/al_faster-rcnn_ssc_combinatorial         --img-dir data/ForestDamages/active_learning_entropy/images_labeled_val         --out-dir work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results         --device cuda:0         --score-thr 0.2         --rounds 16\n",
      "找到 16 张图片\n",
      "\n",
      "处理第 1 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_combinatorial/round_1/epoch_1.pth\n",
      "Round 1 Processing: 100%|███████████████████████| 16/16 [00:01<00:00, 11.96it/s]\n",
      "第 1 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results/round_1/inference_results.json\n",
      "\n",
      "处理第 2 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_combinatorial/round_2/epoch_1.pth\n",
      "Round 2 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.59it/s]\n",
      "第 2 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results/round_2/inference_results.json\n",
      "\n",
      "处理第 3 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_combinatorial/round_3/epoch_1.pth\n",
      "Round 3 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.39it/s]\n",
      "第 3 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results/round_3/inference_results.json\n",
      "\n",
      "处理第 4 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_combinatorial/round_4/epoch_1.pth\n",
      "Round 4 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.23it/s]\n",
      "第 4 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results/round_4/inference_results.json\n",
      "\n",
      "处理第 5 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_combinatorial/round_5/epoch_1.pth\n",
      "Round 5 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.20it/s]\n",
      "第 5 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results/round_5/inference_results.json\n",
      "\n",
      "处理第 6 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_combinatorial/round_6/epoch_1.pth\n",
      "Round 6 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.07it/s]\n",
      "第 6 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results/round_6/inference_results.json\n",
      "\n",
      "处理第 7 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_combinatorial/round_7/epoch_1.pth\n",
      "Round 7 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.54it/s]\n",
      "第 7 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results/round_7/inference_results.json\n",
      "\n",
      "处理第 8 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_combinatorial/round_8/epoch_1.pth\n",
      "Round 8 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.06it/s]\n",
      "第 8 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results/round_8/inference_results.json\n",
      "\n",
      "处理第 9 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_combinatorial/round_9/epoch_1.pth\n",
      "Round 9 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.81it/s]\n",
      "第 9 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results/round_9/inference_results.json\n",
      "\n",
      "处理第 10 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_combinatorial/round_10/epoch_1.pth\n",
      "Round 10 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.52it/s]\n",
      "第 10 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results/round_10/inference_results.json\n",
      "\n",
      "处理第 11 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_combinatorial/round_11/epoch_1.pth\n",
      "Round 11 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.18it/s]\n",
      "第 11 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results/round_11/inference_results.json\n",
      "\n",
      "处理第 12 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_combinatorial/round_12/epoch_1.pth\n",
      "Round 12 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.46it/s]\n",
      "第 12 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results/round_12/inference_results.json\n",
      "\n",
      "处理第 13 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_combinatorial/round_13/epoch_1.pth\n",
      "Round 13 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.02it/s]\n",
      "第 13 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results/round_13/inference_results.json\n",
      "\n",
      "处理第 14 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_combinatorial/round_14/epoch_1.pth\n",
      "Round 14 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.74it/s]\n",
      "第 14 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results/round_14/inference_results.json\n",
      "\n",
      "处理第 15 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_combinatorial/round_15/epoch_1.pth\n",
      "Round 15 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 20.26it/s]\n",
      "第 15 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results/round_15/inference_results.json\n",
      "\n",
      "处理第 16 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_combinatorial/round_16/epoch_1.pth\n",
      "Round 16 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 20.32it/s]\n",
      "第 16 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results/round_16/inference_results.json\n",
      "配置 custom_config/faster-rcnn_ssc_combinatorial.py 推理完成\n",
      "\n",
      "开始处理配置: custom_config/faster-rcnn_ssc_Default.py\n",
      "执行命令: python sual/inference.py         custom_config/faster-rcnn_ssc_Default.py         --base-dir work_dirs/al_faster-rcnn_ssc_default         --img-dir data/ForestDamages/active_learning_entropy/images_labeled_val         --out-dir work_dirs/al_faster-rcnn_ssc_default/inference_results         --device cuda:0         --score-thr 0.2         --rounds 16\n",
      "找到 16 张图片\n",
      "\n",
      "处理第 1 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_default/round_1/epoch_1.pth\n",
      "Round 1 Processing: 100%|███████████████████████| 16/16 [00:01<00:00, 12.15it/s]\n",
      "第 1 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_default/inference_results/round_1/inference_results.json\n",
      "\n",
      "处理第 2 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_default/round_2/epoch_1.pth\n",
      "Round 2 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.14it/s]\n",
      "第 2 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_default/inference_results/round_2/inference_results.json\n",
      "\n",
      "处理第 3 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_default/round_3/epoch_1.pth\n",
      "Round 3 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.25it/s]\n",
      "第 3 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_default/inference_results/round_3/inference_results.json\n",
      "\n",
      "处理第 4 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_default/round_4/epoch_1.pth\n",
      "Round 4 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.72it/s]\n",
      "第 4 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_default/inference_results/round_4/inference_results.json\n",
      "\n",
      "处理第 5 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_default/round_5/epoch_1.pth\n",
      "Round 5 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.20it/s]\n",
      "第 5 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_default/inference_results/round_5/inference_results.json\n",
      "\n",
      "处理第 6 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_default/round_6/epoch_1.pth\n",
      "Round 6 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.93it/s]\n",
      "第 6 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_default/inference_results/round_6/inference_results.json\n",
      "\n",
      "处理第 7 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_default/round_7/epoch_1.pth\n",
      "Round 7 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.59it/s]\n",
      "第 7 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_default/inference_results/round_7/inference_results.json\n",
      "\n",
      "处理第 8 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_default/round_8/epoch_1.pth\n",
      "Round 8 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.72it/s]\n",
      "第 8 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_default/inference_results/round_8/inference_results.json\n",
      "\n",
      "处理第 9 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_default/round_9/epoch_1.pth\n",
      "Round 9 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.74it/s]\n",
      "第 9 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_default/inference_results/round_9/inference_results.json\n",
      "\n",
      "处理第 10 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_default/round_10/epoch_1.pth\n",
      "Round 10 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.51it/s]\n",
      "第 10 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_default/inference_results/round_10/inference_results.json\n",
      "\n",
      "处理第 11 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_default/round_11/epoch_1.pth\n",
      "Round 11 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.52it/s]\n",
      "第 11 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_default/inference_results/round_11/inference_results.json\n",
      "\n",
      "处理第 12 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_default/round_12/epoch_1.pth\n",
      "Round 12 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.05it/s]\n",
      "第 12 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_default/inference_results/round_12/inference_results.json\n",
      "\n",
      "处理第 13 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_default/round_13/epoch_1.pth\n",
      "Round 13 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.66it/s]\n",
      "第 13 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_default/inference_results/round_13/inference_results.json\n",
      "\n",
      "处理第 14 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_default/round_14/epoch_1.pth\n",
      "Round 14 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.48it/s]\n",
      "第 14 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_default/inference_results/round_14/inference_results.json\n",
      "\n",
      "处理第 15 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_default/round_15/epoch_1.pth\n",
      "Round 15 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.91it/s]\n",
      "第 15 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_default/inference_results/round_15/inference_results.json\n",
      "\n",
      "处理第 16 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_default/round_16/epoch_1.pth\n",
      "Round 16 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.94it/s]\n",
      "第 16 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_default/inference_results/round_16/inference_results.json\n",
      "配置 custom_config/faster-rcnn_ssc_Default.py 推理完成\n",
      "\n",
      "开始处理配置: custom_config/faster-rcnn_ssc_RL_wasserstein.py\n",
      "执行命令: python sual/inference.py         custom_config/faster-rcnn_ssc_RL_wasserstein.py         --base-dir work_dirs/al_faster-rcnn_ssc_rl_wasserstein         --img-dir data/ForestDamages/active_learning_entropy/images_labeled_val         --out-dir work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results         --device cuda:0         --score-thr 0.2         --rounds 16\n",
      "找到 16 张图片\n",
      "\n",
      "处理第 1 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/round_1/epoch_1.pth\n",
      "Round 1 Processing: 100%|███████████████████████| 16/16 [00:01<00:00, 11.99it/s]\n",
      "第 1 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results/round_1/inference_results.json\n",
      "\n",
      "处理第 2 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/round_2/epoch_1.pth\n",
      "Round 2 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.16it/s]\n",
      "第 2 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results/round_2/inference_results.json\n",
      "\n",
      "处理第 3 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/round_3/epoch_1.pth\n",
      "Round 3 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.16it/s]\n",
      "第 3 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results/round_3/inference_results.json\n",
      "\n",
      "处理第 4 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/round_4/epoch_1.pth\n",
      "Round 4 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 17.23it/s]\n",
      "第 4 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results/round_4/inference_results.json\n",
      "\n",
      "处理第 5 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/round_5/epoch_1.pth\n",
      "Round 5 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 17.55it/s]\n",
      "第 5 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results/round_5/inference_results.json\n",
      "\n",
      "处理第 6 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/round_6/epoch_1.pth\n",
      "Round 6 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 17.25it/s]\n",
      "第 6 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results/round_6/inference_results.json\n",
      "\n",
      "处理第 7 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/round_7/epoch_1.pth\n",
      "Round 7 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 17.43it/s]\n",
      "第 7 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results/round_7/inference_results.json\n",
      "\n",
      "处理第 8 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/round_8/epoch_1.pth\n",
      "Round 8 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 17.66it/s]\n",
      "第 8 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results/round_8/inference_results.json\n",
      "\n",
      "处理第 9 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/round_9/epoch_1.pth\n",
      "Round 9 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 17.85it/s]\n",
      "第 9 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results/round_9/inference_results.json\n",
      "\n",
      "处理第 10 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/round_10/epoch_1.pth\n",
      "Round 10 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 17.33it/s]\n",
      "第 10 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results/round_10/inference_results.json\n",
      "\n",
      "处理第 11 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/round_11/epoch_1.pth\n",
      "Round 11 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 17.93it/s]\n",
      "第 11 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results/round_11/inference_results.json\n",
      "\n",
      "处理第 12 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/round_12/epoch_1.pth\n",
      "Round 12 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.38it/s]\n",
      "第 12 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results/round_12/inference_results.json\n",
      "\n",
      "处理第 13 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/round_13/epoch_1.pth\n",
      "Round 13 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.58it/s]\n",
      "第 13 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results/round_13/inference_results.json\n",
      "\n",
      "处理第 14 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/round_14/epoch_1.pth\n",
      "Round 14 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.72it/s]\n",
      "第 14 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results/round_14/inference_results.json\n",
      "\n",
      "处理第 15 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/round_15/epoch_1.pth\n",
      "Round 15 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.80it/s]\n",
      "第 15 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results/round_15/inference_results.json\n",
      "\n",
      "处理第 16 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/round_16/epoch_1.pth\n",
      "Round 16 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.26it/s]\n",
      "第 16 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results/round_16/inference_results.json\n",
      "配置 custom_config/faster-rcnn_ssc_RL_wasserstein.py 推理完成\n",
      "\n",
      "开始处理配置: custom_config/faster-rcnn_ssc_RL_kl.py\n",
      "执行命令: python sual/inference.py         custom_config/faster-rcnn_ssc_RL_kl.py         --base-dir work_dirs/al_faster-rcnn_ssc_rl_kl         --img-dir data/ForestDamages/active_learning_entropy/images_labeled_val         --out-dir work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results         --device cuda:0         --score-thr 0.2         --rounds 16\n",
      "找到 16 张图片\n",
      "\n",
      "处理第 1 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_kl/round_1/epoch_1.pth\n",
      "Round 1 Processing: 100%|███████████████████████| 16/16 [00:01<00:00, 11.23it/s]\n",
      "第 1 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results/round_1/inference_results.json\n",
      "\n",
      "处理第 2 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_kl/round_2/epoch_1.pth\n",
      "Round 2 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 17.15it/s]\n",
      "第 2 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results/round_2/inference_results.json\n",
      "\n",
      "处理第 3 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_kl/round_3/epoch_1.pth\n",
      "Round 3 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 17.95it/s]\n",
      "第 3 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results/round_3/inference_results.json\n",
      "\n",
      "处理第 4 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_kl/round_4/epoch_1.pth\n",
      "Round 4 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.05it/s]\n",
      "第 4 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results/round_4/inference_results.json\n",
      "\n",
      "处理第 5 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_kl/round_5/epoch_1.pth\n",
      "Round 5 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.00it/s]\n",
      "第 5 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results/round_5/inference_results.json\n",
      "\n",
      "处理第 6 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_kl/round_6/epoch_1.pth\n",
      "Round 6 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.02it/s]\n",
      "第 6 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results/round_6/inference_results.json\n",
      "\n",
      "处理第 7 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_kl/round_7/epoch_1.pth\n",
      "Round 7 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.54it/s]\n",
      "第 7 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results/round_7/inference_results.json\n",
      "\n",
      "处理第 8 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_kl/round_8/epoch_1.pth\n",
      "Round 8 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.78it/s]\n",
      "第 8 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results/round_8/inference_results.json\n",
      "\n",
      "处理第 9 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_kl/round_9/epoch_1.pth\n",
      "Round 9 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.49it/s]\n",
      "第 9 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results/round_9/inference_results.json\n",
      "\n",
      "处理第 10 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_kl/round_10/epoch_1.pth\n",
      "Round 10 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.92it/s]\n",
      "第 10 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results/round_10/inference_results.json\n",
      "\n",
      "处理第 11 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_kl/round_11/epoch_1.pth\n",
      "Round 11 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.22it/s]\n",
      "第 11 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results/round_11/inference_results.json\n",
      "\n",
      "处理第 12 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_kl/round_12/epoch_1.pth\n",
      "Round 12 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.50it/s]\n",
      "第 12 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results/round_12/inference_results.json\n",
      "\n",
      "处理第 13 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_kl/round_13/epoch_1.pth\n",
      "Round 13 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.92it/s]\n",
      "第 13 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results/round_13/inference_results.json\n",
      "\n",
      "处理第 14 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_kl/round_14/epoch_1.pth\n",
      "Round 14 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 17.99it/s]\n",
      "第 14 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results/round_14/inference_results.json\n",
      "\n",
      "处理第 15 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_kl/round_15/epoch_1.pth\n",
      "Round 15 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.64it/s]\n",
      "第 15 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results/round_15/inference_results.json\n",
      "\n",
      "处理第 16 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_rl_kl/round_16/epoch_1.pth\n",
      "Round 16 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.97it/s]\n",
      "第 16 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results/round_16/inference_results.json\n",
      "配置 custom_config/faster-rcnn_ssc_RL_kl.py 推理完成\n",
      "\n",
      "开始处理配置: custom_config/faster-rcnn_ssc_Wasserstein.py\n",
      "执行命令: python sual/inference.py         custom_config/faster-rcnn_ssc_Wasserstein.py         --base-dir work_dirs/al_faster-rcnn_ssc_wasserstein         --img-dir data/ForestDamages/active_learning_entropy/images_labeled_val         --out-dir work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results         --device cuda:0         --score-thr 0.2         --rounds 16\n",
      "找到 16 张图片\n",
      "\n",
      "处理第 1 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_wasserstein/round_1/epoch_1.pth\n",
      "Round 1 Processing: 100%|███████████████████████| 16/16 [00:01<00:00, 11.45it/s]\n",
      "第 1 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results/round_1/inference_results.json\n",
      "\n",
      "处理第 2 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_wasserstein/round_2/epoch_1.pth\n",
      "Round 2 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.04it/s]\n",
      "第 2 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results/round_2/inference_results.json\n",
      "\n",
      "处理第 3 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_wasserstein/round_3/epoch_1.pth\n",
      "Round 3 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.00it/s]\n",
      "第 3 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results/round_3/inference_results.json\n",
      "\n",
      "处理第 4 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_wasserstein/round_4/epoch_1.pth\n",
      "Round 4 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.32it/s]\n",
      "第 4 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results/round_4/inference_results.json\n",
      "\n",
      "处理第 5 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_wasserstein/round_5/epoch_1.pth\n",
      "Round 5 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 17.89it/s]\n",
      "第 5 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results/round_5/inference_results.json\n",
      "\n",
      "处理第 6 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_wasserstein/round_6/epoch_1.pth\n",
      "Round 6 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.31it/s]\n",
      "第 6 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results/round_6/inference_results.json\n",
      "\n",
      "处理第 7 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_wasserstein/round_7/epoch_1.pth\n",
      "Round 7 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.02it/s]\n",
      "第 7 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results/round_7/inference_results.json\n",
      "\n",
      "处理第 8 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_wasserstein/round_8/epoch_1.pth\n",
      "Round 8 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.58it/s]\n",
      "第 8 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results/round_8/inference_results.json\n",
      "\n",
      "处理第 9 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_wasserstein/round_9/epoch_1.pth\n",
      "Round 9 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.01it/s]\n",
      "第 9 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results/round_9/inference_results.json\n",
      "\n",
      "处理第 10 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_wasserstein/round_10/epoch_1.pth\n",
      "Round 10 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 17.95it/s]\n",
      "第 10 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results/round_10/inference_results.json\n",
      "\n",
      "处理第 11 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_wasserstein/round_11/epoch_1.pth\n",
      "Round 11 Processing: 100%|██████████████████████| 16/16 [00:01<00:00, 15.68it/s]\n",
      "第 11 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results/round_11/inference_results.json\n",
      "\n",
      "处理第 12 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_wasserstein/round_12/epoch_1.pth\n",
      "Round 12 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.45it/s]\n",
      "第 12 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results/round_12/inference_results.json\n",
      "\n",
      "处理第 13 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_wasserstein/round_13/epoch_1.pth\n",
      "Round 13 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.49it/s]\n",
      "第 13 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results/round_13/inference_results.json\n",
      "\n",
      "处理第 14 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_wasserstein/round_14/epoch_1.pth\n",
      "Round 14 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.19it/s]\n",
      "第 14 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results/round_14/inference_results.json\n",
      "\n",
      "处理第 15 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_wasserstein/round_15/epoch_1.pth\n",
      "Round 15 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.76it/s]\n",
      "第 15 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results/round_15/inference_results.json\n",
      "\n",
      "处理第 16 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/al_faster-rcnn_ssc_wasserstein/round_16/epoch_1.pth\n",
      "Round 16 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.40it/s]\n",
      "第 16 轮结果已保存到: work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results/round_16/inference_results.json\n",
      "配置 custom_config/faster-rcnn_ssc_Wasserstein.py 推理完成\n",
      "\n",
      "开始处理配置: custom_config/faster-rcnn_basic_default.py\n",
      "执行命令: python sual/inference.py         custom_config/faster-rcnn_basic_default.py         --base-dir work_dirs/faster-rcnn_basic_default         --img-dir data/ForestDamages/active_learning_entropy/images_labeled_val         --out-dir work_dirs/faster-rcnn_basic_default/inference_results         --device cuda:0         --score-thr 0.2         --rounds 16\n",
      "找到 16 张图片\n",
      "\n",
      "处理第 1 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_basic_default/round_1/epoch_1.pth\n",
      "Round 1 Processing: 100%|███████████████████████| 16/16 [00:01<00:00, 11.64it/s]\n",
      "第 1 轮结果已保存到: work_dirs/faster-rcnn_basic_default/inference_results/round_1/inference_results.json\n",
      "\n",
      "处理第 2 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_basic_default/round_2/epoch_1.pth\n",
      "Round 2 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 17.57it/s]\n",
      "第 2 轮结果已保存到: work_dirs/faster-rcnn_basic_default/inference_results/round_2/inference_results.json\n",
      "\n",
      "处理第 3 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_basic_default/round_3/epoch_1.pth\n",
      "Round 3 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 17.16it/s]\n",
      "第 3 轮结果已保存到: work_dirs/faster-rcnn_basic_default/inference_results/round_3/inference_results.json\n",
      "\n",
      "处理第 4 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_basic_default/round_4/epoch_1.pth\n",
      "Round 4 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 17.95it/s]\n",
      "第 4 轮结果已保存到: work_dirs/faster-rcnn_basic_default/inference_results/round_4/inference_results.json\n",
      "\n",
      "处理第 5 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_basic_default/round_5/epoch_1.pth\n",
      "Round 5 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 17.85it/s]\n",
      "第 5 轮结果已保存到: work_dirs/faster-rcnn_basic_default/inference_results/round_5/inference_results.json\n",
      "\n",
      "处理第 6 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_basic_default/round_6/epoch_1.pth\n",
      "Round 6 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 17.58it/s]\n",
      "第 6 轮结果已保存到: work_dirs/faster-rcnn_basic_default/inference_results/round_6/inference_results.json\n",
      "\n",
      "处理第 7 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_basic_default/round_7/epoch_1.pth\n",
      "Round 7 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 17.65it/s]\n",
      "第 7 轮结果已保存到: work_dirs/faster-rcnn_basic_default/inference_results/round_7/inference_results.json\n",
      "\n",
      "处理第 8 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_basic_default/round_8/epoch_1.pth\n",
      "Round 8 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.06it/s]\n",
      "第 8 轮结果已保存到: work_dirs/faster-rcnn_basic_default/inference_results/round_8/inference_results.json\n",
      "\n",
      "处理第 9 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_basic_default/round_9/epoch_1.pth\n",
      "Round 9 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.21it/s]\n",
      "第 9 轮结果已保存到: work_dirs/faster-rcnn_basic_default/inference_results/round_9/inference_results.json\n",
      "\n",
      "处理第 10 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_basic_default/round_10/epoch_1.pth\n",
      "Round 10 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.39it/s]\n",
      "第 10 轮结果已保存到: work_dirs/faster-rcnn_basic_default/inference_results/round_10/inference_results.json\n",
      "\n",
      "处理第 11 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_basic_default/round_11/epoch_1.pth\n",
      "Round 11 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.51it/s]\n",
      "第 11 轮结果已保存到: work_dirs/faster-rcnn_basic_default/inference_results/round_11/inference_results.json\n",
      "\n",
      "处理第 12 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_basic_default/round_12/epoch_1.pth\n",
      "Round 12 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 17.29it/s]\n",
      "第 12 轮结果已保存到: work_dirs/faster-rcnn_basic_default/inference_results/round_12/inference_results.json\n",
      "\n",
      "处理第 13 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_basic_default/round_13/epoch_1.pth\n",
      "Round 13 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 17.49it/s]\n",
      "第 13 轮结果已保存到: work_dirs/faster-rcnn_basic_default/inference_results/round_13/inference_results.json\n",
      "\n",
      "处理第 14 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_basic_default/round_14/epoch_1.pth\n",
      "Round 14 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.69it/s]\n",
      "第 14 轮结果已保存到: work_dirs/faster-rcnn_basic_default/inference_results/round_14/inference_results.json\n",
      "\n",
      "处理第 15 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_basic_default/round_15/epoch_1.pth\n",
      "Round 15 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.92it/s]\n",
      "第 15 轮结果已保存到: work_dirs/faster-rcnn_basic_default/inference_results/round_15/inference_results.json\n",
      "\n",
      "处理第 16 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_basic_default/round_16/epoch_1.pth\n",
      "Round 16 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.93it/s]\n",
      "第 16 轮结果已保存到: work_dirs/faster-rcnn_basic_default/inference_results/round_16/inference_results.json\n",
      "配置 custom_config/faster-rcnn_basic_default.py 推理完成\n",
      "\n",
      "开始处理配置: custom_config/faster-rcnn_entropy.py\n",
      "执行命令: python sual/inference.py         custom_config/faster-rcnn_entropy.py         --base-dir work_dirs/faster-rcnn_entropy         --img-dir data/ForestDamages/active_learning_entropy/images_labeled_val         --out-dir work_dirs/faster-rcnn_entropy/inference_results         --device cuda:0         --score-thr 0.2         --rounds 16\n",
      "找到 16 张图片\n",
      "\n",
      "处理第 1 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_entropy/round_1/epoch_1.pth\n",
      "Round 1 Processing: 100%|███████████████████████| 16/16 [00:01<00:00, 11.44it/s]\n",
      "第 1 轮结果已保存到: work_dirs/faster-rcnn_entropy/inference_results/round_1/inference_results.json\n",
      "\n",
      "处理第 2 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_entropy/round_2/epoch_1.pth\n",
      "Round 2 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.52it/s]\n",
      "第 2 轮结果已保存到: work_dirs/faster-rcnn_entropy/inference_results/round_2/inference_results.json\n",
      "\n",
      "处理第 3 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_entropy/round_3/epoch_1.pth\n",
      "Round 3 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 17.72it/s]\n",
      "第 3 轮结果已保存到: work_dirs/faster-rcnn_entropy/inference_results/round_3/inference_results.json\n",
      "\n",
      "处理第 4 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_entropy/round_4/epoch_1.pth\n",
      "Round 4 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.58it/s]\n",
      "第 4 轮结果已保存到: work_dirs/faster-rcnn_entropy/inference_results/round_4/inference_results.json\n",
      "\n",
      "处理第 5 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_entropy/round_5/epoch_1.pth\n",
      "Round 5 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 17.93it/s]\n",
      "第 5 轮结果已保存到: work_dirs/faster-rcnn_entropy/inference_results/round_5/inference_results.json\n",
      "\n",
      "处理第 6 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_entropy/round_6/epoch_1.pth\n",
      "Round 6 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 17.58it/s]\n",
      "第 6 轮结果已保存到: work_dirs/faster-rcnn_entropy/inference_results/round_6/inference_results.json\n",
      "\n",
      "处理第 7 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_entropy/round_7/epoch_1.pth\n",
      "Round 7 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 17.45it/s]\n",
      "第 7 轮结果已保存到: work_dirs/faster-rcnn_entropy/inference_results/round_7/inference_results.json\n",
      "\n",
      "处理第 8 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_entropy/round_8/epoch_1.pth\n",
      "Round 8 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.11it/s]\n",
      "第 8 轮结果已保存到: work_dirs/faster-rcnn_entropy/inference_results/round_8/inference_results.json\n",
      "\n",
      "处理第 9 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_entropy/round_9/epoch_1.pth\n",
      "Round 9 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 17.33it/s]\n",
      "第 9 轮结果已保存到: work_dirs/faster-rcnn_entropy/inference_results/round_9/inference_results.json\n",
      "\n",
      "处理第 10 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_entropy/round_10/epoch_1.pth\n",
      "Round 10 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.52it/s]\n",
      "第 10 轮结果已保存到: work_dirs/faster-rcnn_entropy/inference_results/round_10/inference_results.json\n",
      "\n",
      "处理第 11 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_entropy/round_11/epoch_1.pth\n",
      "Round 11 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.85it/s]\n",
      "第 11 轮结果已保存到: work_dirs/faster-rcnn_entropy/inference_results/round_11/inference_results.json\n",
      "\n",
      "处理第 12 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_entropy/round_12/epoch_1.pth\n",
      "Round 12 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 17.91it/s]\n",
      "第 12 轮结果已保存到: work_dirs/faster-rcnn_entropy/inference_results/round_12/inference_results.json\n",
      "\n",
      "处理第 13 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_entropy/round_13/epoch_1.pth\n",
      "Round 13 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 17.97it/s]\n",
      "第 13 轮结果已保存到: work_dirs/faster-rcnn_entropy/inference_results/round_13/inference_results.json\n",
      "\n",
      "处理第 14 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_entropy/round_14/epoch_1.pth\n",
      "Round 14 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 17.14it/s]\n",
      "第 14 轮结果已保存到: work_dirs/faster-rcnn_entropy/inference_results/round_14/inference_results.json\n",
      "\n",
      "处理第 15 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_entropy/round_15/epoch_1.pth\n",
      "Round 15 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.91it/s]\n",
      "第 15 轮结果已保存到: work_dirs/faster-rcnn_entropy/inference_results/round_15/inference_results.json\n",
      "\n",
      "处理第 16 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_entropy/round_16/epoch_1.pth\n",
      "Round 16 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.36it/s]\n",
      "第 16 轮结果已保存到: work_dirs/faster-rcnn_entropy/inference_results/round_16/inference_results.json\n",
      "配置 custom_config/faster-rcnn_entropy.py 推理完成\n",
      "\n",
      "开始处理配置: custom_config/faster-rcnn_least_confid.py\n",
      "执行命令: python sual/inference.py         custom_config/faster-rcnn_least_confid.py         --base-dir work_dirs/faster-rcnn_least_confid         --img-dir data/ForestDamages/active_learning_entropy/images_labeled_val         --out-dir work_dirs/faster-rcnn_least_confid/inference_results         --device cuda:0         --score-thr 0.2         --rounds 16\n",
      "找到 16 张图片\n",
      "\n",
      "处理第 1 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_least_confid/round_1/epoch_1.pth\n",
      "Round 1 Processing: 100%|███████████████████████| 16/16 [00:01<00:00, 11.62it/s]\n",
      "第 1 轮结果已保存到: work_dirs/faster-rcnn_least_confid/inference_results/round_1/inference_results.json\n",
      "\n",
      "处理第 2 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_least_confid/round_2/epoch_1.pth\n",
      "Round 2 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.78it/s]\n",
      "第 2 轮结果已保存到: work_dirs/faster-rcnn_least_confid/inference_results/round_2/inference_results.json\n",
      "\n",
      "处理第 3 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_least_confid/round_3/epoch_1.pth\n",
      "Round 3 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 16.32it/s]\n",
      "第 3 轮结果已保存到: work_dirs/faster-rcnn_least_confid/inference_results/round_3/inference_results.json\n",
      "\n",
      "处理第 4 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_least_confid/round_4/epoch_1.pth\n",
      "Round 4 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 17.36it/s]\n",
      "第 4 轮结果已保存到: work_dirs/faster-rcnn_least_confid/inference_results/round_4/inference_results.json\n",
      "\n",
      "处理第 5 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_least_confid/round_5/epoch_1.pth\n",
      "Round 5 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.52it/s]\n",
      "第 5 轮结果已保存到: work_dirs/faster-rcnn_least_confid/inference_results/round_5/inference_results.json\n",
      "\n",
      "处理第 6 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_least_confid/round_6/epoch_1.pth\n",
      "Round 6 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.02it/s]\n",
      "第 6 轮结果已保存到: work_dirs/faster-rcnn_least_confid/inference_results/round_6/inference_results.json\n",
      "\n",
      "处理第 7 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_least_confid/round_7/epoch_1.pth\n",
      "Round 7 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.11it/s]\n",
      "第 7 轮结果已保存到: work_dirs/faster-rcnn_least_confid/inference_results/round_7/inference_results.json\n",
      "\n",
      "处理第 8 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_least_confid/round_8/epoch_1.pth\n",
      "Round 8 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.01it/s]\n",
      "第 8 轮结果已保存到: work_dirs/faster-rcnn_least_confid/inference_results/round_8/inference_results.json\n",
      "\n",
      "处理第 9 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_least_confid/round_9/epoch_1.pth\n",
      "Round 9 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.95it/s]\n",
      "第 9 轮结果已保存到: work_dirs/faster-rcnn_least_confid/inference_results/round_9/inference_results.json\n",
      "\n",
      "处理第 10 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_least_confid/round_10/epoch_1.pth\n",
      "Round 10 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.82it/s]\n",
      "第 10 轮结果已保存到: work_dirs/faster-rcnn_least_confid/inference_results/round_10/inference_results.json\n",
      "\n",
      "处理第 11 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_least_confid/round_11/epoch_1.pth\n",
      "Round 11 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.82it/s]\n",
      "第 11 轮结果已保存到: work_dirs/faster-rcnn_least_confid/inference_results/round_11/inference_results.json\n",
      "\n",
      "处理第 12 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_least_confid/round_12/epoch_1.pth\n",
      "Round 12 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.65it/s]\n",
      "第 12 轮结果已保存到: work_dirs/faster-rcnn_least_confid/inference_results/round_12/inference_results.json\n",
      "\n",
      "处理第 13 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_least_confid/round_13/epoch_1.pth\n",
      "Round 13 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.47it/s]\n",
      "第 13 轮结果已保存到: work_dirs/faster-rcnn_least_confid/inference_results/round_13/inference_results.json\n",
      "\n",
      "处理第 14 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_least_confid/round_14/epoch_1.pth\n",
      "Round 14 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.76it/s]\n",
      "第 14 轮结果已保存到: work_dirs/faster-rcnn_least_confid/inference_results/round_14/inference_results.json\n",
      "\n",
      "处理第 15 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_least_confid/round_15/epoch_1.pth\n",
      "Round 15 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 17.70it/s]\n",
      "第 15 轮结果已保存到: work_dirs/faster-rcnn_least_confid/inference_results/round_15/inference_results.json\n",
      "\n",
      "处理第 16 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_least_confid/round_16/epoch_1.pth\n",
      "Round 16 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.11it/s]\n",
      "第 16 轮结果已保存到: work_dirs/faster-rcnn_least_confid/inference_results/round_16/inference_results.json\n",
      "配置 custom_config/faster-rcnn_least_confid.py 推理完成\n",
      "\n",
      "开始处理配置: custom_config/faster-rcnn_sor.py\n",
      "执行命令: python sual/inference.py         custom_config/faster-rcnn_sor.py         --base-dir work_dirs/faster-rcnn_sor         --img-dir data/ForestDamages/active_learning_entropy/images_labeled_val         --out-dir work_dirs/faster-rcnn_sor/inference_results         --device cuda:0         --score-thr 0.2         --rounds 16\n",
      "找到 16 张图片\n",
      "\n",
      "处理第 1 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_sor/round_1/epoch_1.pth\n",
      "Round 1 Processing: 100%|███████████████████████| 16/16 [00:01<00:00, 11.88it/s]\n",
      "第 1 轮结果已保存到: work_dirs/faster-rcnn_sor/inference_results/round_1/inference_results.json\n",
      "\n",
      "处理第 2 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_sor/round_2/epoch_1.pth\n",
      "Round 2 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.11it/s]\n",
      "第 2 轮结果已保存到: work_dirs/faster-rcnn_sor/inference_results/round_2/inference_results.json\n",
      "\n",
      "处理第 3 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_sor/round_3/epoch_1.pth\n",
      "Round 3 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.39it/s]\n",
      "第 3 轮结果已保存到: work_dirs/faster-rcnn_sor/inference_results/round_3/inference_results.json\n",
      "\n",
      "处理第 4 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_sor/round_4/epoch_1.pth\n",
      "Round 4 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.86it/s]\n",
      "第 4 轮结果已保存到: work_dirs/faster-rcnn_sor/inference_results/round_4/inference_results.json\n",
      "\n",
      "处理第 5 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_sor/round_5/epoch_1.pth\n",
      "Round 5 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.44it/s]\n",
      "第 5 轮结果已保存到: work_dirs/faster-rcnn_sor/inference_results/round_5/inference_results.json\n",
      "\n",
      "处理第 6 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_sor/round_6/epoch_1.pth\n",
      "Round 6 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.40it/s]\n",
      "第 6 轮结果已保存到: work_dirs/faster-rcnn_sor/inference_results/round_6/inference_results.json\n",
      "\n",
      "处理第 7 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_sor/round_7/epoch_1.pth\n",
      "Round 7 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.10it/s]\n",
      "第 7 轮结果已保存到: work_dirs/faster-rcnn_sor/inference_results/round_7/inference_results.json\n",
      "\n",
      "处理第 8 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_sor/round_8/epoch_1.pth\n",
      "Round 8 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.60it/s]\n",
      "第 8 轮结果已保存到: work_dirs/faster-rcnn_sor/inference_results/round_8/inference_results.json\n",
      "\n",
      "处理第 9 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_sor/round_9/epoch_1.pth\n",
      "Round 9 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.20it/s]\n",
      "第 9 轮结果已保存到: work_dirs/faster-rcnn_sor/inference_results/round_9/inference_results.json\n",
      "\n",
      "处理第 10 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_sor/round_10/epoch_1.pth\n",
      "Round 10 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.73it/s]\n",
      "第 10 轮结果已保存到: work_dirs/faster-rcnn_sor/inference_results/round_10/inference_results.json\n",
      "\n",
      "处理第 11 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_sor/round_11/epoch_1.pth\n",
      "Round 11 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 20.12it/s]\n",
      "第 11 轮结果已保存到: work_dirs/faster-rcnn_sor/inference_results/round_11/inference_results.json\n",
      "\n",
      "处理第 12 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_sor/round_12/epoch_1.pth\n",
      "Round 12 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.63it/s]\n",
      "第 12 轮结果已保存到: work_dirs/faster-rcnn_sor/inference_results/round_12/inference_results.json\n",
      "\n",
      "处理第 13 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_sor/round_13/epoch_1.pth\n",
      "Round 13 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 20.12it/s]\n",
      "第 13 轮结果已保存到: work_dirs/faster-rcnn_sor/inference_results/round_13/inference_results.json\n",
      "\n",
      "处理第 14 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_sor/round_14/epoch_1.pth\n",
      "Round 14 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 20.06it/s]\n",
      "第 14 轮结果已保存到: work_dirs/faster-rcnn_sor/inference_results/round_14/inference_results.json\n",
      "\n",
      "处理第 15 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_sor/round_15/epoch_1.pth\n",
      "Round 15 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.90it/s]\n",
      "第 15 轮结果已保存到: work_dirs/faster-rcnn_sor/inference_results/round_15/inference_results.json\n",
      "\n",
      "处理第 16 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_sor/round_16/epoch_1.pth\n",
      "Round 16 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.22it/s]\n",
      "第 16 轮结果已保存到: work_dirs/faster-rcnn_sor/inference_results/round_16/inference_results.json\n",
      "配置 custom_config/faster-rcnn_sor.py 推理完成\n",
      "\n",
      "开始处理配置: custom_config/faster-rcnn_margin_default.py\n",
      "执行命令: python sual/inference.py         custom_config/faster-rcnn_margin_default.py         --base-dir work_dirs/faster-rcnn_margin_default         --img-dir data/ForestDamages/active_learning_entropy/images_labeled_val         --out-dir work_dirs/faster-rcnn_margin_default/inference_results         --device cuda:0         --score-thr 0.2         --rounds 16\n",
      "找到 16 张图片\n",
      "\n",
      "处理第 1 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_margin_default/round_1/epoch_1.pth\n",
      "Round 1 Processing: 100%|███████████████████████| 16/16 [00:01<00:00, 10.99it/s]\n",
      "第 1 轮结果已保存到: work_dirs/faster-rcnn_margin_default/inference_results/round_1/inference_results.json\n",
      "\n",
      "处理第 2 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_margin_default/round_2/epoch_1.pth\n",
      "Round 2 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.02it/s]\n",
      "第 2 轮结果已保存到: work_dirs/faster-rcnn_margin_default/inference_results/round_2/inference_results.json\n",
      "\n",
      "处理第 3 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_margin_default/round_3/epoch_1.pth\n",
      "Round 3 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.42it/s]\n",
      "第 3 轮结果已保存到: work_dirs/faster-rcnn_margin_default/inference_results/round_3/inference_results.json\n",
      "\n",
      "处理第 4 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_margin_default/round_4/epoch_1.pth\n",
      "Round 4 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.17it/s]\n",
      "第 4 轮结果已保存到: work_dirs/faster-rcnn_margin_default/inference_results/round_4/inference_results.json\n",
      "\n",
      "处理第 5 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_margin_default/round_5/epoch_1.pth\n",
      "Round 5 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.17it/s]\n",
      "第 5 轮结果已保存到: work_dirs/faster-rcnn_margin_default/inference_results/round_5/inference_results.json\n",
      "\n",
      "处理第 6 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_margin_default/round_6/epoch_1.pth\n",
      "Round 6 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.16it/s]\n",
      "第 6 轮结果已保存到: work_dirs/faster-rcnn_margin_default/inference_results/round_6/inference_results.json\n",
      "\n",
      "处理第 7 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_margin_default/round_7/epoch_1.pth\n",
      "Round 7 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.04it/s]\n",
      "第 7 轮结果已保存到: work_dirs/faster-rcnn_margin_default/inference_results/round_7/inference_results.json\n",
      "\n",
      "处理第 8 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_margin_default/round_8/epoch_1.pth\n",
      "Round 8 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 18.52it/s]\n",
      "第 8 轮结果已保存到: work_dirs/faster-rcnn_margin_default/inference_results/round_8/inference_results.json\n",
      "\n",
      "处理第 9 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_margin_default/round_9/epoch_1.pth\n",
      "Round 9 Processing: 100%|███████████████████████| 16/16 [00:00<00:00, 19.02it/s]\n",
      "第 9 轮结果已保存到: work_dirs/faster-rcnn_margin_default/inference_results/round_9/inference_results.json\n",
      "\n",
      "处理第 10 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_margin_default/round_10/epoch_1.pth\n",
      "Round 10 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.17it/s]\n",
      "第 10 轮结果已保存到: work_dirs/faster-rcnn_margin_default/inference_results/round_10/inference_results.json\n",
      "\n",
      "处理第 11 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_margin_default/round_11/epoch_1.pth\n",
      "Round 11 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.49it/s]\n",
      "第 11 轮结果已保存到: work_dirs/faster-rcnn_margin_default/inference_results/round_11/inference_results.json\n",
      "\n",
      "处理第 12 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_margin_default/round_12/epoch_1.pth\n",
      "Round 12 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.54it/s]\n",
      "第 12 轮结果已保存到: work_dirs/faster-rcnn_margin_default/inference_results/round_12/inference_results.json\n",
      "\n",
      "处理第 13 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_margin_default/round_13/epoch_1.pth\n",
      "Round 13 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.08it/s]\n",
      "第 13 轮结果已保存到: work_dirs/faster-rcnn_margin_default/inference_results/round_13/inference_results.json\n",
      "\n",
      "处理第 14 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_margin_default/round_14/epoch_1.pth\n",
      "Round 14 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.43it/s]\n",
      "第 14 轮结果已保存到: work_dirs/faster-rcnn_margin_default/inference_results/round_14/inference_results.json\n",
      "\n",
      "处理第 15 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_margin_default/round_15/epoch_1.pth\n",
      "Round 15 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 19.79it/s]\n",
      "第 15 轮结果已保存到: work_dirs/faster-rcnn_margin_default/inference_results/round_15/inference_results.json\n",
      "\n",
      "处理第 16 轮...\n",
      "Loads checkpoint by local backend from path: work_dirs/faster-rcnn_margin_default/round_16/epoch_1.pth\n",
      "Round 16 Processing: 100%|██████████████████████| 16/16 [00:00<00:00, 18.84it/s]\n",
      "第 16 轮结果已保存到: work_dirs/faster-rcnn_margin_default/inference_results/round_16/inference_results.json\n",
      "配置 custom_config/faster-rcnn_margin_default.py 推理完成\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# faster-rcnn_ssc_RL_wasserstein.py\n",
    "# faster-rcnn_ssc_RL_kl.py\n",
    "# faster-rcnn_ssc_Wasserstein.py\n",
    "# faster-rcnn_ssc_Default.py\n",
    "# faster-rcnn_ssc_combinatorial.py\n",
    "\n",
    "# !python sual/inference.py \\\n",
    "#     custom_config/faster-rcnn_ssc_combinatorial.py \\\n",
    "#     --base-dir work_dirs/al_faster-rcnn_ssc_combinatorial \\\n",
    "#     --img-dir data/ForestDamages/val2024 \\\n",
    "#     --out-dir work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results \\\n",
    "#     --device cuda:0 \\\n",
    "#     --score-thr 0.3 \\\n",
    "#     --rounds 16\n",
    "\n",
    "# 定义配置列表\n",
    "\n",
    "configs = [\n",
    "    {\n",
    "        'config': 'custom_config/faster-rcnn_ssc_combinatorial.py',\n",
    "        'base_dir': 'work_dirs/al_faster-rcnn_ssc_combinatorial',\n",
    "        'out_dir': 'work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results'\n",
    "    },\n",
    "    {\n",
    "        'config': 'custom_config/faster-rcnn_ssc_Default.py',\n",
    "        'base_dir': 'work_dirs/al_faster-rcnn_ssc_default',\n",
    "        'out_dir': 'work_dirs/al_faster-rcnn_ssc_default/inference_results'\n",
    "    },\n",
    "\n",
    "    {\n",
    "        'config': 'custom_config/faster-rcnn_ssc_RL_wasserstein.py',\n",
    "        'base_dir': 'work_dirs/al_faster-rcnn_ssc_rl_wasserstein',\n",
    "        'out_dir': 'work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results'\n",
    "    },\n",
    "    {\n",
    "        'config': 'custom_config/faster-rcnn_ssc_RL_kl.py',\n",
    "        'base_dir': 'work_dirs/al_faster-rcnn_ssc_rl_kl',\n",
    "        'out_dir': 'work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results'\n",
    "    },\n",
    "    {\n",
    "        \"config\":\"custom_config/faster-rcnn_ssc_Wasserstein.py\",\n",
    "        \"base_dir\":\"work_dirs/al_faster-rcnn_ssc_wasserstein\",\n",
    "        \"out_dir\":\"work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'config': 'custom_config/faster-rcnn_basic_default.py',\n",
    "        'base_dir': 'work_dirs/faster-rcnn_basic_default',\n",
    "        'out_dir': 'work_dirs/faster-rcnn_basic_default/inference_results'\n",
    "    },\n",
    "    {\n",
    "        'config': 'custom_config/faster-rcnn_entropy.py',\n",
    "        'base_dir': 'work_dirs/faster-rcnn_entropy',\n",
    "        'out_dir': 'work_dirs/faster-rcnn_entropy/inference_results'\n",
    "    },\n",
    "    {\n",
    "        'config': 'custom_config/faster-rcnn_least_confid.py',\n",
    "        'base_dir': 'work_dirs/faster-rcnn_least_confid',\n",
    "        'out_dir': 'work_dirs/faster-rcnn_least_confid/inference_results'\n",
    "    },\n",
    "    {\n",
    "        'config': 'custom_config/faster-rcnn_sor.py',\n",
    "        'base_dir': 'work_dirs/faster-rcnn_sor',\n",
    "        'out_dir': 'work_dirs/faster-rcnn_sor/inference_results'\n",
    "    },\n",
    "    {\n",
    "        'config': 'custom_config/faster-rcnn_margin_default.py',\n",
    "        'base_dir': 'work_dirs/faster-rcnn_margin_default',\n",
    "        'out_dir': 'work_dirs/faster-rcnn_margin_default/inference_results'\n",
    "    },\n",
    "    \n",
    "]\n",
    "\n",
    "# 遍历配置运行推理\n",
    "for cfg in configs:\n",
    "    print(f\"\\n开始处理配置: {cfg['config']}\")\n",
    "    cmd = f\"\"\"python sual/inference.py \\\n",
    "        {cfg['config']} \\\n",
    "        --base-dir {cfg['base_dir']} \\\n",
    "        --img-dir data/ForestDamages/active_learning_entropy/images_labeled_val \\\n",
    "        --out-dir {cfg['out_dir']} \\\n",
    "        --device cuda:0 \\\n",
    "        --score-thr 0.2 \\\n",
    "        --rounds 16\"\"\"\n",
    "    \n",
    "    print(f\"执行命令: {cmd}\")\n",
    "    !{cmd}\n",
    "    print(f\"配置 {cfg['config']} 推理完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.绘制各轮次teacher模型的置信度分布和置信度变化趋势"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_confidence_scores(json_file, conf_threshold=0.2):\n",
    "    \"\"\"从推理结果JSON文件中提取置信度分数\"\"\"\n",
    "    with open(json_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    all_scores = []\n",
    "    for img_name, result in results.items():\n",
    "        scores = result['predictions']['scores']\n",
    "        filtered_scores = [score for score in scores if score >= conf_threshold]\n",
    "        all_scores.extend(filtered_scores)\n",
    "    \n",
    "    return all_scores\n",
    "\n",
    "def plot_confidence_distribution(all_rounds_data, save_dir, conf_threshold=0.2):\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)  # 自动创建目录\n",
    "    print(f\"图片将保存至: {save_dir.absolute()}\")\n",
    "    \"\"\"绘制所有轮次的置信度分布图\"\"\"\n",
    "    plt.rcParams.update({\n",
    "        'figure.figsize': (20, 20),\n",
    "        'font.size': 12,\n",
    "        'axes.labelsize': 14,\n",
    "        'axes.titlesize': 16\n",
    "    })\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 4, figsize=(20, 20))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # 计算最大频率和统计信息\n",
    "    max_frequency = 0\n",
    "    rounds_stats = {\n",
    "        'mean_scores': [],\n",
    "        'median_scores': [],\n",
    "        'total_boxes': [],\n",
    "        'high_conf_ratio': []\n",
    "    }\n",
    "    \n",
    "    # 设置直方图的bins\n",
    "    bin_width = 0.05\n",
    "    bins = np.arange(conf_threshold, 1.05, bin_width)\n",
    "    \n",
    "    # 找到最大频率\n",
    "    valid_rounds = 0  # 记录有效轮次数\n",
    "    for scores in all_rounds_data:\n",
    "        if scores:\n",
    "            valid_rounds += 1\n",
    "            hist_data = np.histogram(scores, bins=bins)\n",
    "            max_frequency = max(max_frequency, np.max(hist_data[0]))\n",
    "    \n",
    "    # 绘制每轮分布图\n",
    "    for round_idx, scores in enumerate(all_rounds_data, 1):\n",
    "        ax = axes[round_idx - 1]\n",
    "        if scores:\n",
    "            n, bins, patches = ax.hist(scores, bins=bins, alpha=0.7,\n",
    "                                     color='#2878B5', edgecolor='black')\n",
    "            \n",
    "            mean_score = np.mean(scores)\n",
    "            median_score = np.median(scores)\n",
    "            total_boxes = len(scores)\n",
    "            high_conf_ratio = len([s for s in scores if s >= 0.5]) / len(scores)\n",
    "            \n",
    "            rounds_stats['mean_scores'].append(mean_score)\n",
    "            rounds_stats['median_scores'].append(median_score)\n",
    "            rounds_stats['total_boxes'].append(total_boxes)\n",
    "            rounds_stats['high_conf_ratio'].append(high_conf_ratio)\n",
    "            \n",
    "            title = (f'Round {round_idx}\\n'\n",
    "                    f'Total Boxes: {total_boxes}\\n'\n",
    "                    f'Mean: {mean_score:.3f}, Median: {median_score:.3f}\\n'\n",
    "                    f'High Conf Ratio: {high_conf_ratio:.2%}')\n",
    "            \n",
    "            ax.set_title(title)\n",
    "            ax.set_xlabel('Confidence Score')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            ax.axvline(mean_score, color='#C82423', linestyle='--', alpha=0.8,\n",
    "                      label=f'Mean: {mean_score:.3f}')\n",
    "            ax.axvline(median_score, color='#28A428', linestyle='--', alpha=0.8,\n",
    "                      label=f'Median: {median_score:.3f}')\n",
    "            \n",
    "            ax.legend()\n",
    "            ax.set_ylim(0, max_frequency * 1.1)\n",
    "        else:\n",
    "            ax.set_title(f'Round {round_idx}\\nNo Data')\n",
    "            # 添加空数据以保持数组长度一致\n",
    "            rounds_stats['mean_scores'].append(np.nan)\n",
    "            rounds_stats['median_scores'].append(np.nan)\n",
    "            rounds_stats['total_boxes'].append(0)\n",
    "            rounds_stats['high_conf_ratio'].append(np.nan)\n",
    "        \n",
    "        ax.set_xlim(conf_threshold - 0.05, 1.05)\n",
    "        ticks = np.arange(np.ceil(conf_threshold * 10) / 10, 1.1, 0.1)\n",
    "        ax.set_xticks(ticks)\n",
    "    \n",
    "    main_title = f'Confidence Score Distribution (Threshold ≥ {conf_threshold})'\n",
    "    plt.suptitle(main_title, fontsize=20, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 保存分布图\n",
    "    dist_plot_path = os.path.join(save_dir, f'confidence_distribution_thr_{conf_threshold}.jpg')\n",
    "    print(f\"分布图将保存至: {dist_plot_path}\")\n",
    "    plt.savefig(dist_plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 绘制趋势图\n",
    "    if valid_rounds > 0:  # 只在有有效数据时绘制趋势图\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        rounds_range = range(1, len(all_rounds_data) + 1)\n",
    "        \n",
    "        # 转换为numpy数组以便处理nan值\n",
    "        mean_scores = np.array(rounds_stats['mean_scores'])\n",
    "        high_conf_ratio = np.array(rounds_stats['high_conf_ratio'])\n",
    "        \n",
    "        # 只绘制非nan值\n",
    "        valid_mask = ~np.isnan(mean_scores)\n",
    "        plt.plot(np.array(list(rounds_range))[valid_mask], \n",
    "                mean_scores[valid_mask],\n",
    "                label='Mean Confidence', marker='o', color='#C82423')\n",
    "        \n",
    "        valid_mask = ~np.isnan(high_conf_ratio)\n",
    "        plt.plot(np.array(list(rounds_range))[valid_mask], \n",
    "                high_conf_ratio[valid_mask],\n",
    "                label='High Conf Ratio (≥0.5)', marker='s', color='#28A428')\n",
    "        \n",
    "        plt.xlabel('Round')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title(f'Model Performance Trend (Threshold ≥ {conf_threshold})')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        trend_plot_path = os.path.join(save_dir, f'confidence_trend_thr_{conf_threshold}.jpg')\n",
    "        plt.savefig(trend_plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    return rounds_stats\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "处理目录: work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results\n",
      "图片将保存至: /data/22_huangguobin/mmdetection-3.1.0/work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results\n",
      "分布图将保存至: work_dirs/al_faster-rcnn_ssc_rl_kl/inference_results/confidence_distribution_thr_0.2.jpg\n",
      "\n",
      "处理目录: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results\n",
      "图片将保存至: /data/22_huangguobin/mmdetection-3.1.0/work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results\n",
      "分布图将保存至: work_dirs/al_faster-rcnn_ssc_rl_wasserstein/inference_results/confidence_distribution_thr_0.2.jpg\n",
      "\n",
      "处理目录: work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results\n",
      "图片将保存至: /data/22_huangguobin/mmdetection-3.1.0/work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results\n",
      "分布图将保存至: work_dirs/al_faster-rcnn_ssc_wasserstein/inference_results/confidence_distribution_thr_0.2.jpg\n",
      "\n",
      "处理目录: work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results\n",
      "图片将保存至: /data/22_huangguobin/mmdetection-3.1.0/work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results\n",
      "分布图将保存至: work_dirs/al_faster-rcnn_ssc_combinatorial/inference_results/confidence_distribution_thr_0.2.jpg\n",
      "\n",
      "处理目录: work_dirs/al_faster-rcnn_ssc_default/inference_results\n",
      "图片将保存至: /data/22_huangguobin/mmdetection-3.1.0/work_dirs/al_faster-rcnn_ssc_default/inference_results\n",
      "分布图将保存至: work_dirs/al_faster-rcnn_ssc_default/inference_results/confidence_distribution_thr_0.2.jpg\n",
      "\n",
      "处理目录: work_dirs/faster-rcnn_margin_default/inference_results\n",
      "图片将保存至: /data/22_huangguobin/mmdetection-3.1.0/work_dirs/faster-rcnn_margin_default/inference_results\n",
      "分布图将保存至: work_dirs/faster-rcnn_margin_default/inference_results/confidence_distribution_thr_0.2.jpg\n",
      "\n",
      "处理目录: work_dirs/faster-rcnn_basic_default/inference_results\n",
      "图片将保存至: /data/22_huangguobin/mmdetection-3.1.0/work_dirs/faster-rcnn_basic_default/inference_results\n",
      "分布图将保存至: work_dirs/faster-rcnn_basic_default/inference_results/confidence_distribution_thr_0.2.jpg\n",
      "\n",
      "处理目录: work_dirs/faster-rcnn_sor/inference_results\n",
      "图片将保存至: /data/22_huangguobin/mmdetection-3.1.0/work_dirs/faster-rcnn_sor/inference_results\n",
      "分布图将保存至: work_dirs/faster-rcnn_sor/inference_results/confidence_distribution_thr_0.2.jpg\n",
      "\n",
      "处理目录: work_dirs/faster-rcnn_entropy/inference_results\n",
      "图片将保存至: /data/22_huangguobin/mmdetection-3.1.0/work_dirs/faster-rcnn_entropy/inference_results\n",
      "分布图将保存至: work_dirs/faster-rcnn_entropy/inference_results/confidence_distribution_thr_0.2.jpg\n",
      "\n",
      "处理目录: work_dirs/faster-rcnn_least_confid/inference_results\n",
      "图片将保存至: /data/22_huangguobin/mmdetection-3.1.0/work_dirs/faster-rcnn_least_confid/inference_results\n",
      "分布图将保存至: work_dirs/faster-rcnn_least_confid/inference_results/confidence_distribution_thr_0.2.jpg\n"
     ]
    }
   ],
   "source": [
    "# 分析结果\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# 遍历work_dirs下的所有目录\n",
    "work_dirs = os.listdir(\"work_dirs\")\n",
    "for work_dir in work_dirs:\n",
    "    if work_dir.startswith(\"al_faster-rcnn_ssc_\") or work_dir.startswith(\"faster-rcnn_\"):\n",
    "        inference_results_dir = f\"work_dirs/{work_dir}/inference_results\"\n",
    "        print(f\"\\n处理目录: {inference_results_dir}\")\n",
    "        conf_threshold = 0.2\n",
    "\n",
    "        # 收集所有轮次的数据\n",
    "        all_rounds_data = []\n",
    "        for round_idx in range(1, 17):\n",
    "            json_file = os.path.join(inference_results_dir, f'round_{round_idx}', 'inference_results.json')\n",
    "            if os.path.exists(json_file):\n",
    "                round_scores = extract_confidence_scores(json_file, conf_threshold)\n",
    "                all_rounds_data.append(round_scores)\n",
    "                # print(f\"Round {round_idx}: {len(round_scores)} detections above threshold {conf_threshold}\")\n",
    "            else:\n",
    "                # print(f\"Round {round_idx}: No results file found\")\n",
    "                all_rounds_data.append([])\n",
    "\n",
    "        # 绘制分布图并获取统计信息\n",
    "        stats = plot_confidence_distribution(all_rounds_data, \n",
    "                                          save_dir=inference_results_dir,\n",
    "                                          conf_threshold=conf_threshold)\n",
    "\n",
    "        # # 打印统计信息\n",
    "        # print(\"\\n统计信息概要：\")\n",
    "        # for round_idx, (mean_score, high_ratio) in enumerate(zip(stats['mean_scores'], \n",
    "        #                                                        stats['high_conf_ratio']), 1):\n",
    "        #     if not np.isnan(mean_score):\n",
    "        #         print(f\"Round {round_idx}:\")\n",
    "        #         print(f\"  平均置信度: {mean_score:.3f}\")\n",
    "        #         print(f\"  高置信度比例: {high_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.各个实验置信度变化趋势"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40961/4252195862.py:127: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  legend = ax.legend(fontsize=11,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "比较图已保存至: confidence_comparison.jpg\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_confidence_scores(json_file, conf_threshold=0.4):\n",
    "    \"\"\"\n",
    "    从推理结果JSON文件中提取置信度分数\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_file, 'r') as f:\n",
    "            results = json.load(f)\n",
    "        \n",
    "        all_scores = []\n",
    "        # 遍历每个图片的结果\n",
    "        for img_result in results.values():\n",
    "            if isinstance(img_result, dict) and 'predictions' in img_result:\n",
    "                scores = img_result['predictions'].get('scores', [])\n",
    "                filtered_scores = [score for score in scores if score >= conf_threshold]\n",
    "                all_scores.extend(filtered_scores)\n",
    "        return all_scores\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {json_file}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def compare_confidence_trends(base_dir='work_dirs', save_path='confidence_comparison.jpg'):\n",
    "    \"\"\"\n",
    "    比较不同实验的置信度趋势，使用更专业的配色和线型\n",
    "    \"\"\"\n",
    "    # 实验名称和对应的显示标签\n",
    "    # 实验名称和对应的显示标签\n",
    "    experiment_labels = {\n",
    "        # 'al_faster-rcnn_ssc_combinatorial': 'SSC Combinatorial',\n",
    "        # 'al_faster-rcnn_ssc_default': 'SSC *',\n",
    "        # 'al_faster-rcnn_ssc_rl_kl': 'SSC RL-KL',\n",
    "        # 'al_faster-rcnn_ssc_wasserstein': 'SSC Wasserstein',\n",
    "        # \"al_faster-rcnn_ssc_rl_wasserstein\": \"SSC RL-Wasserstein\",\n",
    "        \"faster-rcnn_entropy\": \"Entropy *\",\n",
    "        \"faster-rcnn_least_confid\": \"Least Confidence *\",\n",
    "        \"faster-rcnn_sor\": \"SOR *\",\n",
    "        \"faster-rcnn_margin_default\": \"Margin *\",\n",
    "        \"faster-rcnn_basic_default\": \"Basic *\"\n",
    "    }\n",
    "\n",
    "    # 专业的配色方案 - 所有颜色唯一且视觉可区分\n",
    "    colors = {\n",
    "        # 'SSC Combinatorial': '#992224',    # 深红色\n",
    "        # 'SSC *': '#4DAF4A',               # 翠绿色\n",
    "        # 'SSC RL-KL': '#7895C1',           # 柔和蓝\n",
    "        # 'SSC Wasserstein': '#8074C8',     # 紫罗兰\n",
    "        # 'SSC RL-Wasserstein': '#EFBB67',  # 暖橙色\n",
    "        'Entropy *': '#A8CBDF',           # 浅蓝\n",
    "        'Least Confidence *': '#E3625D',  # 珊瑚红\n",
    "        'SOR *': '#B54764',              # 深玫红\n",
    "        'Margin *': '#5C7A29',           # 橄榄绿 (新增)\n",
    "        'Basic *': '#E78AC3'             # 粉红色 (新增)\n",
    "    }\n",
    "\n",
    "    # 不同的线型和标记 - 修正了拼写错误并确保多样性\n",
    "    styles = {\n",
    "        # 'SSC Combinatorial': {'linestyle': '-', 'marker': 'o'},\n",
    "        # 'SSC *': {'linestyle': '--', 'marker': 's'},\n",
    "        # 'SSC RL-KL': {'linestyle': '-.', 'marker': '^'},\n",
    "        # 'SSC Wasserstein': {'linestyle': ':', 'marker': 'D'},\n",
    "        # 'SSC RL-Wasserstein': {'linestyle': '-', 'marker': 'v'},\n",
    "        'Entropy *': {'linestyle': '-', 'marker': 'o'},\n",
    "        'Least Confidence *': {'linestyle': '-', 'marker': 's'},\n",
    "        'SOR *': {'linestyle': '-', 'marker': 'D'},\n",
    "        'Margin *': {'linestyle': '--', 'marker': 'p'},  # 五边形标记\n",
    "        'Basic *': {'linestyle': ':', 'marker': '*'}\n",
    "    }\n",
    "    \n",
    "    # 使用matplotlib内置样式\n",
    "    plt.style.use('classic')\n",
    "    fig = plt.figure(figsize=(10, 6))  # 调整图形大小以适应右侧图例\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # 设置背景色和网格\n",
    "    ax.set_facecolor('#f8f9fa')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    ax.grid(True, linestyle='--', alpha=0.3, color='gray')\n",
    "    \n",
    "    # 遍历每个实验\n",
    "    for exp_dir in os.listdir(base_dir):\n",
    "        if exp_dir in experiment_labels:\n",
    "            inference_dir = os.path.join(base_dir, exp_dir, 'inference_results')\n",
    "            mean_scores = []\n",
    "            \n",
    "            # 收集每轮的平均置信度\n",
    "            for round_idx in range(1, 17):\n",
    "                json_file = os.path.join(inference_dir, f'round_{round_idx}', 'inference_results.json')\n",
    "                if os.path.exists(json_file):\n",
    "                    scores = extract_confidence_scores(json_file, conf_threshold=0.4)\n",
    "                    if scores:\n",
    "                        mean_score = np.mean(scores)\n",
    "                        mean_scores.append(mean_score)\n",
    "                    else:\n",
    "                        mean_scores.append(np.nan)\n",
    "                else:\n",
    "                    mean_scores.append(np.nan)\n",
    "            \n",
    "            # 绘制趋势线\n",
    "            rounds = range(1, len(mean_scores) + 1)\n",
    "            valid_mask = ~np.isnan(mean_scores)\n",
    "            label = experiment_labels[exp_dir]\n",
    "            \n",
    "            ax.plot(np.array(list(rounds))[valid_mask], \n",
    "                   np.array(mean_scores)[valid_mask],\n",
    "                   label=label,\n",
    "                   color=colors[label],\n",
    "                   linestyle=styles[label]['linestyle'],\n",
    "                   marker=styles[label]['marker'],\n",
    "                   linewidth=2.5,\n",
    "                   markersize=8,\n",
    "                   markerfacecolor='white',\n",
    "                   markeredgewidth=2,\n",
    "                   markeredgecolor=colors[label])\n",
    "    \n",
    "    # 优化图表样式\n",
    "    ax.set_xlabel('Active Learning Round', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Mean Confidence Score', fontsize=13, fontweight='bold')\n",
    "    ax.set_title('Comparison of Mean Confidence Scores\\nAcross Different Methods', \n",
    "                fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # 优化图例位置和样式\n",
    "    legend = ax.legend(fontsize=11, \n",
    "                      frameon=True,\n",
    "                      facecolor='white',\n",
    "                      edgecolor='lightgray',\n",
    "                      loc='upper right',  # 改为右上角\n",
    "                      bbox_to_anchor=(1, 1),\n",
    "                        ncol=2)  # 微调位置\n",
    "    \n",
    "    # 设置坐标轴范围和刻度\n",
    "    ax.set_ylim(0.3, 1.0)\n",
    "    ax.set_xlim(0.5, 16.5)\n",
    "    ax.set_xticks(range(1, 17))\n",
    "    ax.set_yticks(np.arange(0.3, 1.1, 0.1))\n",
    "    \n",
    "    # 设置刻度标签字体大小\n",
    "    ax.tick_params(axis='both', labelsize=11)\n",
    "    \n",
    "    # 移除上边框和右边框\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_linewidth(0.5)\n",
    "    ax.spines['bottom'].set_linewidth(0.5)\n",
    "    \n",
    "    # 保存图片，确保图例不被裁剪\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight', pad_inches=0.2)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"比较图已保存至: {save_path}\")\n",
    "\n",
    "# 调用函数\n",
    "compare_confidence_trends()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量测试验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python sual/batch_test.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
