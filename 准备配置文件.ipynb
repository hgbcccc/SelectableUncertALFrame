{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae89ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine import Config\n",
    "import os\n",
    "\n",
    "# 简化的配置文件修改方法\n",
    "def update_single_config(config_path):\n",
    "    \"\"\"直接通过cfg属性修改配置文件\"\"\"\n",
    "    # 加载配置文件\n",
    "    cfg = Config.fromfile(config_path)\n",
    "    \n",
    "    # 从文件名提取信息\n",
    "    filename = os.path.basename(config_path)\n",
    "    \n",
    "    # 确定模型类型\n",
    "    if 'cascade-rcnn' in filename:\n",
    "        model_type = 'cascade'\n",
    "    elif 'faster-rcnn' in filename:\n",
    "        model_type = 'faster'\n",
    "    elif 'retinanet' in filename:\n",
    "        model_type = 'retinanet'\n",
    "    else:\n",
    "        model_type = 'cascade'\n",
    "    \n",
    "    # 确定采样方法\n",
    "    sampling_methods = ['ssc', 'sor', 'random', 'entropy', 'mus_cdb', 'margin', 'least_confidence']\n",
    "    sampling_method = 'ssc'  # 默认值\n",
    "    for method in sampling_methods:\n",
    "        if method in filename:\n",
    "            sampling_method = method\n",
    "            break\n",
    "    \n",
    "    print(f\"更新配置: {filename} ({model_type}_{sampling_method})\")\n",
    "    \n",
    "    # 构建新的数据路径\n",
    "    data_root = f'data/ForestDamages/active_learning_{model_type}_{sampling_method}'\n",
    "    \n",
    "    # 直接修改cfg的属性\n",
    "    # 1. 修改数据根目录\n",
    "    cfg.data_root = data_root\n",
    "    \n",
    "    # 2. 修改训练数据加载器路径\n",
    "    cfg.train_dataloader.dataset.data_root = data_root\n",
    "    cfg.train_dataloader.dataset.ann_file = f'{data_root}/annotations/instances_labeled_train.json'\n",
    "    cfg.train_dataloader.dataset.data_prefix.img = f'{data_root}/images_labeled_train'\n",
    "    \n",
    "    # 3. 修改验证数据加载器路径  \n",
    "    cfg.val_dataloader.dataset.data_root = data_root\n",
    "    cfg.val_dataloader.dataset.ann_file = f'{data_root}/annotations/instances_labeled_val.json'\n",
    "    cfg.val_dataloader.dataset.data_prefix.img = f'{data_root}/images_labeled_val'\n",
    "    \n",
    "    # 4. 修改验证评估器路径\n",
    "    cfg.val_evaluator.ann_file = f'{data_root}/annotations/instances_labeled_val.json'\n",
    "    \n",
    "    # 5. 修改active_learning配置路径\n",
    "    cfg.active_learning.data_root = data_root\n",
    "    cfg.active_learning.ann_file = f'{data_root}/annotations/instances_unlabeled.json'\n",
    "    cfg.active_learning.data_prefix.img = f'{data_root}/images_unlabeled'\n",
    "    cfg.active_learning.train_pool_cfg.data_root = data_root\n",
    "    \n",
    "    # 6. 统一batch_size设置（重要：确保不同采样策略使用相同的batch size）\n",
    "    # 训练batch size\n",
    "    cfg.train_dataloader.batch_size = 4  # 统一设置为4，确保一致性\n",
    "    \n",
    "    # 验证batch size  \n",
    "    cfg.val_dataloader.batch_size = 4\n",
    "    \n",
    "    # 主动学习推理batch size\n",
    "    if hasattr(cfg, 'active_learning') and hasattr(cfg.active_learning, 'inference_options'):\n",
    "        cfg.active_learning.inference_options.batch_size = 8  # 推理可以稍大一些\n",
    "    \n",
    "    # 7. 设置工作目录\n",
    "    cfg.work_dir = f'work_dirs/{model_type}_{sampling_method}'\n",
    "    \n",
    "    # 保存修改后的配置\n",
    "    cfg.dump(config_path)\n",
    "    print(f\"✓ 已保存: {data_root}, batch_size=4 (统一设置)\")\n",
    "\n",
    "# 批量更新所有配置文件\n",
    "config_dir = 'al_configs'\n",
    "config_files = []\n",
    "\n",
    "# 收集所有配置文件\n",
    "for root, dirs, files in os.walk(config_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.py'):\n",
    "            config_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"找到 {len(config_files)} 个配置文件\")\n",
    "\n",
    "# 批量更新\n",
    "for config_file in sorted(config_files):\n",
    "    try:\n",
    "        update_single_config(config_file)\n",
    "    except Exception as e:\n",
    "        print(f\"✗ 更新失败 {config_file}: {e}\")\n",
    "\n",
    "print(\"批量更新完成！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27166c6",
   "metadata": {},
   "source": [
    "### 准备配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990e1921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 专门用于统一所有配置文件的batch_size设置\n",
    "def unify_batch_sizes(target_batch_size=4):\n",
    "    \"\"\"\n",
    "    统一所有配置文件的batch_size，解决不同采样策略性能不一致的问题\n",
    "    \"\"\"\n",
    "    config_dir = 'al_configs'\n",
    "    config_files = []\n",
    "    \n",
    "    # 收集所有配置文件\n",
    "    for root, dirs, files in os.walk(config_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.py'):\n",
    "                config_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"=== 统一batch_size设置为: {target_batch_size} ===\")\n",
    "    \n",
    "    for config_file in sorted(config_files):\n",
    "        try:\n",
    "            cfg = Config.fromfile(config_file)\n",
    "            \n",
    "            # 检查当前的batch_size设置\n",
    "            train_bs = cfg.train_dataloader.batch_size\n",
    "            val_bs = cfg.val_dataloader.batch_size\n",
    "            \n",
    "            # 统一设置batch_size\n",
    "            cfg.train_dataloader.batch_size = target_batch_size\n",
    "            cfg.val_dataloader.batch_size = target_batch_size\n",
    "            \n",
    "            # 如果有主动学习推理设置，也统一\n",
    "            if hasattr(cfg, 'active_learning') and hasattr(cfg.active_learning, 'inference_options'):\n",
    "                old_inference_bs = cfg.active_learning.inference_options.batch_size\n",
    "                cfg.active_learning.inference_options.batch_size = target_batch_size * 2  # 推理可以稍大\n",
    "                print(f\"{os.path.basename(config_file)}: train({train_bs}->{target_batch_size}), val({val_bs}->{target_batch_size}), inference({old_inference_bs}->{target_batch_size*2})\")\n",
    "            else:\n",
    "                print(f\"{os.path.basename(config_file)}: train({train_bs}->{target_batch_size}), val({val_bs}->{target_batch_size})\")\n",
    "            \n",
    "            # 保存配置\n",
    "            cfg.dump(config_file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ 处理失败 {config_file}: {e}\")\n",
    "    \n",
    "    print(\"=== batch_size统一完成 ===\")\n",
    "\n",
    "# 执行统一batch_size操作\n",
    "# unify_batch_sizes(4)  # 取消注释来执行\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c9821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解决mini-batch组成差异的核心问题\n",
    "def fix_minibatch_consistency():\n",
    "    \"\"\"\n",
    "    解决不同采样策略mini-batch组成不一致的问题\n",
    "    \"\"\"\n",
    "    config_dir = 'al_configs'\n",
    "    config_files = []\n",
    "    \n",
    "    # 收集所有配置文件\n",
    "    for root, dirs, files in os.walk(config_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.py'):\n",
    "                config_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(\"=== 修复mini-batch一致性问题 ===\")\n",
    "    \n",
    "    for config_file in sorted(config_files):\n",
    "        try:\n",
    "            cfg = Config.fromfile(config_file)\n",
    "            \n",
    "            filename = os.path.basename(config_file)\n",
    "            print(f\"处理: {filename}\")\n",
    "            \n",
    "            # 1. 固定随机种子 - 确保数据加载顺序一致\n",
    "            cfg.randomness = dict(\n",
    "                seed=42,                    # 固定种子\n",
    "                deterministic=True,         # 确定性行为\n",
    "                diff_rank_seed=False        # 不同rank使用相同种子\n",
    "            )\n",
    "            \n",
    "            # 2. 修改数据采样器 - 禁用shuffle确保顺序一致\n",
    "            if hasattr(cfg, 'train_dataloader') and hasattr(cfg.train_dataloader, 'sampler'):\n",
    "                cfg.train_dataloader.sampler = dict(\n",
    "                    type='DefaultSampler', \n",
    "                    shuffle=False,          # 禁用随机打乱\n",
    "                    seed=42                 # 固定种子\n",
    "                )\n",
    "            \n",
    "            # 3. 禁用AspectRatioBatchSampler - 避免按宽高比重新组合batch\n",
    "            if hasattr(cfg, 'train_dataloader'):\n",
    "                cfg.train_dataloader.batch_sampler = None  # 使用默认batch采样\n",
    "            \n",
    "            # 4. 固定数据处理pipeline中的随机性\n",
    "            if hasattr(cfg, 'train_pipeline'):\n",
    "                for i, transform in enumerate(cfg.train_pipeline):\n",
    "                    if transform.get('type') == 'RandomFlip':\n",
    "                        # 可以选择完全禁用或固定种子\n",
    "                        cfg.train_pipeline[i]['prob'] = 0.0  # 禁用随机翻转\n",
    "                    elif transform.get('type') == 'PhotoMetricDistortion':\n",
    "                        # 禁用颜色增强的随机性\n",
    "                        cfg.train_pipeline[i] = dict(type='Identity')  # 替换为无操作\n",
    "            \n",
    "            # 5. 确保验证也使用一致的设置\n",
    "            if hasattr(cfg, 'val_dataloader') and hasattr(cfg.val_dataloader, 'sampler'):\n",
    "                cfg.val_dataloader.sampler = dict(\n",
    "                    type='DefaultSampler',\n",
    "                    shuffle=False,\n",
    "                    seed=42\n",
    "                )\n",
    "            \n",
    "            print(f\"  ✓ 已固定随机种子和采样顺序\")\n",
    "            \n",
    "            # 保存配置\n",
    "            cfg.dump(config_file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ 处理失败 {config_file}: {e}\")\n",
    "    \n",
    "    print(\"=== mini-batch一致性修复完成 ===\")\n",
    "\n",
    "# 选择性解决方案：只固定种子但保留数据增强\n",
    "def fix_seeds_only():\n",
    "    \"\"\"只固定随机种子，保持数据增强的多样性\"\"\"\n",
    "    config_dir = 'al_configs'\n",
    "    config_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(config_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.py'):\n",
    "                config_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(\"=== 仅固定随机种子 ===\")\n",
    "    \n",
    "    for config_file in sorted(config_files):\n",
    "        try:\n",
    "            cfg = Config.fromfile(config_file)\n",
    "            filename = os.path.basename(config_file)\n",
    "            \n",
    "            # 固定全局随机种子\n",
    "            cfg.randomness = dict(\n",
    "                seed=42,\n",
    "                deterministic=True,\n",
    "                diff_rank_seed=False\n",
    "            )\n",
    "            \n",
    "            # 固定数据采样器种子，但保持shuffle\n",
    "            if hasattr(cfg, 'train_dataloader'):\n",
    "                cfg.train_dataloader.sampler = dict(\n",
    "                    type='DefaultSampler',\n",
    "                    shuffle=True,  # 保持shuffle但种子固定\n",
    "                    seed=42\n",
    "                )\n",
    "            \n",
    "            cfg.dump(config_file)\n",
    "            print(f\"✓ {filename}: 已固定种子\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ {filename}: {e}\")\n",
    "\n",
    "# 终极解决方案：确保所有策略使用完全相同的训练样本顺序\n",
    "def ensure_identical_training_order():\n",
    "    \"\"\"\n",
    "    确保所有采样策略在每个epoch中使用完全相同的样本顺序\n",
    "    这是最彻底的解决方案\n",
    "    \"\"\"\n",
    "    config_dir = 'al_configs'\n",
    "    \n",
    "    print(\"=== 确保训练样本顺序完全一致 ===\")\n",
    "    print(\"方法1: 使用固定的sample list\")\n",
    "    print(\"方法2: 禁用所有随机性\")\n",
    "    print(\"方法3: 使用相同的random state\")\n",
    "    \n",
    "    # 添加环境变量配置\n",
    "    env_config = \"\"\"\n",
    "# 添加到配置文件末尾\n",
    "env_cfg = dict(\n",
    "    cudnn_benchmark=False,     # 禁用cudnn benchmark确保确定性\n",
    "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),\n",
    "    dist_cfg=dict(backend='nccl')\n",
    ")\n",
    "\n",
    "# 确保完全确定性的训练\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 设置所有随机种子\n",
    "def set_all_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_all_seeds(42)\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"推荐在主训练脚本中添加以上代码确保完全确定性\")\n",
    "\n",
    "# 执行选择（取消注释需要的方案）\n",
    "# fix_minibatch_consistency()     # 最严格：禁用所有随机性\n",
    "# fix_seeds_only()                # 中等：固定种子但保持增强\n",
    "# ensure_identical_training_order() # 查看终极方案\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f47a95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主动学习专用解决方案：确保所有策略在每轮使用相同的已标注数据\n",
    "def sync_active_learning_data():\n",
    "    \"\"\"\n",
    "    主动学习场景下的特殊处理：\n",
    "    确保不同采样策略在每一轮训练中使用完全相同的已标注数据池\n",
    "    \"\"\"\n",
    "    print(\"=== 主动学习数据同步方案 ===\")\n",
    "    print(\"\"\"\n",
    "    主动学习中mini-batch差异的核心问题：\n",
    "    \n",
    "    1. 问题根源：\n",
    "       - 不同采样策略选择不同的样本进行标注\n",
    "       - 即使batch_size相同，但每轮训练的数据池不同\n",
    "       - 导致模型学习到不同的特征分布\n",
    "    \n",
    "    2. 解决方案：\n",
    "       \n",
    "       方案A: 固定初始标注池（推荐用于对比实验）\n",
    "       ├── 所有策略从相同的初始标注样本开始\n",
    "       ├── 每轮训练使用完全相同的已标注数据\n",
    "       └── 只有新选择的样本不同，但已标注部分保持一致\n",
    "       \n",
    "       方案B: 数据增强一致性\n",
    "       ├── 固定数据增强的随机种子\n",
    "       ├── 确保相同图片在不同策略中得到相同的增强\n",
    "       └── 保持训练的随机性但增强过程一致\n",
    "       \n",
    "       方案C: 批次组成控制\n",
    "       ├── 控制每个batch中不同类别样本的比例\n",
    "       ├── 确保不同策略的batch有相似的样本分布\n",
    "       └── 使用ClassAwareSampler或自定义采样器\n",
    "    \n",
    "    3. 实现建议：\n",
    "    \"\"\")\n",
    "    \n",
    "    # 为主动学习添加专用配置\n",
    "    al_config_template = '''\n",
    "# 主动学习专用配置\n",
    "active_learning = dict(\n",
    "    # 数据同步设置\n",
    "    sync_settings=dict(\n",
    "        use_fixed_initial_pool=True,        # 使用固定的初始标注池\n",
    "        initial_pool_seed=42,               # 初始池选择种子\n",
    "        batch_composition_control=True,     # 控制batch组成\n",
    "        augmentation_sync=True,             # 同步数据增强\n",
    "    ),\n",
    "    \n",
    "    # 确保一致的推理设置\n",
    "    inference_options=dict(\n",
    "        batch_size=8,\n",
    "        deterministic=True,                 # 确定性推理\n",
    "        score_thr=0.05,\n",
    "        seed=42,                           # 推理种子\n",
    "    ),\n",
    "    \n",
    "    # 样本选择同步\n",
    "    sample_selection=dict(\n",
    "        num_samples=200,\n",
    "        selection_seed=42,                  # 选择过程种子\n",
    "        use_deterministic_selection=True,   # 确定性选择\n",
    "    )\n",
    ")\n",
    "\n",
    "# 训练配置同步\n",
    "train_cfg = dict(\n",
    "    type='EpochBasedTrainLoop',\n",
    "    max_epochs=3,\n",
    "    val_interval=1,\n",
    "    # 确定性训练设置\n",
    "    deterministic=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# 随机性控制\n",
    "randomness = dict(\n",
    "    seed=42,\n",
    "    deterministic=True,\n",
    "    diff_rank_seed=False\n",
    ")\n",
    "'''\n",
    "    \n",
    "    print(\"建议在配置文件中添加以上同步设置\")\n",
    "    print(\"\\n4. 代码实现示例：\")\n",
    "    \n",
    "    code_example = '''\n",
    "# 在训练脚本中添加\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def ensure_reproducibility():\n",
    "    # 设置所有随机种子\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # 确定性设置\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # 环境变量\n",
    "    import os\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# 在每轮主动学习开始前调用\n",
    "ensure_reproducibility()\n",
    "'''\n",
    "    \n",
    "    print(code_example)\n",
    "\n",
    "# 快速修复函数：添加确定性设置到所有配置\n",
    "def add_deterministic_settings():\n",
    "    \"\"\"为所有配置文件添加确定性设置\"\"\"\n",
    "    config_dir = 'al_configs'\n",
    "    config_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(config_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.py'):\n",
    "                config_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(\"=== 添加确定性设置 ===\")\n",
    "    \n",
    "    for config_file in sorted(config_files):\n",
    "        try:\n",
    "            cfg = Config.fromfile(config_file)\n",
    "            filename = os.path.basename(config_file)\n",
    "            \n",
    "            # 添加随机性控制\n",
    "            cfg.randomness = dict(seed=42, deterministic=True, diff_rank_seed=False)\n",
    "            \n",
    "            # 修改环境配置\n",
    "            if not hasattr(cfg, 'env_cfg'):\n",
    "                cfg.env_cfg = dict()\n",
    "            cfg.env_cfg.update(dict(\n",
    "                cudnn_benchmark=False,  # 重要：禁用benchmark确保确定性\n",
    "                mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),\n",
    "                dist_cfg=dict(backend='nccl')\n",
    "            ))\n",
    "            \n",
    "            # 修改数据采样器确保一致性\n",
    "            if hasattr(cfg, 'train_dataloader'):\n",
    "                cfg.train_dataloader.sampler = dict(\n",
    "                    type='DefaultSampler', \n",
    "                    shuffle=True,  # 保持shuffle但种子固定\n",
    "                    seed=42\n",
    "                )\n",
    "                \n",
    "                # 添加worker初始化函数确保每个worker的种子一致\n",
    "                cfg.train_dataloader.worker_init_fn = 'seed_worker'\n",
    "            \n",
    "            cfg.dump(config_file)\n",
    "            print(f\"✓ {filename}: 已添加确定性设置\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ {filename}: {e}\")\n",
    "    \n",
    "    print(\"=== 确定性设置添加完成 ===\")\n",
    "\n",
    "# 执行函数\n",
    "# add_deterministic_settings()  # 取消注释执行\n",
    "# sync_active_learning_data()   # 查看详细方案\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cce8be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试单个配置文件\n",
    "config_path = 'al_configs/cascade-rcnn/cascade-rcnn_r101_ssc.py'\n",
    "\n",
    "# 加载配置文件\n",
    "cfg = Config.fromfile(config_path)\n",
    "\n",
    "# 查看关键配置信息\n",
    "print(\"=== 当前配置信息 ===\")\n",
    "print(f\"数据根目录: {cfg.data_root}\")\n",
    "print(f\"训练数据: {cfg.train_dataloader.dataset.ann_file}\")\n",
    "print(f\"验证数据: {cfg.val_dataloader.dataset.ann_file}\")\n",
    "print(f\"主动学习数据: {cfg.active_learning.ann_file}\")\n",
    "print(f\"工作目录: {getattr(cfg, 'work_dir', '未设置')}\")\n",
    "print(f\"批次大小: {cfg.train_dataloader.batch_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38dcc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置文件后处理工具\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from mmengine import Config\n",
    "\n",
    "class ConfigPostProcessor:\n",
    "    def __init__(self, base_dir=\"al_configs\"):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        \n",
    "    def extract_info_from_filename(self, filepath):\n",
    "        \"\"\"从文件名中提取模型类型和采样方法信息\"\"\"\n",
    "        filename = filepath.stem\n",
    "        \n",
    "        # 提取模型类型\n",
    "        if \"cascade-rcnn\" in filename:\n",
    "            model_type = \"cascade-rcnn\"\n",
    "        elif \"faster-rcnn\" in filename:\n",
    "            model_type = \"faster-rcnn\"\n",
    "        elif \"retinanet\" in filename:\n",
    "            model_type = \"retinanet\"\n",
    "        else:\n",
    "            model_type = \"unknown\"\n",
    "            \n",
    "        # 提取采样方法\n",
    "        sampling_methods = [\"ssc\", \"sor\", \"random\", \"entropy\", \"mus_cdb\", \"margin\", \"least_confidence\"]\n",
    "        sampling_method = \"unknown\"\n",
    "        \n",
    "        for method in sampling_methods:\n",
    "            if method in filename.replace(\"-\", \"_\").replace(\" \", \"_\"):\n",
    "                sampling_method = method\n",
    "                break\n",
    "                \n",
    "        return model_type, sampling_method\n",
    "    \n",
    "    def update_active_learning_paths(self, config_path):\n",
    "        \"\"\"更新主动学习相关的数据路径\"\"\"\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # 从文件名提取信息\n",
    "        model_type, sampling_method = self.extract_info_from_filename(config_path)\n",
    "        \n",
    "        # 生成新的路径\n",
    "        model_prefix = model_type.replace(\"-\", \"_\")\n",
    "        new_data_root = f\"data/ForestDamages/active_learning_{model_prefix}_{sampling_method}\"\n",
    "        \n",
    "        # 更新路径的正则表达式\n",
    "        patterns_replacements = [\n",
    "            # 更新 data_root\n",
    "            (r\"data_root\\s*=\\s*['\\\"][^'\\\"]*['\\\"]\", f\"data_root='{new_data_root}'\"),\n",
    "            # 更新 ann_file 中的路径\n",
    "            (r\"ann_file\\s*=\\s*['\\\"]([^'\\\"]*)/annotations/instances_unlabeled\\.json['\\\"]\",\n",
    "             f\"ann_file='{new_data_root}/annotations/instances_unlabeled.json'\"),\n",
    "            # 更新 data_prefix 中的路径\n",
    "            (r\"data_prefix\\s*=\\s*dict\\s*\\(\\s*img\\s*=\\s*['\\\"]([^'\\\"]*)/images_unlabeled['\\\"]\",\n",
    "             f\"data_prefix=dict(img='{new_data_root}/images_unlabeled'\"),\n",
    "            # 更新 train_pool_cfg 中的路径\n",
    "            (r\"(train_pool_cfg\\s*=\\s*dict\\s*\\(\\s*data_root\\s*=\\s*['\\\"])[^'\\\"]*(['\\\"])\",\n",
    "             rf\"\\g<1>{new_data_root}\\g<2>\"),\n",
    "        ]\n",
    "        \n",
    "        for pattern, replacement in patterns_replacements:\n",
    "            content = re.sub(pattern, replacement, content)\n",
    "            \n",
    "        return content\n",
    "    \n",
    "    def update_batch_sizes(self, content, model_type, sampling_method):\n",
    "        \"\"\"根据模型类型和采样方法优化batch size\"\"\"\n",
    "        \n",
    "        # 定义不同情况下的最优batch size\n",
    "        batch_size_map = {\n",
    "            (\"cascade-rcnn\", \"sor\"): 2,      # SOR方法内存需求大\n",
    "            (\"cascade-rcnn\", \"mus_cdb\"): 32, # MUS-CDB可以用较大batch\n",
    "            (\"faster-rcnn\", \"sor\"): 2,\n",
    "            (\"faster-rcnn\", \"mus_cdb\"): 32,\n",
    "            (\"retinanet\", \"sor\"): 4,         # RetinaNet相对内存友好\n",
    "            (\"retinanet\", \"mus_cdb\"): 64,\n",
    "        }\n",
    "        \n",
    "        # 默认batch size\n",
    "        default_batch_sizes = {\n",
    "            \"cascade-rcnn\": 16,\n",
    "            \"faster-rcnn\": 16, \n",
    "            \"retinanet\": 32\n",
    "        }\n",
    "        \n",
    "        # 获取合适的batch size\n",
    "        batch_size = batch_size_map.get(\n",
    "            (model_type, sampling_method), \n",
    "            default_batch_sizes.get(model_type, 16)\n",
    "        )\n",
    "        \n",
    "        # 更新batch_size\n",
    "        content = re.sub(\n",
    "            r\"batch_size\\s*=\\s*\\d+\",\n",
    "            f\"batch_size={batch_size}\",\n",
    "            content\n",
    "        )\n",
    "        \n",
    "        return content\n",
    "    \n",
    "    def update_score_thresholds(self, content, model_type):\n",
    "        \"\"\"根据模型类型调整检测阈值\"\"\"\n",
    "        \n",
    "        thresholds = {\n",
    "            \"cascade-rcnn\": {\"score\": 0.05, \"nms\": 0.5},  # Cascade R-CNN可以用更低阈值\n",
    "            \"faster-rcnn\": {\"score\": 0.08, \"nms\": 0.4},   # 标准设置\n",
    "            \"retinanet\": {\"score\": 0.1, \"nms\": 0.3}       # RetinaNet需要更高阈值\n",
    "        }\n",
    "        \n",
    "        if model_type in thresholds:\n",
    "            threshold_info = thresholds[model_type]\n",
    "            \n",
    "            # 更新 score_threshold\n",
    "            content = re.sub(\n",
    "                r\"score_threshold\\s*=\\s*[\\d.]+\",\n",
    "                f\"score_threshold = {threshold_info['score']}\",\n",
    "                content\n",
    "            )\n",
    "            \n",
    "            # 更新 nms_iou_threshold  \n",
    "            content = re.sub(\n",
    "                r\"nms_iou_threshold\\s*=\\s*[\\d.]+\",\n",
    "                f\"nms_iou_threshold = {threshold_info['nms']}\",\n",
    "                content\n",
    "            )\n",
    "            \n",
    "        return content\n",
    "    \n",
    "    def add_missing_work_dir(self, content, config_path):\n",
    "        \"\"\"添加工作目录配置\"\"\"\n",
    "        filename = config_path.stem\n",
    "        \n",
    "        if \"work_dir\" not in content:\n",
    "            work_dir = f\"work_dirs/{filename}\"\n",
    "            content += f\"\\n\\n# 工作目录\\nwork_dir = '{work_dir}'\\n\"\n",
    "            \n",
    "        return content\n",
    "    \n",
    "    def process_config_file(self, config_path):\n",
    "        \"\"\"处理单个配置文件\"\"\"\n",
    "        print(f\"🔧 处理配置文件: {config_path}\")\n",
    "        \n",
    "        # 提取模型信息\n",
    "        model_type, sampling_method = self.extract_info_from_filename(config_path)\n",
    "        print(f\"   模型: {model_type}, 采样方法: {sampling_method}\")\n",
    "        \n",
    "        # 更新路径\n",
    "        content = self.update_active_learning_paths(config_path)\n",
    "        \n",
    "        # 更新参数\n",
    "        content = self.update_batch_sizes(content, model_type, sampling_method)\n",
    "        content = self.update_score_thresholds(content, model_type)\n",
    "        content = self.add_missing_work_dir(content, config_path)\n",
    "        \n",
    "        # 保存文件\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "            \n",
    "        print(f\"   ✅ 完成\")\n",
    "        return True\n",
    "    \n",
    "    def get_all_config_files(self):\n",
    "        \"\"\"获取所有配置文件路径\"\"\"\n",
    "        config_files = []\n",
    "        model_types = [\"cascade-rcnn\", \"faster-rcnn\", \"retinanet\"]\n",
    "        \n",
    "        for model_type in model_types:\n",
    "            model_dir = self.base_dir / model_type\n",
    "            if model_dir.exists():\n",
    "                config_files.extend(list(model_dir.glob(\"*.py\")))\n",
    "        return config_files\n",
    "    \n",
    "    def process_all_configs(self):\n",
    "        \"\"\"处理所有配置文件\"\"\"\n",
    "        config_files = self.get_all_config_files()\n",
    "        \n",
    "        print(f\"🚀 找到 {len(config_files)} 个配置文件\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        success_count = 0\n",
    "        for config_file in config_files:\n",
    "            try:\n",
    "                if self.process_config_file(config_file):\n",
    "                    success_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 处理 {config_file} 时出错: {e}\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(f\"✅ 处理完成: {success_count}/{len(config_files)} 个文件成功\")\n",
    "        \n",
    "        return success_count == len(config_files)\n",
    "    \n",
    "    def validate_config(self, config_path):\n",
    "        \"\"\"验证配置文件是否可以正常加载\"\"\"\n",
    "        try:\n",
    "            cfg = Config.fromfile(str(config_path))\n",
    "            print(f\"✅ 配置文件验证成功: {config_path.name}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 配置文件验证失败: {config_path.name}, 错误: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def validate_all_configs(self):\n",
    "        \"\"\"验证所有配置文件\"\"\"\n",
    "        config_files = self.get_all_config_files()\n",
    "        \n",
    "        print(\"🔍 验证所有配置文件...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        success_count = 0\n",
    "        for config_file in config_files:\n",
    "            if self.validate_config(config_file):\n",
    "                success_count += 1\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(f\"验证完成: {success_count}/{len(config_files)} 个配置文件通过验证\")\n",
    "\n",
    "# 创建处理器实例\n",
    "processor = ConfigPostProcessor()\n",
    "print(\"✅ 配置文件后处理器初始化完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6028b8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 执行配置文件后处理\n",
    "print(\"开始处理所有配置文件...\")\n",
    "success = processor.process_all_configs()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n🎉 所有配置文件更新成功!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ 部分配置文件更新失败，请检查错误信息\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051d988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 验证配置文件加载\n",
    "print(\"验证配置文件是否可以正常加载...\")\n",
    "processor.validate_all_configs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca0fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 测试单个配置文件加载 (原有的测试代码)\n",
    "print(\"测试单个配置文件加载...\")\n",
    "\n",
    "try:\n",
    "    # 加载配置文件\n",
    "    cfg = Config.fromfile('al_configs/cascade-rcnn/cascade-rcnn_F_r101_ssc_16_200.py')\n",
    "    \n",
    "    # 打印配置信息\n",
    "    print(\"✅ 配置文件加载成功!\")\n",
    "    print(f\"模型类型: {cfg.model.type}\")\n",
    "    print(f\"数据根目录: {cfg.active_learning.data_root}\")\n",
    "    print(f\"检测阈值: {cfg.score_threshold}\")\n",
    "    print(f\"最大检测框数: {cfg.max_boxes_per_img}\")\n",
    "    \n",
    "    # 保存到文件\n",
    "    with open('generated_config.py', 'w') as f:\n",
    "        f.write(cfg.pretty_text)\n",
    "    print(\"✅ 配置文件已保存到 generated_config.py\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 配置文件加载失败: {e}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 🎯 配置文件后处理说明\n",
    "\n",
    "### 主要功能：\n",
    "1. **自动路径更新**：根据文件名自动生成正确的主动学习数据路径\n",
    "2. **智能参数调整**：\n",
    "   - SOR方法使用较小batch size (内存需求大)\n",
    "   - MUS-CDB方法使用较大batch size (可并行处理)\n",
    "   - 根据模型类型调整检测阈值\n",
    "3. **工作目录配置**：自动添加工作目录配置\n",
    "4. **验证机制**：确保处理后的配置文件可以正常加载\n",
    "\n",
    "### 参数优化策略：\n",
    "- **Cascade R-CNN**: `batch_size=16`, `score_threshold=0.05`\n",
    "- **Faster R-CNN**: `batch_size=16`, `score_threshold=0.08`\n",
    "- **RetinaNet**: `batch_size=32`, `score_threshold=0.1`\n",
    "\n",
    "### 特殊方法优化：\n",
    "- **SOR方法**: `batch_size=2` (内存友好)\n",
    "- **MUS-CDB方法**: `batch_size=32+` (高效并行)\n",
    "\n",
    "### 使用方法：\n",
    "1. 运行第一个cell初始化处理器\n",
    "2. 运行第二个cell执行批量处理\n",
    "3. 运行第三个cell验证所有配置文件\n",
    "4. 运行第四个cell测试单个配置文件加载\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "812127f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active_learning = dict(\n",
      "    ann_file=\n",
      "    'data/ForestDamages/active_learning_cascade_ssc/annotations/instances_unlabeled.json',\n",
      "    data_prefix=dict(\n",
      "        img='data/ForestDamages/active_learning_cascade_ssc/images_unlabeled'),\n",
      "    data_root='data/ForestDamages/active_learning_cascade_ssc',\n",
      "    inference_options=dict(\n",
      "        batch_size=16,\n",
      "        sample_size=0,\n",
      "        save_results=True,\n",
      "        score_thr=0.08,\n",
      "        selected_metric='ssc_score',\n",
      "        uncertainty_methods=[\n",
      "            'ssc',\n",
      "        ]),\n",
      "    max_iterations=16,\n",
      "    sample_selection=dict(\n",
      "        num_samples=200,\n",
      "        rl_metric='',\n",
      "        sample_selector='default',\n",
      "        uncertainty_metric='ssc_score'),\n",
      "    train_pool_cfg=dict(\n",
      "        ann_file='annotations/instances_labeled_train.json',\n",
      "        data_prefix=dict(img='images_labeled_train'),\n",
      "        data_root='data/ForestDamages/active_learning_cascade_ssc'))\n",
      "data_root = 'data/ForestDamages/active_learning_cascade_sor'\n",
      "dataset_type = 'ForestDamagesDataset'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(interval=1, max_keep_ckpts=3, type='CheckpointHook'),\n",
      "    logger=dict(interval=10, type='LoggerHook'),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    visualization=dict(type='DetVisualizationHook'))\n",
      "default_scope = 'mmdet'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=True,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "load_from = None\n",
      "log_level = 'INFO'\n",
      "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)\n",
      "max_boxes_per_img = 250\n",
      "metainfo = dict(\n",
      "    classes=(\n",
      "        'Aspen',\n",
      "        'Birch',\n",
      "        'Other',\n",
      "        'Pine',\n",
      "        'Spruce',\n",
      "    ),\n",
      "    palette=[\n",
      "        (\n",
      "            220,\n",
      "            20,\n",
      "            60,\n",
      "        ),\n",
      "        (\n",
      "            119,\n",
      "            11,\n",
      "            32,\n",
      "        ),\n",
      "        (\n",
      "            0,\n",
      "            0,\n",
      "            142,\n",
      "        ),\n",
      "        (\n",
      "            0,\n",
      "            0,\n",
      "            230,\n",
      "        ),\n",
      "        (\n",
      "            106,\n",
      "            0,\n",
      "            228,\n",
      "        ),\n",
      "    ])\n",
      "model = dict(\n",
      "    backbone=dict(\n",
      "        depth=101,\n",
      "        frozen_stages=1,\n",
      "        init_cfg=dict(checkpoint='torchvision://resnet101', type='Pretrained'),\n",
      "        norm_cfg=dict(requires_grad=True, type='BN'),\n",
      "        norm_eval=True,\n",
      "        num_stages=4,\n",
      "        out_indices=(\n",
      "            0,\n",
      "            1,\n",
      "            2,\n",
      "            3,\n",
      "        ),\n",
      "        style='pytorch',\n",
      "        type='ResNet'),\n",
      "    data_preprocessor=dict(\n",
      "        bgr_to_rgb=True,\n",
      "        mean=[\n",
      "            123.675,\n",
      "            116.28,\n",
      "            103.53,\n",
      "        ],\n",
      "        pad_size_divisor=32,\n",
      "        std=[\n",
      "            58.395,\n",
      "            57.12,\n",
      "            57.375,\n",
      "        ],\n",
      "        type='DetDataPreprocessor'),\n",
      "    neck=dict(\n",
      "        in_channels=[\n",
      "            256,\n",
      "            512,\n",
      "            1024,\n",
      "            2048,\n",
      "        ],\n",
      "        num_outs=5,\n",
      "        out_channels=256,\n",
      "        type='FPN'),\n",
      "    roi_head=dict(\n",
      "        bbox_head=[\n",
      "            dict(\n",
      "                bbox_coder=dict(\n",
      "                    target_means=[\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                    ],\n",
      "                    target_stds=[\n",
      "                        0.1,\n",
      "                        0.1,\n",
      "                        0.2,\n",
      "                        0.2,\n",
      "                    ],\n",
      "                    type='DeltaXYWHBBoxCoder'),\n",
      "                fc_out_channels=1024,\n",
      "                in_channels=256,\n",
      "                loss_bbox=dict(beta=1.0, loss_weight=1.0, type='SmoothL1Loss'),\n",
      "                loss_cls=dict(\n",
      "                    alpha=0.25,\n",
      "                    gamma=2.0,\n",
      "                    loss_weight=1.0,\n",
      "                    type='FocalLoss',\n",
      "                    use_sigmoid=True),\n",
      "                num_classes=5,\n",
      "                reg_class_agnostic=True,\n",
      "                roi_feat_size=7,\n",
      "                type='Shared2FCBBoxHead'),\n",
      "            dict(\n",
      "                bbox_coder=dict(\n",
      "                    target_means=[\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                    ],\n",
      "                    target_stds=[\n",
      "                        0.05,\n",
      "                        0.05,\n",
      "                        0.1,\n",
      "                        0.1,\n",
      "                    ],\n",
      "                    type='DeltaXYWHBBoxCoder'),\n",
      "                fc_out_channels=1024,\n",
      "                in_channels=256,\n",
      "                loss_bbox=dict(beta=1.0, loss_weight=1.0, type='SmoothL1Loss'),\n",
      "                loss_cls=dict(\n",
      "                    alpha=0.25,\n",
      "                    gamma=2.0,\n",
      "                    loss_weight=1.0,\n",
      "                    type='FocalLoss',\n",
      "                    use_sigmoid=True),\n",
      "                num_classes=5,\n",
      "                reg_class_agnostic=True,\n",
      "                roi_feat_size=7,\n",
      "                type='Shared2FCBBoxHead'),\n",
      "            dict(\n",
      "                bbox_coder=dict(\n",
      "                    target_means=[\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                    ],\n",
      "                    target_stds=[\n",
      "                        0.033,\n",
      "                        0.033,\n",
      "                        0.067,\n",
      "                        0.067,\n",
      "                    ],\n",
      "                    type='DeltaXYWHBBoxCoder'),\n",
      "                fc_out_channels=1024,\n",
      "                in_channels=256,\n",
      "                loss_bbox=dict(beta=1.0, loss_weight=1.0, type='SmoothL1Loss'),\n",
      "                loss_cls=dict(\n",
      "                    alpha=0.25,\n",
      "                    gamma=2.0,\n",
      "                    loss_weight=1.0,\n",
      "                    type='FocalLoss',\n",
      "                    use_sigmoid=True),\n",
      "                num_classes=5,\n",
      "                reg_class_agnostic=True,\n",
      "                roi_feat_size=7,\n",
      "                type='Shared2FCBBoxHead'),\n",
      "        ],\n",
      "        bbox_roi_extractor=dict(\n",
      "            featmap_strides=[\n",
      "                4,\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "            ],\n",
      "            out_channels=256,\n",
      "            roi_layer=dict(output_size=7, sampling_ratio=0, type='RoIAlign'),\n",
      "            type='SingleRoIExtractor'),\n",
      "        num_stages=3,\n",
      "        stage_loss_weights=[\n",
      "            1,\n",
      "            0.5,\n",
      "            0.25,\n",
      "        ],\n",
      "        type='CascadeRoIHead'),\n",
      "    rpn_head=dict(\n",
      "        anchor_generator=dict(\n",
      "            ratios=[\n",
      "                0.5,\n",
      "                1.0,\n",
      "                2.0,\n",
      "            ],\n",
      "            scales=[\n",
      "                8,\n",
      "            ],\n",
      "            strides=[\n",
      "                4,\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "                64,\n",
      "            ],\n",
      "            type='AnchorGenerator'),\n",
      "        bbox_coder=dict(\n",
      "            target_means=[\n",
      "                0.0,\n",
      "                0.0,\n",
      "                0.0,\n",
      "                0.0,\n",
      "            ],\n",
      "            target_stds=[\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "            ],\n",
      "            type='DeltaXYWHBBoxCoder'),\n",
      "        feat_channels=256,\n",
      "        in_channels=256,\n",
      "        loss_bbox=dict(loss_weight=1.0, type='L1Loss'),\n",
      "        loss_cls=dict(\n",
      "            alpha=0.25,\n",
      "            gamma=2.0,\n",
      "            loss_weight=1.0,\n",
      "            type='FocalLoss',\n",
      "            use_sigmoid=True),\n",
      "        type='RPNHead'),\n",
      "    test_cfg=dict(\n",
      "        rcnn=dict(\n",
      "            max_per_img=250,\n",
      "            nms=dict(iou_threshold=0.4, type='nms'),\n",
      "            score_thr=0.08),\n",
      "        rpn=dict(\n",
      "            max_per_img=2000,\n",
      "            min_bbox_size=0,\n",
      "            nms=dict(iou_threshold=0.6, type='nms'),\n",
      "            nms_pre=2000)),\n",
      "    train_cfg=dict(\n",
      "        rcnn=[\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    ignore_iof_thr=-1,\n",
      "                    match_low_quality=False,\n",
      "                    min_pos_iou=0.5,\n",
      "                    neg_iou_thr=0.5,\n",
      "                    pos_iou_thr=0.5,\n",
      "                    type='MaxIoUAssigner'),\n",
      "                debug=False,\n",
      "                pos_weight=-1,\n",
      "                sampler=dict(\n",
      "                    add_gt_as_proposals=True,\n",
      "                    neg_pos_ub=-1,\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    type='RandomSampler')),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    ignore_iof_thr=-1,\n",
      "                    match_low_quality=False,\n",
      "                    min_pos_iou=0.6,\n",
      "                    neg_iou_thr=0.6,\n",
      "                    pos_iou_thr=0.6,\n",
      "                    type='MaxIoUAssigner'),\n",
      "                debug=False,\n",
      "                pos_weight=-1,\n",
      "                sampler=dict(\n",
      "                    add_gt_as_proposals=True,\n",
      "                    neg_pos_ub=-1,\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    type='RandomSampler')),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    ignore_iof_thr=-1,\n",
      "                    match_low_quality=False,\n",
      "                    min_pos_iou=0.7,\n",
      "                    neg_iou_thr=0.7,\n",
      "                    pos_iou_thr=0.7,\n",
      "                    type='MaxIoUAssigner'),\n",
      "                debug=False,\n",
      "                pos_weight=-1,\n",
      "                sampler=dict(\n",
      "                    add_gt_as_proposals=True,\n",
      "                    neg_pos_ub=-1,\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    type='RandomSampler')),\n",
      "        ],\n",
      "        rpn=dict(\n",
      "            allowed_border=0,\n",
      "            assigner=dict(\n",
      "                ignore_iof_thr=-1,\n",
      "                match_low_quality=True,\n",
      "                min_pos_iou=0.3,\n",
      "                neg_iou_thr=0.3,\n",
      "                pos_iou_thr=0.7,\n",
      "                type='MaxIoUAssigner'),\n",
      "            debug=False,\n",
      "            pos_weight=-1,\n",
      "            sampler=dict(\n",
      "                add_gt_as_proposals=False,\n",
      "                neg_pos_ub=-1,\n",
      "                num=256,\n",
      "                pos_fraction=0.5,\n",
      "                type='RandomSampler')),\n",
      "        rpn_proposal=dict(\n",
      "            max_per_img=2000,\n",
      "            min_bbox_size=0,\n",
      "            nms=dict(iou_threshold=0.7, type='nms'),\n",
      "            nms_pre=2000)),\n",
      "    type='CascadeRCNN')\n",
      "nms_iou_threshold = 0.4\n",
      "optim_wrapper = dict(\n",
      "    clip_grad=dict(max_norm=35, norm_type=2),\n",
      "    optimizer=dict(\n",
      "        betas=(\n",
      "            0.9,\n",
      "            0.999,\n",
      "        ), lr=0.0001, type='AdamW', weight_decay=0.05),\n",
      "    type='OptimWrapper')\n",
      "param_scheduler = [\n",
      "    dict(\n",
      "        begin=0, by_epoch=False, end=500, start_factor=0.001, type='LinearLR'),\n",
      "    dict(\n",
      "        T_max=48,\n",
      "        begin=0,\n",
      "        by_epoch=True,\n",
      "        convert_to_iter_based=True,\n",
      "        end=48,\n",
      "        eta_min=1e-06,\n",
      "        type='CosineAnnealingLR'),\n",
      "]\n",
      "resume = False\n",
      "score_threshold = 0.08\n",
      "test_cfg = dict(type='TestLoop')\n",
      "test_dataloader = dict(\n",
      "    batch_size=4,\n",
      "    dataset=dict(\n",
      "        ann_file='annotations/instances_val2024.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='val2024/'),\n",
      "        data_root='data/ForestDamages',\n",
      "        metainfo=dict(\n",
      "            classes=(\n",
      "                'Aspen',\n",
      "                'Birch',\n",
      "                'Other',\n",
      "                'Pine',\n",
      "                'Spruce',\n",
      "            ),\n",
      "            palette=[\n",
      "                (\n",
      "                    220,\n",
      "                    20,\n",
      "                    60,\n",
      "                ),\n",
      "                (\n",
      "                    119,\n",
      "                    11,\n",
      "                    32,\n",
      "                ),\n",
      "                (\n",
      "                    0,\n",
      "                    0,\n",
      "                    142,\n",
      "                ),\n",
      "                (\n",
      "                    0,\n",
      "                    0,\n",
      "                    230,\n",
      "                ),\n",
      "                (\n",
      "                    106,\n",
      "                    0,\n",
      "                    228,\n",
      "                ),\n",
      "            ]),\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1280,\n",
      "                1280,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='ForestDamagesDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=4,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(\n",
      "    ann_file='data/ForestDamages/annotations/instances_val2024.json',\n",
      "    backend_args=None,\n",
      "    format_only=False,\n",
      "    metric='bbox',\n",
      "    type='CocoMetric')\n",
      "test_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        1536,\n",
      "        1536,\n",
      "    ), type='Resize'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'scale_factor',\n",
      "        ),\n",
      "        type='PackDetInputs'),\n",
      "]\n",
      "train_cfg = dict(max_epochs=3, type='EpochBasedTrainLoop', val_interval=1)\n",
      "train_dataloader = dict(\n",
      "    batch_sampler=dict(type='AspectRatioBatchSampler'),\n",
      "    batch_size=2,\n",
      "    dataset=dict(\n",
      "        ann_file=\n",
      "        'data/ForestDamages/active_learning_cascade_sor/annotations/instances_labeled_train.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(\n",
      "            img=\n",
      "            'data/ForestDamages/active_learning_cascade_sor/images_labeled_train'\n",
      "        ),\n",
      "        data_root='data/ForestDamages/active_learning_cascade_sor',\n",
      "        filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
      "        metainfo=dict(\n",
      "            classes=(\n",
      "                'Aspen',\n",
      "                'Birch',\n",
      "                'Other',\n",
      "                'Pine',\n",
      "                'Spruce',\n",
      "            ),\n",
      "            palette=[\n",
      "                (\n",
      "                    220,\n",
      "                    20,\n",
      "                    60,\n",
      "                ),\n",
      "                (\n",
      "                    119,\n",
      "                    11,\n",
      "                    32,\n",
      "                ),\n",
      "                (\n",
      "                    0,\n",
      "                    0,\n",
      "                    142,\n",
      "                ),\n",
      "                (\n",
      "                    0,\n",
      "                    0,\n",
      "                    230,\n",
      "                ),\n",
      "                (\n",
      "                    106,\n",
      "                    0,\n",
      "                    228,\n",
      "                ),\n",
      "            ]),\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1280,\n",
      "                1280,\n",
      "            ), type='Resize'),\n",
      "            dict(prob=0.5, type='RandomFlip'),\n",
      "            dict(type='PackDetInputs'),\n",
      "        ],\n",
      "        type='ActiveCocoDataset'),\n",
      "    num_workers=4,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
      "train_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        1280,\n",
      "        1280,\n",
      "    ), type='Resize'),\n",
      "    dict(\n",
      "        brightness_delta=32,\n",
      "        contrast_range=(\n",
      "            0.5,\n",
      "            1.5,\n",
      "        ),\n",
      "        hue_delta=18,\n",
      "        saturation_range=(\n",
      "            0.5,\n",
      "            1.5,\n",
      "        ),\n",
      "        type='PhotoMetricDistortion'),\n",
      "    dict(\n",
      "        min_crop_size=0.3,\n",
      "        min_ious=(\n",
      "            0.4,\n",
      "            0.5,\n",
      "            0.6,\n",
      "            0.7,\n",
      "        ),\n",
      "        type='MinIoURandomCrop'),\n",
      "    dict(angle_range=(\n",
      "        -20,\n",
      "        20,\n",
      "    ), type='RandomRotate'),\n",
      "    dict(prob=0.5, type='RandomFlip'),\n",
      "    dict(type='PackDetInputs'),\n",
      "]\n",
      "val_cfg = dict(type='ValLoop')\n",
      "val_dataloader = dict(\n",
      "    batch_size=4,\n",
      "    dataset=dict(\n",
      "        ann_file=\n",
      "        'data/ForestDamages/active_learning_cascade_sor/annotations/instances_labeled_val.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(\n",
      "            img=\n",
      "            'data/ForestDamages/active_learning_cascade_sor/images_labeled_val'\n",
      "        ),\n",
      "        data_root='data/ForestDamages/active_learning_cascade_sor',\n",
      "        metainfo=dict(\n",
      "            classes=(\n",
      "                'Aspen',\n",
      "                'Birch',\n",
      "                'Other',\n",
      "                'Pine',\n",
      "                'Spruce',\n",
      "            ),\n",
      "            palette=[\n",
      "                (\n",
      "                    220,\n",
      "                    20,\n",
      "                    60,\n",
      "                ),\n",
      "                (\n",
      "                    119,\n",
      "                    11,\n",
      "                    32,\n",
      "                ),\n",
      "                (\n",
      "                    0,\n",
      "                    0,\n",
      "                    142,\n",
      "                ),\n",
      "                (\n",
      "                    0,\n",
      "                    0,\n",
      "                    230,\n",
      "                ),\n",
      "                (\n",
      "                    106,\n",
      "                    0,\n",
      "                    228,\n",
      "                ),\n",
      "            ]),\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1536,\n",
      "                1536,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='ActiveCocoDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=4,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(\n",
      "    ann_file=\n",
      "    'data/ForestDamages/active_learning_cascade_sor/annotations/instances_labeled_val.json',\n",
      "    backend_args=None,\n",
      "    classwise=True,\n",
      "    format_only=False,\n",
      "    metric='bbox',\n",
      "    type='CocoMetric')\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "    dict(type='TensorboardVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='DetLocalVisualizer',\n",
      "    vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "        dict(type='TensorboardVisBackend'),\n",
      "    ])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmengine import Config\n",
    "\n",
    "# 加载配置文件\n",
    "cfg = Config.fromfile('al_configs/cascade-rcnn/cascade-rcnn_F_r101_ssc_16_200.py')\n",
    "\n",
    "    # 直接修改cfg的属性\n",
    "    # 1. 修改数据根目录\n",
    "cfg.data_root = data_root\n",
    "\n",
    "# 2. 修改训练数据加载器路径\n",
    "cfg.train_dataloader.dataset.data_root = data_root\n",
    "cfg.train_dataloader.dataset.ann_file = f'{data_root}/annotations/instances_labeled_train.json'\n",
    "cfg.train_dataloader.dataset.data_prefix.img = f'{data_root}/images_labeled_train'\n",
    "\n",
    "# 3. 修改验证数据加载器路径  \n",
    "cfg.val_dataloader.dataset.data_root = data_root\n",
    "cfg.val_dataloader.dataset.ann_file = f'{data_root}/annotations/instances_labeled_val.json'\n",
    "cfg.val_dataloader.dataset.data_prefix.img = f'{data_root}/images_labeled_val'\n",
    "\n",
    "# 4. 修改验证评估器路径\n",
    "cfg.val_evaluator.ann_file = f'{data_root}/annotations/instances_labeled_val.json'\n",
    "\n",
    "# 5. 修改active_learning配置路径\n",
    "cfg.active_learning.data_root = data_root\n",
    "cfg.active_learning.ann_file = f'{data_root}/annotations/instances_unlabeled.json'\n",
    "cfg.active_learning.data_prefix.img = f'{data_root}/images_unlabeled'\n",
    "cfg.active_learning.train_pool_cfg.data_root = data_root\n",
    "\n",
    "# 6. 设置工作目录\n",
    "cfg.work_dir = f'work_dirs/{model_type}_{sampling_method}'\n",
    "\n",
    "# 打印配置信息\n",
    "print(cfg.pretty_text)\n",
    "\n",
    "# 保存到文件\n",
    "with open('try.py', 'w') as f:\n",
    "    f.write(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55fab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "->\n",
    "\n",
    "from mmengine import Config\n",
    "import os\n",
    "\n",
    "# 简化的配置文件修改方法\n",
    "def update_single_config(config_path):\n",
    "    \"\"\"直接通过cfg属性修改配置文件\"\"\"\n",
    "    # 加载配置文件\n",
    "    cfg = Config.fromfile(config_path)\n",
    "    \n",
    "    # 从文件名提取信息\n",
    "    filename = os.path.basename(config_path)\n",
    "    \n",
    "    # 确定模型类型\n",
    "    if 'cascade-rcnn' in filename:\n",
    "        model_type = 'cascade'\n",
    "    elif 'faster-rcnn' in filename:\n",
    "        model_type = 'faster'\n",
    "    elif 'retinanet' in filename:\n",
    "        model_type = 'retinanet'\n",
    "    else:\n",
    "        model_type = 'cascade'\n",
    "    \n",
    "    # 确定采样方法\n",
    "    sampling_methods = ['ssc', 'sor', 'random', 'entropy', 'mus_cdb', 'margin', 'least_confidence']\n",
    "    sampling_method = 'ssc'  # 默认值\n",
    "    for method in sampling_methods:\n",
    "        if method in filename:\n",
    "            sampling_method = method\n",
    "            break\n",
    "    \n",
    "    print(f\"更新配置: {filename} ({model_type}_{sampling_method})\")\n",
    "    \n",
    "    # 构建新的数据路径\n",
    "    data_root = f'data/ForestDamages/active_learning_{model_type}_{sampling_method}'\n",
    "    \n",
    "    # 直接修改cfg的属性\n",
    "    # 1. 修改数据根目录\n",
    "    cfg.data_root = data_root\n",
    "    \n",
    "    # 2. 修改训练数据加载器路径\n",
    "    cfg.train_dataloader.dataset.data_root = data_root\n",
    "    cfg.train_dataloader.dataset.ann_file = f'{data_root}/annotations/instances_labeled_train.json'\n",
    "    cfg.train_dataloader.dataset.data_prefix.img = f'{data_root}/images_labeled_train'\n",
    "    \n",
    "    # 3. 修改验证数据加载器路径  \n",
    "    cfg.val_dataloader.dataset.data_root = data_root\n",
    "    cfg.val_dataloader.dataset.ann_file = f'{data_root}/annotations/instances_labeled_val.json'\n",
    "    cfg.val_dataloader.dataset.data_prefix.img = f'{data_root}/images_labeled_val'\n",
    "    \n",
    "    # 4. 修改验证评估器路径\n",
    "    cfg.val_evaluator.ann_file = f'{data_root}/annotations/instances_labeled_val.json'\n",
    "    \n",
    "    # 5. 修改active_learning配置路径\n",
    "    cfg.active_learning.data_root = data_root\n",
    "    cfg.active_learning.ann_file = f'{data_root}/annotations/instances_unlabeled.json'\n",
    "    cfg.active_learning.data_prefix.img = f'{data_root}/images_unlabeled'\n",
    "    cfg.active_learning.train_pool_cfg.data_root = data_root\n",
    "    \n",
    "    # 6. 设置工作目录\n",
    "    cfg.work_dir = f'work_dirs/{model_type}_{sampling_method}'\n",
    "    \n",
    "    # 保存修改后的配置\n",
    "    cfg.dump(config_path)\n",
    "    print(f\"✓ 已保存: {data_root}\")\n",
    "\n",
    "# 批量更新所有配置文件\n",
    "config_dir = 'al_configs'\n",
    "config_files = []\n",
    "\n",
    "# 收集所有配置文件\n",
    "for root, dirs, files in os.walk(config_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.py'):\n",
    "            config_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"找到 {len(config_files)} 个配置文件\")\n",
    "\n",
    "# 批量更新\n",
    "for config_file in sorted(config_files):\n",
    "    try:\n",
    "        update_single_config(config_file)\n",
    "    except Exception as e:\n",
    "        print(f\"✗ 更新失败 {config_file}: {e}\")\n",
    "\n",
    "print(\"批量更新完成！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
