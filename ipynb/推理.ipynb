{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is a weight file, automatically assign the model to --weights\n",
      "Loads checkpoint by local backend from path: work_dirs\\yolov3_forestdamage\\epoch_90.pth\n",
      "12/30 20:27:26 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmdet\" in the \"function\" registry tree. As a workaround, the current \"function\" registry in \"mmengine\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmdet\" is a correct scope, or whether the registry is initialized.\n",
      "\n",
      "Inference ----------------------------------------   \n",
      "Inference ----------------------------------------   \n",
      "Inference ----------------------------------------   \n",
      "Inference ----------------------------------------   \n",
      "Inference ----------------------------------------   \n",
      "e:\\sual\\mmengine\\mmengine\\visualization\\visualizer.py:760: UserWarning: \n",
      "Warning: The bbox is out of bounds, the drawn bbox may not be in the image\n",
      "  warnings.warn(\n",
      "Inference ----------------------------------------   \n",
      "e:\\sual\\mmengine\\mmengine\\visualization\\visualizer.py:831: UserWarning: \n",
      "Warning: The polygon is out of bounds, the drawn polygon may not be in the \n",
      "image\n",
      "  warnings.warn(\n",
      "Inference ----------------------------------------   \n",
      "e:\\sual\\mmengine\\mmengine\\visualization\\visualizer.py:508: UserWarning: \n",
      "Warning: The text is out of bounds, the drawn text may not be in the image\n",
      "  warnings.warn(\n",
      "Inference ----------------------------------------   \n",
      "Inference ----------------------------------------   \n",
      "Inference ----------------------------------------   \n",
      "Inference ----------------------------------------   \n",
      "results have been saved at outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\sual\\mmengine\\mmengine\\visualization\\visualizer.py:196: UserWarning: Failed to add <class 'mmengine.visualization.vis_backend.LocalVisBackend'>, please provide the `save_dir` argument.\n",
      "  warnings.warn(f'Failed to add {vis_backend.__class__}, '\n"
     ]
    }
   ],
   "source": [
    "!python image_demo.py raw\\00a788fb-24d8-4ca8-b2bf-2f241bb0df6b.jpg work_dirs\\yolov3_forestdamage\\epoch_90.pth --texts Aspen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs\\fcos_forestdamage\\epoch_25.pth\n",
      "enable_uncertainty value: True\n",
      "\n",
      "推理完成! 共处理 20 张图片\n",
      "结果保存在: 11/20250223_005420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing images:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "Processing images:   5%|▌         | 1/20 [00:03<01:10,  3.73s/it]2025-02-23 00:54:23,923 - __main__ - WARNING - Memory usage high, triggering garbage collection\n",
      "\n",
      "Processing images:  25%|██▌       | 5/20 [00:05<00:12,  1.17it/s]2025-02-23 00:54:25,315 - __main__ - WARNING - Memory usage high, triggering garbage collection\n",
      "\n",
      "Processing images:  45%|████▌     | 9/20 [00:06<00:06,  1.74it/s]2025-02-23 00:54:26,691 - __main__ - WARNING - Memory usage high, triggering garbage collection\n",
      "\n",
      "Processing images:  65%|██████▌   | 13/20 [00:07<00:03,  2.09it/s]2025-02-23 00:54:28,133 - __main__ - WARNING - Memory usage high, triggering garbage collection\n",
      "\n",
      "Processing images:  85%|████████▌ | 17/20 [00:09<00:01,  2.27it/s]\n",
      "Processing images: 100%|██████████| 20/20 [00:09<00:00,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# 使用不确定性度量模块\n",
    "!python -m sual.inference.detector \\\n",
    "    work_dirs\\fcos_forestdamage\\fcos_forestdamage.py \\\n",
    "    work_dirs\\fcos_forestdamage\\epoch_25.pth \\\n",
    "    raw \\\n",
    "    --device cuda:0 \\\n",
    "    --batch-size 4 \\\n",
    "    --num-workers 8 \\\n",
    "    --output-dir 11 \\\n",
    "    --score-thr 0.4 \\\n",
    "    --uncertainty-methods sor \n",
    "    # --enable-mc-dropout \\\n",
    "    # --mc-dropout-times 2\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rasterio\n",
    "# 读取tif文件\n",
    "tif_file_path = \"data/OAM/active_learning/images_labeled_train/5a2d4387bac48e5b1c64facc_4183.tif\"\n",
    "tmpds = rasterio.open(tif_file_path)\n",
    "tmpband = tmpds.count  # 获取波段数\n",
    "tmpband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在读取文件: data/bamberg_coco1024/annotations/备份/instances_tree_eval2023.json\n",
      "处理完成！\n",
      "处理的标注数量: 25306\n",
      "已保存到: data/bamberg_coco1024/annotations/instances_val2023.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def process_json_file(input_file, output_file):\n",
    "    \"\"\"\n",
    "    处理单个json文件，删除segmentation和area信息\n",
    "    \n",
    "    参数:\n",
    "    input_file: 输入json文件路径\n",
    "    output_file: 输出json文件路径\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 读取json文件\n",
    "        print(f\"正在读取文件: {input_file}\")\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # 记录原始标注数量\n",
    "        original_count = len(data['annotations'])\n",
    "        \n",
    "        # 处理annotations列表\n",
    "        if 'annotations' in data:\n",
    "            for annotation in data['annotations']:\n",
    "                # 删除segmentation字段\n",
    "                if 'segmentation' in annotation:\n",
    "                    del annotation['segmentation']\n",
    "        \n",
    "        # 保存处理后的文件\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"处理完成！\")\n",
    "        print(f\"处理的标注数量: {original_count}\")\n",
    "        print(f\"已保存到: {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"处理文件时出错: {str(e)}\")\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"data/bamberg_coco1024/annotations/备份/instances_tree_eval2023.json\"  # 输入json文件\n",
    "    output_file = \"data/bamberg_coco1024/annotations/instances_val2023.json\"  # 输出json文件\n",
    "    \n",
    "    process_json_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并数据集...\n",
      "数据根目录: E:\\SUAL\\SUAL\\data\\ForestDamages\n",
      "训练集标注文件: E:\\SUAL\\SUAL\\data\\ForestDamages\\annotations\\instances_train2024.json\n",
      "验证集标注文件: E:\\SUAL\\SUAL\\data\\ForestDamages\\annotations\\instances_val2024.json\n",
      "训练集图片目录: E:\\SUAL\\SUAL\\data\\ForestDamages\\train2024\n",
      "验证集图片目录: E:\\SUAL\\SUAL\\data\\ForestDamages\\val2024\n",
      "合并后总图像数: 1692\n",
      "合并后总标注数: 94615\n",
      "\n",
      "数据集分割:\n",
      "总样本数: 1692\n",
      "训练集大小: 67 (4.0%)\n",
      "验证集大小: 16 (1.0%)\n",
      "未标注集大小: 1609 (95.0%)\n",
      "\n",
      "处理 labeled_train 数据集...\n",
      "从以下目录复制图片:\n",
      "  - E:\\SUAL\\SUAL\\data\\ForestDamages\\train2024\n",
      "  - E:\\SUAL\\SUAL\\data\\ForestDamages\\val2024\n",
      "成功复制 67 张图片到 data\\ForestDamages\\active_learning\\images_labeled_train\n",
      "保存标注到: data\\ForestDamages\\active_learning\\annotations\\instances_labeled_train.json\n",
      "图像数量: 67\n",
      "标注数量: 3774\n",
      "\n",
      "处理 labeled_val 数据集...\n",
      "从以下目录复制图片:\n",
      "  - E:\\SUAL\\SUAL\\data\\ForestDamages\\train2024\n",
      "  - E:\\SUAL\\SUAL\\data\\ForestDamages\\val2024\n",
      "成功复制 16 张图片到 data\\ForestDamages\\active_learning\\images_labeled_val\n",
      "保存标注到: data\\ForestDamages\\active_learning\\annotations\\instances_labeled_val.json\n",
      "图像数量: 16\n",
      "标注数量: 1061\n",
      "\n",
      "处理 unlabeled 数据集...\n",
      "从以下目录复制图片:\n",
      "  - E:\\SUAL\\SUAL\\data\\ForestDamages\\train2024\n",
      "  - E:\\SUAL\\SUAL\\data\\ForestDamages\\val2024\n",
      "成功复制 1609 张图片到 data\\ForestDamages\\active_learning\\images_unlabeled\n",
      "保存标注到: data\\ForestDamages\\active_learning\\annotations\\instances_unlabeled.json\n",
      "图像数量: 1609\n",
      "标注数量: 89780\n",
      "\n",
      "数据集准备完成!\n",
      "\n",
      "目录结构:\n",
      "data/ForestDamages/active_learning/\n",
      "├── images_labeled_train/\n",
      "├── images_labeled_val/\n",
      "├── images_unlabeled/\n",
      "└── annotations/\n",
      "    ├── instances_labeled_train.json\n",
      "    ├── instances_labeled_val.json\n",
      "    └── instances_unlabeled.json\n"
     ]
    }
   ],
   "source": [
    "!python ./tools/prepare_active_dataset.py \\\n",
    "    data/ForestDamages \\\n",
    "    data/ForestDamages/active_learning \\\n",
    "    --train-ratio 0.04 \\\n",
    "    --val-ratio 0.01 \\\n",
    "    --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已随机保留13张图片到: data/bamberg_coco1024/active_learning/images_labeled_val_mini\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "def keep_n_images(input_folder, n, output_folder=None):\n",
    "    \"\"\"\n",
    "    随机保留文件夹中的n张图片\n",
    "    \n",
    "    参数:\n",
    "    input_folder: 输入图片文件夹路径\n",
    "    n: 要保留的图片数量\n",
    "    output_folder: 输出文件夹路径（可选，如果不指定则在原文件夹中删除）\n",
    "    \"\"\"\n",
    "    # 获取所有tif文件\n",
    "    image_files = list(Path(input_folder).glob('*.tif'))\n",
    "    total_images = len(image_files)\n",
    "    \n",
    "    # 检查n的值是否合理\n",
    "    if n >= total_images:\n",
    "        print(f\"要保留的数量({n})大于或等于现有图片数量({total_images})，无需处理\")\n",
    "        return\n",
    "    \n",
    "    # 随机选择要保留的文件\n",
    "    keep_files = random.sample(image_files, n)\n",
    "    \n",
    "    if output_folder:\n",
    "        # 如果指定了输出文件夹，创建并复制文件\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        for file in keep_files:\n",
    "            shutil.copy2(file, Path(output_folder) / file.name)\n",
    "        print(f\"已随机保留{n}张图片到: {output_folder}\")\n",
    "    else:\n",
    "        # 如果没有指定输出文件夹，删除未被选中的文件\n",
    "        delete_files = set(image_files) - set(keep_files)\n",
    "        for file in delete_files:\n",
    "            os.remove(file)\n",
    "        print(f\"已随机保留{n}张图片，删除了{len(delete_files)}张图片\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"data/bamberg_coco1024/active_learning/images_labeled_val\"  # 输入图片文件夹\n",
    "    n = 13  # 要保留的图片数量\n",
    "    output_folder = \"data/bamberg_coco1024/active_learning/images_labeled_val_mini\"  # 输出文件夹（可选）\n",
    "    \n",
    "    keep_n_images(input_folder, n, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当训练集为\\(50\\)张时，验证集约为\\(13\\)张，未标注集约为\\(563\\)张。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "配置验证通过:\n",
      "- max_iterations: 10\n",
      "- samples_per_iteration: 20\n",
      "02/10 18:52:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "开始第 1/10 轮主动学习...\n",
      "02/10 18:52:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: win32\n",
      "    Python: 3.9.20 (main, Oct  3 2024, 07:38:01) [MSC v.1929 64 bit (AMD64)]\n",
      "    CUDA available: True\n",
      "    MUSA available: False\n",
      "    numpy_random_seed: 1108712915\n",
      "    GPU 0: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "    CUDA_HOME: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.0\n",
      "    NVCC: Cuda compilation tools, release 12.0, V12.0.76\n",
      "    MSVC: 用于 x64 的 Microsoft (R) C/C++ 优化编译器 19.41.34123 版\n",
      "    GCC: n/a\n",
      "    PyTorch: 2.0.1+cu118\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - C++ Version: 199711\n",
      "  - MSVC 193431937\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)\n",
      "  - OpenMP 2019\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.8\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_37,code=compute_37\n",
      "  - CuDNN 8.7\n",
      "  - Magma 2.5.4\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=C:/actions-runner/_work/pytorch/pytorch/builder/windows/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj /FS -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=OFF, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.15.2+cu118\n",
      "    OpenCV: 4.10.0\n",
      "    MMEngine: 0.10.5\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: False\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
      "    seed: 1108712915\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "02/10 18:52:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
      "active_learning = dict(\n",
      "    dataset=dict(\n",
      "        data_root='data/bamberg_coco1024/active_learning/',\n",
      "        distance_metric='euclidean',\n",
      "        sampling_strategy='max_min_distance',\n",
      "        spectral_indices=[\n",
      "            'ndvi',\n",
      "            'ndwi',\n",
      "            'evi',\n",
      "        ],\n",
      "        type='MultispectralActiveDataset'),\n",
      "    feature_extractor=dict(\n",
      "        index_weights=dict(evi=1.0, ndvi=1.0, ndwi=1.0),\n",
      "        selected_indices=[\n",
      "            'ndvi',\n",
      "            'ndwi',\n",
      "            'evi',\n",
      "        ],\n",
      "        type='SpectralFeatureExtractor'),\n",
      "    loop=dict(max_rounds=2, samples_per_round=10),\n",
      "    max_iterations=10,\n",
      "    samples_per_iteration=20,\n",
      "    sampling_strategy=dict(\n",
      "        distance_metric='euclidean',\n",
      "        strategy='max_min_distance',\n",
      "        type='SpectralSamplingStrategy'))\n",
      "auto_scale_lr = dict(base_batch_size=4, enable=False)\n",
      "backend_args = None\n",
      "custom_imports = dict(\n",
      "    allow_failed_imports=False,\n",
      "    imports=[\n",
      "        'sual.core.datasets.multispectral_active_dataset',\n",
      "        'sual.core.uncertainty.spectral_metrics',\n",
      "        'sual.core.datasets.transforms.preprocessor',\n",
      "    ])\n",
      "data_preprocessor = dict(\n",
      "    bgr_to_rgb=True,\n",
      "    device='cuda:0',\n",
      "    mean=[\n",
      "        123.675,\n",
      "        116.28,\n",
      "        103.53,\n",
      "        115.5,\n",
      "    ],\n",
      "    num_channels=4,\n",
      "    pad_size_divisor=32,\n",
      "    pad_value=0,\n",
      "    std=[\n",
      "        58.395,\n",
      "        57.12,\n",
      "        57.375,\n",
      "        57.8,\n",
      "    ],\n",
      "    type='MultiSpectralDetDataPreprocessor')\n",
      "data_root = 'e:\\\\SUAL\\\\SUAL\\\\data\\\\bamberg_coco1024\\\\active_learning'\n",
      "dataset_type = 'BambergDataset'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(interval=1, max_keep_ckpts=3, type='CheckpointHook'),\n",
      "    logger=dict(interval=20, type='LoggerHook'),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    visualization=dict(type='DetVisualizationHook'))\n",
      "default_scope = 'mmdet'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=False,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "find_unused_parameters = True\n",
      "input_size = (\n",
      "    512,\n",
      "    512,\n",
      ")\n",
      "load_from = None\n",
      "log_level = 'INFO'\n",
      "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)\n",
      "model = dict(\n",
      "    backbone=dict(\n",
      "        act_cfg=dict(negative_slope=0.1, type='LeakyReLU'),\n",
      "        init_cfg=dict(\n",
      "            checkpoint='open-mmlab://mmdet/mobilenet_v2', type='Pretrained'),\n",
      "        out_indices=(\n",
      "            2,\n",
      "            4,\n",
      "            6,\n",
      "        ),\n",
      "        type='MobileNetV2',\n",
      "        widen_factor=1.0),\n",
      "    bbox_head=dict(\n",
      "        anchor_generator=dict(\n",
      "            base_sizes=[\n",
      "                [\n",
      "                    (\n",
      "                        220,\n",
      "                        125,\n",
      "                    ),\n",
      "                    (\n",
      "                        128,\n",
      "                        222,\n",
      "                    ),\n",
      "                    (\n",
      "                        264,\n",
      "                        266,\n",
      "                    ),\n",
      "                ],\n",
      "                [\n",
      "                    (\n",
      "                        35,\n",
      "                        87,\n",
      "                    ),\n",
      "                    (\n",
      "                        102,\n",
      "                        96,\n",
      "                    ),\n",
      "                    (\n",
      "                        60,\n",
      "                        240,\n",
      "                    ),\n",
      "                ],\n",
      "                [\n",
      "                    (\n",
      "                        10,\n",
      "                        15,\n",
      "                    ),\n",
      "                    (\n",
      "                        24,\n",
      "                        36,\n",
      "                    ),\n",
      "                    (\n",
      "                        72,\n",
      "                        42,\n",
      "                    ),\n",
      "                ],\n",
      "            ],\n",
      "            strides=[\n",
      "                32,\n",
      "                16,\n",
      "                8,\n",
      "            ],\n",
      "            type='YOLOAnchorGenerator'),\n",
      "        bbox_coder=dict(type='YOLOBBoxCoder'),\n",
      "        featmap_strides=[\n",
      "            32,\n",
      "            16,\n",
      "            8,\n",
      "        ],\n",
      "        in_channels=[\n",
      "            96,\n",
      "            96,\n",
      "            96,\n",
      "        ],\n",
      "        loss_cls=dict(\n",
      "            loss_weight=1.0,\n",
      "            reduction='sum',\n",
      "            type='CrossEntropyLoss',\n",
      "            use_sigmoid=True),\n",
      "        loss_conf=dict(\n",
      "            loss_weight=1.0,\n",
      "            reduction='sum',\n",
      "            type='CrossEntropyLoss',\n",
      "            use_sigmoid=True),\n",
      "        loss_wh=dict(loss_weight=2.0, reduction='sum', type='MSELoss'),\n",
      "        loss_xy=dict(\n",
      "            loss_weight=2.0,\n",
      "            reduction='sum',\n",
      "            type='CrossEntropyLoss',\n",
      "            use_sigmoid=True),\n",
      "        num_classes=1,\n",
      "        out_channels=[\n",
      "            96,\n",
      "            96,\n",
      "            96,\n",
      "        ],\n",
      "        type='YOLOV3Head'),\n",
      "    data_preprocessor=dict(\n",
      "        bgr_to_rgb=True,\n",
      "        device='cuda:0',\n",
      "        mean=[\n",
      "            123.675,\n",
      "            116.28,\n",
      "            103.53,\n",
      "            115.5,\n",
      "        ],\n",
      "        num_channels=4,\n",
      "        pad_size_divisor=32,\n",
      "        pad_value=0,\n",
      "        std=[\n",
      "            58.395,\n",
      "            57.12,\n",
      "            57.375,\n",
      "            57.8,\n",
      "        ],\n",
      "        type='MultiSpectralDetDataPreprocessor'),\n",
      "    neck=dict(\n",
      "        in_channels=[\n",
      "            320,\n",
      "            96,\n",
      "            32,\n",
      "        ],\n",
      "        num_scales=3,\n",
      "        out_channels=[\n",
      "            96,\n",
      "            96,\n",
      "            96,\n",
      "        ],\n",
      "        type='YOLOV3Neck'),\n",
      "    test_cfg=dict(\n",
      "        conf_thr=0.005,\n",
      "        max_per_img=100,\n",
      "        min_bbox_size=0,\n",
      "        nms=dict(iou_threshold=0.3, type='nms'),\n",
      "        nms_pre=1000,\n",
      "        score_thr=0.05),\n",
      "    train_cfg=dict(\n",
      "        assigner=dict(\n",
      "            min_pos_iou=0,\n",
      "            neg_iou_thr=0.5,\n",
      "            pos_iou_thr=0.5,\n",
      "            type='GridAssigner')),\n",
      "    type='YOLOV3')\n",
      "optim_wrapper = dict(\n",
      "    clip_grad=dict(max_norm=35, norm_type=2),\n",
      "    optimizer=dict(lr=0.003, momentum=0.9, type='SGD', weight_decay=0.005),\n",
      "    type='OptimWrapper')\n",
      "param_scheduler = [\n",
      "    dict(begin=0, by_epoch=False, end=200, start_factor=0.01, type='LinearLR'),\n",
      "    dict(by_epoch=True, gamma=0.1, milestones=[\n",
      "        5,\n",
      "        7,\n",
      "    ], type='MultiStepLR'),\n",
      "]\n",
      "resume = True\n",
      "test_cfg = dict(type='TestLoop')\n",
      "test_dataloader = dict(\n",
      "    batch_size=4,\n",
      "    dataset=dict(\n",
      "        ann_file='annotations/instances_labeled_val.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='images_labeled_val/'),\n",
      "        data_root='e:\\\\SUAL\\\\SUAL\\\\data\\\\bamberg_coco1024\\\\active_learning',\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadMultiSpectralTiffFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                320,\n",
      "                320,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='BambergDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=4,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(\n",
      "    ann_file=\n",
      "    'data/bamberg_coco1024/active_learning/annotations/instances_val2024.json',\n",
      "    backend_args=None,\n",
      "    metric='bbox',\n",
      "    type='CocoMetric')\n",
      "test_pipeline = [\n",
      "    dict(backend_args=None, type='LoadMultiSpectralTiffFromFile'),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        512,\n",
      "        512,\n",
      "    ), type='Resize'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'scale_factor',\n",
      "        ),\n",
      "        type='PackDetInputs'),\n",
      "]\n",
      "train_cfg = dict(max_epochs=2, type='EpochBasedTrainLoop', val_interval=1)\n",
      "train_dataloader = dict(\n",
      "    batch_sampler=dict(type='AspectRatioBatchSampler'),\n",
      "    batch_size=2,\n",
      "    dataset=dict(\n",
      "        dataset=dict(\n",
      "            ann_file='annotations/instances_labeled_train.json',\n",
      "            backend_args=None,\n",
      "            data_prefix=dict(img='images_labeled_train'),\n",
      "            data_root='e:\\\\SUAL\\\\SUAL\\\\data\\\\bamberg_coco1024\\\\active_learning',\n",
      "            filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
      "            pipeline=[\n",
      "                dict(backend_args=None, type='LoadMultiSpectralTiffFromFile'),\n",
      "                dict(type='LoadAnnotations', with_bbox=True),\n",
      "                dict(\n",
      "                    mean=[\n",
      "                        123.675,\n",
      "                        116.28,\n",
      "                        103.53,\n",
      "                        115.5,\n",
      "                    ],\n",
      "                    ratio_range=(\n",
      "                        1,\n",
      "                        2,\n",
      "                    ),\n",
      "                    to_rgb=True,\n",
      "                    type='Expand'),\n",
      "                dict(\n",
      "                    min_crop_size=0.3,\n",
      "                    min_ious=(\n",
      "                        0.4,\n",
      "                        0.5,\n",
      "                        0.6,\n",
      "                        0.7,\n",
      "                        0.8,\n",
      "                        0.9,\n",
      "                    ),\n",
      "                    type='MinIoURandomCrop'),\n",
      "                dict(keep_ratio=True, scale=(\n",
      "                    320,\n",
      "                    320,\n",
      "                ), type='Resize'),\n",
      "                dict(prob=0.5, type='RandomFlip'),\n",
      "                dict(type='PhotoMetricDistortion'),\n",
      "                dict(type='PackDetInputs'),\n",
      "            ],\n",
      "            type='BambergDataset'),\n",
      "        times=1,\n",
      "        type='RepeatDataset'),\n",
      "    num_workers=8,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
      "train_pipeline = [\n",
      "    dict(backend_args=None, type='LoadMultiSpectralTiffFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(\n",
      "        mean=[\n",
      "            123.675,\n",
      "            116.28,\n",
      "            103.53,\n",
      "            115.5,\n",
      "        ],\n",
      "        ratio_range=(\n",
      "            1,\n",
      "            2,\n",
      "        ),\n",
      "        to_rgb=True,\n",
      "        type='Expand'),\n",
      "    dict(\n",
      "        min_crop_size=0.3,\n",
      "        min_ious=(\n",
      "            0.4,\n",
      "            0.5,\n",
      "            0.6,\n",
      "            0.7,\n",
      "            0.8,\n",
      "            0.9,\n",
      "        ),\n",
      "        type='MinIoURandomCrop'),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        512,\n",
      "        512,\n",
      "    ), type='Resize'),\n",
      "    dict(prob=0.5, type='RandomFlip'),\n",
      "    dict(type='PhotoMetricDistortion'),\n",
      "    dict(type='PackDetInputs'),\n",
      "]\n",
      "val_cfg = dict(type='ValLoop')\n",
      "val_dataloader = dict(\n",
      "    batch_size=2,\n",
      "    dataset=dict(\n",
      "        ann_file='annotations\\\\instances_labeled_val.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='images_labeled_val/'),\n",
      "        data_root='e:\\\\SUAL\\\\SUAL\\\\data\\\\bamberg_coco1024\\\\active_learning',\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadMultiSpectralTiffFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                512,\n",
      "                512,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='ForestDamagesDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=8,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(\n",
      "    ann_file=\n",
      "    'data/bamberg_coco1024/active_learning/annotations/instances_labeled_val.json',\n",
      "    backend_args=None,\n",
      "    metric='bbox',\n",
      "    type='CocoMetric')\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='DetLocalVisualizer',\n",
      "    vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "    ])\n",
      "work_dir = 'work_dirs\\\\mutilspace_yolov3\\\\round_1'\n",
      "\n",
      "02/10 18:52:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "02/10 18:52:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "02/10 18:52:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - load model from: open-mmlab://mmdet/mobilenet_v2\n",
      "02/10 18:52:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Loads checkpoint by openmmlab backend from path: open-mmlab://mmdet/mobilenet_v2\n",
      "Did not find last_checkpoint to be resumed.\n",
      "02/10 18:52:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Auto resumed from the latest checkpoint None.\n",
      "02/10 18:52:22 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "02/10 18:52:22 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "02/10 18:52:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to e:\\SUAL\\SUAL\\work_dirs\\mutilspace_yolov3\\round_1.\n",
      "02/10 18:52:48 - mmengine - \u001b[5m\u001b[4m\u001b[31mERROR\u001b[0m - e:\\SUAL\\SUAL\\tools\\multispectral_al_train.py - main - 517 - 训练过程出错: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "!python tools/multispectral_al_train.py al_yolov3_mutilspac.py --work-dir  work_dirs/mutilspace_yolov3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已完成数据集合并和采样：\n",
      "- 选择了 100 张图片\n",
      "- 包含 8303 个标注\n",
      "- 数据保存在 data/ForestDamages/sampled_dataset 目录下\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def merge_and_sample_coco(\n",
    "    train_json_path=\"data/ForestDamages/annotations/instances_train2024.json\",\n",
    "    val_json_path=\"data/ForestDamages/annotations/instances_val2024.json\",\n",
    "    train_img_dir=\"data/ForestDamages/train2024\",\n",
    "    val_img_dir=\"data/ForestDamages/val2024\",\n",
    "    output_dir=\"data/ForestDamages/sampled_dataset\",\n",
    "    num_samples=100\n",
    "):\n",
    "    # 创建输出目录\n",
    "    output_img_dir = Path(output_dir) / \"images\"\n",
    "    output_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 读取训练集和验证集的标注文件\n",
    "    with open(train_json_path, 'r', encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "    with open(val_json_path, 'r', encoding='utf-8') as f:\n",
    "        val_data = json.load(f)\n",
    "    \n",
    "    # 合并数据集\n",
    "    merged_images = train_data['images'] + val_data['images']\n",
    "    merged_annotations = train_data['annotations'] + val_data['annotations']\n",
    "    \n",
    "    # 随机选择100张图片\n",
    "    selected_images = random.sample(merged_images, num_samples)\n",
    "    selected_image_ids = {img['id'] for img in selected_images}\n",
    "    \n",
    "    # 筛选对应的标注信息\n",
    "    selected_annotations = [\n",
    "        ann for ann in merged_annotations \n",
    "        if ann['image_id'] in selected_image_ids\n",
    "    ]\n",
    "    \n",
    "    # 创建新的COCO格式数据集\n",
    "    new_dataset = {\n",
    "        'categories': train_data['categories'],\n",
    "        'images': selected_images,\n",
    "        'annotations': selected_annotations\n",
    "    }\n",
    "    \n",
    "    # 可选字段，如果存在则添加\n",
    "    optional_fields = ['info', 'licenses']\n",
    "    for field in optional_fields:\n",
    "        if field in train_data:\n",
    "            new_dataset[field] = train_data[field]\n",
    "    \n",
    "    # 保存新的标注文件\n",
    "    output_json_path = Path(output_dir) / \"annotations.json\"\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(new_dataset, f, indent=4)\n",
    "    \n",
    "    # 复制选中的图片到新目录\n",
    "    for img in selected_images:\n",
    "        img_name = img['file_name']\n",
    "        train_path = Path(train_img_dir) / img_name\n",
    "        val_path = Path(val_img_dir) / img_name\n",
    "        \n",
    "        # 确定源图片路径\n",
    "        if train_path.exists():\n",
    "            src_path = train_path\n",
    "        elif val_path.exists():\n",
    "            src_path = val_path\n",
    "        else:\n",
    "            print(f\"警告：找不到图片 {img_name}\")\n",
    "            continue\n",
    "        \n",
    "        # 复制图片到新目录\n",
    "        shutil.copy2(src_path, output_img_dir / img_name)\n",
    "    \n",
    "    print(f\"已完成数据集合并和采样：\")\n",
    "    print(f\"- 选择了 {len(selected_images)} 张图片\")\n",
    "    print(f\"- 包含 {len(selected_annotations)} 个标注\")\n",
    "    print(f\"- 数据保存在 {output_dir} 目录下\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_and_sample_coco()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并数据集...\n",
      "数据根目录: E:\\SUAL\\SUAL\\data\\ForestDamages\n",
      "训练集标注文件: E:\\SUAL\\SUAL\\data\\ForestDamages\\annotations\\instances_train2024.json\n",
      "验证集标注文件: E:\\SUAL\\SUAL\\data\\ForestDamages\\annotations\\instances_val2024.json\n",
      "训练集图片目录: E:\\SUAL\\SUAL\\data\\ForestDamages\\train2024\n",
      "验证集图片目录: E:\\SUAL\\SUAL\\data\\ForestDamages\\val2024\n",
      "合并后总图像数: 1692\n",
      "合并后总标注数: 94615\n",
      "\n",
      "数据集分割:\n",
      "总样本数: 1692\n",
      "训练集大小: 67 (4.0%)\n",
      "验证集大小: 16 (1.0%)\n",
      "未标注集大小: 1609 (95.0%)\n",
      "\n",
      "处理 labeled_train 数据集...\n",
      "从以下目录复制图片:\n",
      "  - E:\\SUAL\\SUAL\\data\\ForestDamages\\train2024\n",
      "  - E:\\SUAL\\SUAL\\data\\ForestDamages\\val2024\n",
      "成功复制 67 张图片到 data\\ForestDamages\\active_learning_ssc\\images_labeled_train\n",
      "保存标注到: data\\ForestDamages\\active_learning_ssc\\annotations\\instances_labeled_train.json\n",
      "图像数量: 67\n",
      "标注数量: 3774\n",
      "\n",
      "处理 labeled_val 数据集...\n",
      "从以下目录复制图片:\n",
      "  - E:\\SUAL\\SUAL\\data\\ForestDamages\\train2024\n",
      "  - E:\\SUAL\\SUAL\\data\\ForestDamages\\val2024\n",
      "成功复制 16 张图片到 data\\ForestDamages\\active_learning_ssc\\images_labeled_val\n",
      "保存标注到: data\\ForestDamages\\active_learning_ssc\\annotations\\instances_labeled_val.json\n",
      "图像数量: 16\n",
      "标注数量: 1061\n",
      "\n",
      "处理 unlabeled 数据集...\n",
      "从以下目录复制图片:\n",
      "  - E:\\SUAL\\SUAL\\data\\ForestDamages\\train2024\n",
      "  - E:\\SUAL\\SUAL\\data\\ForestDamages\\val2024\n",
      "成功复制 1609 张图片到 data\\ForestDamages\\active_learning_ssc\\images_unlabeled\n",
      "保存标注到: data\\ForestDamages\\active_learning_ssc\\annotations\\instances_unlabeled.json\n",
      "图像数量: 1609\n",
      "标注数量: 89780\n",
      "\n",
      "数据集准备完成!\n",
      "\n",
      "目录结构:\n",
      "data/ForestDamages/active_learning_ssc/\n",
      "├── images_labeled_train/\n",
      "├── images_labeled_val/\n",
      "├── images_unlabeled/\n",
      "└── annotations/\n",
      "    ├── instances_labeled_train.json\n",
      "    ├── instances_labeled_val.json\n",
      "    └── instances_unlabeled.json\n"
     ]
    }
   ],
   "source": [
    "!python tools/prepare_active_dataset.py \\\n",
    "    data/ForestDamages \\\n",
    "    data/ForestDamages/active_learning_ssc \\\n",
    "    --train-ratio 0.04 \\\n",
    "    --val-ratio 0.01 \\\n",
    "    --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
